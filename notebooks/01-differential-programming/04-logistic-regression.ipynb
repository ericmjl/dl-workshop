{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic regression builds upon linear regression.\n",
    "We use logistic regression to perform **binary classification**, \n",
    "that is, distinguishing between two classes. \n",
    "Typically, we label one of the classes with the integer 0, \n",
    "and the other class with the integer 1.\n",
    "\n",
    "What does the model look like?\n",
    "To help you build intuition, let's visualize logistic regression using pictures again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Form\n",
    "\n",
    "Here is logistic regression in matrix form.\n",
    "\n",
    "![](../figures/logreg-matrices.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Diagram\n",
    "\n",
    "Now, here's logistic regression in a neural diagram:\n",
    "\n",
    "![](../figures/logreg-neural.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Activity\n",
    "\n",
    "As should be evident from the pictures, \n",
    "logistic regression builds upon linear regression \n",
    "simply by **changing the activation function \n",
    "from an \"identity\" function to a \"logistic\" function**. \n",
    "In the one-dimensional case, \n",
    "it has the same two parameters as one-dimensional linear regression,  $w$ and $b$. \n",
    "Let's use an interactive visualization \n",
    "to visualize how the parameters $w$ and $b$ affect the shape of the curve.\n",
    "\n",
    "(Note: this exercise is best done in a live notebook!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_workshop.answers import logistic\n",
    "logistic??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(w=FloatSlider(value=0, min=-5, max=5), b=FloatSlider(value=0, min=-5, max=5))\n",
    "def plot_logistic(w, b):\n",
    "    x = np.linspace(-10, 10, 1000)\n",
    "    z = w * x + b  # linear transform on x\n",
    "    y = logistic(z)    \n",
    "    plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "1. What do $w$ and $b$ control, respectively? \n",
    "2. Where else do you see logistic-shaped curves?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make simulated data\n",
    "\n",
    "In this section, we're going to show that we can optimize a logistic regression model using the same set of ingredients. \n",
    "\n",
    "First off, let's start by simulating some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 100)\n",
    "w = 2\n",
    "b = 1\n",
    "z = w * x + b + npr.random(size=len(x))\n",
    "y_true = np.round(logistic(z))\n",
    "plt.scatter(x, y_true, alpha=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "How would we quantify how good or bad our model is? In this case, we use the logistic loss function, also known as the binary cross entropy loss function.\n",
    "\n",
    "Expressed in equation form, it looks like this:\n",
    "\n",
    "$$L = -\\sum (y \\log(p) + (1-y)\\log(1-p)$$\n",
    "\n",
    "Here:\n",
    "\n",
    "- $y$ is the actual class, namely $1$ or $0$.\n",
    "- $p$ is the predicted class.\n",
    "\n",
    "If you're staring at this equation, and thinking that it looks a lot like the Bernoulli distribution log likelihood, you are right!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Let's think about the loss function for a moment:\n",
    "\n",
    "- What happens to the term $y \\log(p)$ when $y=0$ and $y=1$? What about the $(1-y)\\log(1-p)$ term?\n",
    "- What happens to both terms when $p \\approx 0$ and when $p \\approx 1$ (but still bounded between 0 and 1)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Write down the logistic regression model\n",
    "\n",
    "Using the same patterns as you did before for the linear model,\n",
    "define a function called `logistic_model`,\n",
    "which accepts parameters `theta` and data `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Define logistic model\n",
    "def logistic_model(theta, x):\n",
    "    pass\n",
    "\n",
    "from dl_workshop.answers import logistic_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Write down the logistic loss function\n",
    "\n",
    "Now, write down the logistic loss function.\n",
    "It is defined as the negative binary cross entropy\n",
    "between the ground truth and the predicted.\n",
    "\n",
    "The binary cross entropy function is available for you to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_workshop.answers import binary_cross_entropy\n",
    "\n",
    "binary_cross_entropy??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Define logistic loss function, using flattened parameters\n",
    "def logistic_loss(p, model, x, y):\n",
    "    pass\n",
    "\n",
    "from dl_workshop.answers import logistic_loss\n",
    "logistic_loss??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the gradient of the loss function, using `grad`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Define gradient of loss function.\n",
    "dlogistic_loss = grad(logistic_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Initialize logistic regression model parameters using random numbers\n",
    "\n",
    "Because the parameters are identical to linear regression,\n",
    "you probably can use the same `initialize_linear_params` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_workshop.answers import initialize_linear_params\n",
    "\n",
    "theta = initialize_linear_params()\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Write the training loop!\n",
    "\n",
    "This will look very similar to the linear model training loop,\n",
    "because the same two parameters are being optimized.\n",
    "The thing that should change is the loss function and gradient of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_workshop.answers import model_optimization_loop\n",
    "\n",
    "losses, theta = model_optimization_loop(\n",
    "    theta,\n",
    "    logistic_model,\n",
    "    logistic_loss,\n",
    "    x,\n",
    "    y_true,\n",
    "    n_steps=5000,\n",
    "    step_size=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that the values are off from the true value. Why is this so? Partly it's because of the noise that we added, and we also rounded off values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y_true, alpha=0.3)\n",
    "plt.plot(x, logistic_model(theta, x), color='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "What if we did not round off the values, and did not add noise to the original data? Try re-running the model without those two."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-workshop",
   "language": "python",
   "name": "dl-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
