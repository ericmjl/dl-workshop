{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logistic regression builds upon linear regression.\n",
    "We use logistic regression to perform **binary classification**, \n",
    "that is, distinguishing between two classes. \n",
    "Typically, we label one of the classes with the integer 0, \n",
    "and the other class with the integer 1.\n",
    "\n",
    "What does the model look like?\n",
    "To help you build intuition, let's visualize logistic regression using pictures again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix Form\n",
    "\n",
    "Here is logistic regression in matrix form.\n",
    "\n",
    "![](./figures/logreg-matrices.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Diagram\n",
    "\n",
    "Now, here's logistic regression in a neural diagram:\n",
    "\n",
    "![](./figures/logreg-neural.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Activity\n",
    "\n",
    "As should be evident from the pictures, \n",
    "logistic regression builds upon linear regression \n",
    "simply by **changing the activation function \n",
    "from an \"identity\" function to a \"logistic\" function**. \n",
    "In the one-dimensional case, \n",
    "it has the same two parameters as one-dimensional linear regression,  $w$ and $b$. \n",
    "Let's use an interactive visualization \n",
    "to visualize how the parameters $w$ and $b$ affect the shape of the curve.\n",
    "\n",
    "(Note: this exercise is best done in a live notebook!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_workshop.answers import logistic\n",
    "logistic??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, FloatSlider\n",
    "\n",
    "@interact(w=FloatSlider(value=0, min=-5, max=5), b=FloatSlider(value=0, min=-5, max=5))\n",
    "def plot_logistic(w, b):\n",
    "    x = np.linspace(-10, 10, 1000)\n",
    "    z = w * x + b  # linear transform on x\n",
    "    y = logistic(z)    \n",
    "    plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters of the model\n",
    "\n",
    "As we can see, there are two parameters $w$ and $b$.\n",
    "For those who may be encountering the model for the first time,\n",
    "this is what each of them control:\n",
    "\n",
    "- $w$ controls how steep the step function is between 0 and 1 on the y-axis. Its sign also controls whether class 1 is associated with smaller values or larger values.\n",
    "- $b$ controls the midpoint location of the curve. More negative values of $b$ shift it to the left; more positive values of $b$ shift it to the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make simulated data\n",
    "\n",
    "Once again, we are going to use simulated data to help us anchor our understanding.\n",
    "Along the way, we will see how logistic regression, once again, fits inside the same framework\n",
    "of \"model, loss, optimizer\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy.random as npr\n",
    "\n",
    "x = np.linspace(-5, 5, 100)\n",
    "w = 2\n",
    "b = 1\n",
    "z = w * x + b + npr.random(size=len(x))\n",
    "y_true = np.round(logistic(z))\n",
    "plt.scatter(x, y_true, alpha=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we set $w$ to 2 and $b$ to 1, added some noise in there too, and rounded off the logistic-transformed values to between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification Loss Function\n",
    "\n",
    "How would we quantify how good or bad our model is?\n",
    "In this case, we use the _logistic_ loss function, also known as the _binary cross entropy_ loss function.\n",
    "\n",
    "Expressed in equation form, it looks like this:\n",
    "\n",
    "$$L = -\\sum (y \\log(p) + (1-y)\\log(1-p)$$\n",
    "\n",
    "Here:\n",
    "\n",
    "- $y$ is the actual class, namely $1$ or $0$.\n",
    "- $p$ is the predicted class.\n",
    "\n",
    "If you're staring at this equation, and thinking that it looks a lot like the Bernoulli distribution log likelihood, you are right!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Let's think about the loss function for a moment:\n",
    "\n",
    "- What happens to the term $y \\log(p)$ when $y=0$ and $y=1$? What about the $(1-y)\\log(1-p)$ term?\n",
    "- What happens to both terms when $p \\approx 0$ and when $p \\approx 1$ (but still bounded between 0 and 1)?\n",
    "\n",
    "The answers are as follows:\n",
    "\n",
    "- When $y=0$, $y \\log(p) = $, and when $y=1$, $(1-y)\\log(1-p) = 0$.\n",
    "- When $p \\rightarrow 0$, then $\\log(p)$ approaches negative infinity. Likewise for $\\log(1-p)$ when $p \\rightarrow 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Write down the logistic regression model\n",
    "\n",
    "Using the same patterns as you did before for the linear model,\n",
    "define a function called `logistic_model`,\n",
    "which accepts parameters `theta` and data `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Define logistic model\n",
    "def logistic_model(theta, x):\n",
    "    pass\n",
    "\n",
    "from dl_workshop.answers import logistic_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Write down the logistic loss function\n",
    "\n",
    "Now, write down the logistic loss function.\n",
    "It is defined as the negative binary cross entropy\n",
    "between the ground truth and the predicted.\n",
    "\n",
    "The binary cross entropy function is available for you to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_workshop.answers import binary_cross_entropy\n",
    "\n",
    "binary_cross_entropy??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Define logistic loss function\n",
    "def logistic_loss(params, model, x, y):\n",
    "    pass\n",
    "\n",
    "from dl_workshop.answers import logistic_loss\n",
    "logistic_loss??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the gradient of the loss function, using `grad`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad\n",
    "from dl_workshop.answers import dlogistic_loss\n",
    "\n",
    "# Exercise: Define gradient of loss function.\n",
    "# dlogistic_loss = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Initialize logistic regression model parameters using random numbers\n",
    "\n",
    "Because the parameters are identical to linear regression,\n",
    "you probably can use the same `initialize_linear_params` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_workshop.answers import initialize_linear_params\n",
    "\n",
    "theta = initialize_linear_params()\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Write the training loop!\n",
    "\n",
    "This will look very similar to the linear model training loop,\n",
    "because the same two parameters are being optimized.\n",
    "The thing that should change is the loss function and gradient of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_workshop.answers import model_optimization_loop\n",
    "\n",
    "losses, theta = model_optimization_loop(\n",
    "    theta,\n",
    "    logistic_model,\n",
    "    logistic_loss,\n",
    "    x,\n",
    "    y_true,\n",
    "    n_steps=5000,\n",
    "    step_size=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that the values are off from the true value. Why is this so? Partly it's because of the noise that we added, and we also rounded off values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also print out the losses to check that \"learning\" has happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We might argue that the model hasn't yet converged,\n",
    "so we haven't yet figured out the parameters that best explain the data,\n",
    "given the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, checking the model against the actual values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y_true, alpha=0.3)\n",
    "plt.plot(x, logistic_model(theta, x), color='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, we might say that the model parameters _could_ have been optimized further,\n",
    "but as it stands, I'd say we did a pretty good job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "What if we did not round off the values, and did not add noise to the original data? Try re-running the model without those two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Let's recap what we learned here.\n",
    "\n",
    "1. We saw that logistic regression is nothing more than a natural extension of linear regression.\n",
    "1. We saw the introduction of the logistic loss function, and some of its properties.\n",
    "1. We finally saw that we could optimize a model, leveraging the same `grad` function from JAX.\n",
    "\n",
    "To reinforce point 1, let's look at logistic regression in matrix form again.\n",
    "\n",
    "![](./figures/logreg-matrices.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See how there is an extra function $g$ (in yellow), which is the logistic function, that is tacked on.\n",
    "\n",
    "To further reinforce the ideas, we should look at the neural diagram once more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./figures/logreg-neural.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, it's linear model + one more function.\n",
    "\n",
    "Remember this pattern: it will make neural networks much clearer in the next section!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-workshop",
   "language": "python",
   "name": "dl-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
