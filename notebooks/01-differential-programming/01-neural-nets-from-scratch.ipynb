{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks from Scratch\n",
    "\n",
    "In this chapter, we are going to explore differential computing\n",
    "in the place where it was most highly leveraged: the training of neural networks.\n",
    "Now, as with all topics, to learn something most clearly,\n",
    "it pays to have an _anchoring example_ that we start with.\n",
    "\n",
    "In this section, we'll lean heavily on linear regression as that _anchoring example_.\n",
    "We'll also explore what gradient-based optimization is,\n",
    "see an elementary example of that in action,\n",
    "and then connect those ideas back to optimization of a linear model.\n",
    "Once we're done there,\n",
    "then we'll see the exact same ideas in action with a logistic regression model,\n",
    "before finally seeing them in action again with a neural network model.\n",
    "\n",
    "The big takeaway from this chapter is that basically all supervised learning tasks can be broken into:\n",
    "\n",
    "- model\n",
    "- loss\n",
    "- optimizer\n",
    "\n",
    "Hope you enjoy it! If you're ready, let's take a look at linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as np\n",
    "from jax import jit\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, FloatSlider\n",
    "from pyprojroot import here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "Linear regression is foundational to deep learning.\n",
    "It should be a model that everybody has been exposed to before in school.\n",
    "\n",
    "A humorous take I have heard about linear models is that\n",
    "if you zoom in enough into whatever system of the world you're modelling,\n",
    "_anything_ can basically look linear.\n",
    "\n",
    "One of the advantages of linear models is its simplicity.\n",
    "It basically has two parameters, one explaining a \"baseline\" (intercept)\n",
    "and the other explaining strength of relationships (slope).\n",
    "\n",
    "Yet one of the disadvantages of linear models is also its simplicity.\n",
    "A linear model has a strong presumption of linearity.\n",
    "\n",
    "NOTE TO SELF: I need to rewrite this introduction. It is weak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equation Form\n",
    "\n",
    "Linear regression, as a model, is expressed as follows:\n",
    "\n",
    "$$y = wx + b$$\n",
    "\n",
    "Here:\n",
    "\n",
    "- The **model** is the equation, $y = wx + b$.\n",
    "- $y$ is the output data.\n",
    "- $x$ is our input data.\n",
    "- $w$ is a slope parameter.\n",
    "- $b$ is our intercept parameter.\n",
    "- Implicit in the model is the fact that we have transformed $y$ by another function, the \"identity\" function, $f(x) = x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model, $y$ and $x$ are, in a sense, \"fixed\",\n",
    "because this is the data that we have obtained.\n",
    "On the other hand, $w$ and $b$ are the parameters of interest,\n",
    "and *we are interested in **learning** the parameter values for $w$ and $b$\n",
    "that let our model best explain the data*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Simulated Data\n",
    "\n",
    "To explore this idea in a bit more depth as applied to a linear regression model,\n",
    "let us start by making some simulated data with a bit of injected noise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Simulate Data\n",
    "\n",
    "Fill in `w_true` and `b_true` with values that you like, or else leave them alone and follow along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_workshop.answers import x, w_true, b_true, noise\n",
    "\n",
    "# exercise: specify ground truth w as w_true.\n",
    "# w_true = ...\n",
    "\n",
    "# exercise: specify ground truth b as b_true\n",
    "# b_true = ...\n",
    "\n",
    "\n",
    "# exercise: write a function to return the linear equation\n",
    "def make_y(x, w, b):\n",
    "    \"\"\"Your answer here.\"\"\"\n",
    "    return None\n",
    "\n",
    "# Comment out my answer below so it doesn't clobber over yours.\n",
    "from dl_workshop.answers import make_y\n",
    "\n",
    "y = make_y(x, w_true, b_true)\n",
    "\n",
    "# Plot ground truth data\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Take bad guesses\n",
    "\n",
    "Now, let's plot what would be a very bad estimate of $w$ and $b$.\n",
    "Replace the values assigned to `w` and `b` with something of your preference,\n",
    "or feel free to leave them alone and go on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a very bad estimate\n",
    "w = -5  # exercise: fill in a bad value for w\n",
    "b = 3   # exercise: fill in a bad value for b\n",
    "y_est = w * x + b  # exercise: fill in the equation.\n",
    "plt.plot(x, y_est, color='red', label='bad model')\n",
    "plt.scatter(x, y, label='data')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Loss Function\n",
    "\n",
    "How bad is our model?\n",
    "We can quantify this by looking at a metric called the \"mean squared error\".\n",
    "The mean squared error is defined as \"the average of the sum of squared errors\".\n",
    "\n",
    "\"Mean squared error\" is but one of many **loss functions**\n",
    "that are available in deep learning frameworks.\n",
    "It is commonly used for regression tasks.\n",
    "\n",
    "Loss functions are designed to quantify how bad our model is in predicting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Mean Squared Error\n",
    "\n",
    "Implement the mean squred error function in NumPy code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"Implement the function here\"\"\"\n",
    "\n",
    "from dl_workshop.answers import mse\n",
    "\n",
    "# Calculate the mean squared error between \n",
    "print(mse(y, y_est))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity: Optimize model by hand.\n",
    "\n",
    "Now, we're going to optimize this model by hand.\n",
    "If you're viewing this on the website, I'd encourage you to launch a binder session to play around!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ipywidgets import interact, FloatSlider\n",
    "import seaborn as sns\n",
    "\n",
    "@interact(w=FloatSlider(value=0, min=-10, max=10), b=FloatSlider(value=0, min=-10, max=30))\n",
    "def plot_model(w, b):\n",
    "    y_est = w * x + b\n",
    "    plt.scatter(x, y)\n",
    "    plt.plot(x, y_est)\n",
    "    plt.title(f\"MSE: {mse(y, y_est):.2f}\")\n",
    "    sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Minimization\n",
    "\n",
    "As you were optimizing the model, notice what happens to the mean squared error score: it goes down!\n",
    "\n",
    "Implicit in what you were doing is gradient-based optimization.\n",
    "As a \"human\" doing the optimization,\n",
    "you were aware that you needed to move the sliders for $w$ and $b$ \n",
    "in particular directions in order to get a best-fit model.\n",
    "The thing we'd like to learn how to do now\n",
    "is _to get a computer to automatically perform this procedure_.\n",
    "Let's see how to make that happen.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-workshop",
   "language": "python",
   "name": "dl-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
