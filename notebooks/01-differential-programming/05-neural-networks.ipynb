{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "Neural networks are basically very powerful versions of logistic regressions. Like linear and logistic regression, they also take our data and map it to some output, but does so without ever knowing what the true equation form is.\n",
    "\n",
    "That's all a neural network model is: an arbitrarily powerful model.\n",
    "How do feed forward neural networks look like?\n",
    "To give you an intuition for this, let's see one example of a deep neural network in pictures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix diagram\n",
    "\n",
    "As usual, in a matrix diagram.\n",
    "\n",
    "![](../figures/deepnet_regressor-matrices.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural diagram\n",
    "\n",
    "And for correspondence, let's visualize this in a neural diagram.\n",
    "\n",
    "![](../figures/deepnet_regressor-neural.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "- When would we want to use a neural network? When would we not want to use a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Data\n",
    "\n",
    "We are going to try using some real data from the UCI Machine Learning Repository to something related to our work: QSAR modelling!\n",
    "\n",
    "With the dataset below, we want to predict whether a compound is biodegradable based on a series of 41 chemical descriptors.\n",
    "\n",
    "The dataset was taken from: https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation#. I have also prepared the data such that it is split into X (the predictors) and Y (the class that we are trying to predict), so that you do not have to worry about manipulating pandas DataFrames.\n",
    "\n",
    "Let's read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X = pd.read_csv(here() / 'data/biodeg_X.csv', index_col=0)\n",
    "y = pd.read_csv(here() / 'data/biodeg_y.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network model definition\n",
    "\n",
    "Now, let's write a neural network model. \n",
    "This neural network model starts with 41 input nodes,\n",
    "has 1 hidden layer with 20 nodes, and 1 output layer with 1 node.\n",
    "Because this is a classification problem,\n",
    "we will use a logistic activation function right at the end.\n",
    "\n",
    "The parameter shapes define the size of the neural network,\n",
    "while the model function determines the transformations.\n",
    "\n",
    "Let's start with the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(size):\n",
    "    return npr.normal(size=size)\n",
    "\n",
    "params = dict()\n",
    "params['w1'] = noise((41, 20))\n",
    "params['b1'] = noise((20,))\n",
    "params['w2'] = noise((20, 1))\n",
    "params['b2'] = noise((1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define the model as a Python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_model(theta, x):\n",
    "    # \"a1\" is the activation from layer 1\n",
    "    a1 = np.tanh(np.dot(x, theta['w1']) + theta['b1'])\n",
    "    # \"a2\" is the activation from layer 2.\n",
    "    # Explain why we need logistic at the end.\n",
    "    a2 = logistic(np.dot(a1, theta['w2']) + theta['b2'])\n",
    "    return a2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization loop\n",
    "\n",
    "Now, write the optimization loop!\n",
    "It will look very similar to the optimization loop\n",
    "that we wrote for the logistic regression classification model.\n",
    "The difference here is the model that is used,\n",
    "as well as the initialized set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, params = model_optimization_loop(\n",
    "    params, \n",
    "    neural_network_model, \n",
    "    logistic_loss,\n",
    "    X.values,\n",
    "    y.values,\n",
    "    step_size=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.title(f\"final loss: {losses[-1]:.2f}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize trained model performance\n",
    "\n",
    "We can use a confusion matrix to see how \"confused\" a model was.\n",
    "Read more on [Wikipedia](https://en.wikipedia.org/wiki/Confusion_matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = neural_network_model(params, X.values)\n",
    "confusion_matrix(y, np.round(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(confusion_matrix(y, np.round(y_pred)))\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('actual');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "Deep learning, and more generally any modelling, has the following ingredients:\n",
    "\n",
    "1. A model and its associated parameters to be optimized.\n",
    "2. A loss function against which we are optimizing parameters.\n",
    "3. An optimization routine.\n",
    "\n",
    "You have seen these three ingredients at play with 3 different models: a linear regression model, a logistic regression model, and a deep feed forward neural network model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Pictures\n",
    "\n",
    "Here is a summary of what we've learned in this tutorial!\n",
    "\n",
    "![](../figures/infographic.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caveats of this tutorial\n",
    "\n",
    "Deep learning is an active field of research. I have only shown you the basics here. In addition, I have also intentionally omitted certain aspects of machine learning practice, such as \n",
    "\n",
    "- splitting our data into training and testing sets, \n",
    "- performing model selection using cross-validation\n",
    "- tuning hyperparameters, such as trying out optimizers\n",
    "- regularizing the model, using L1/L2 regularization or dropout\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parting Thoughts\n",
    "\n",
    "Deep learning is nothing more than optimization of a model with a really large number of parameters.\n",
    "\n",
    "In its current state, it is not artificial intelligence.\n",
    "You should not be afraid of it;\n",
    "it is nothing more than a really powerful model that maps X to Y."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-workshop",
   "language": "python",
   "name": "dl-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
