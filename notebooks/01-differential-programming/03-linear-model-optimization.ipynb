{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizing Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are we optimizing?\n",
    "\n",
    "In linear regression, we are:\n",
    "\n",
    "- minimizing (i.e. optimizing) the loss function \n",
    "- _with respect to_ the linear regression parameters.\n",
    "\n",
    "Here are the parallels to the example above:\n",
    "\n",
    "- In the example above, we minimized $f(w)$, the polynomial function. With linear regression, we are minimizing the mean squared error.\n",
    "- In the example above, we minimized $f(w)$ with respect to $w$, where $w$ is the key parameter of $f$. With linear regression, we minimize mean squared error of our model prediction with respect to the linear regression parameters. (Let's call the parameters collectively $\\theta$, such that $\\theta = (w, b)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingredients for \"Optimizing\" a Model\n",
    "\n",
    "At this point, we have learned what the ingredients are for optimizing a model:\n",
    "\n",
    "1. A model, which is a function that maps inputs $x$ to outputs $y$, and its parameters of the model. \n",
    "    1. Not to belabour the point, but in our linear regression case, this is $w$ and $b$; \n",
    "    1. Usually, in the literature, we call this **parameter set** $\\theta$, such that $\\theta$ encompasses all parameters of the model.\n",
    "2. Loss function, which tells us how bad our predictions are.\n",
    "3. Optimization routine, which tells the computer how to adjust the parameter values to minimize the loss function.\n",
    "\n",
    "**Keep note:** Because we are optimizing the loss w.r.t. two parameters, finding the $w$ and $b$ coordinates that minimize the loss is like finding the minima of a bowl.\n",
    "\n",
    "The latter point, which is \"how to adjust the parameter values to minimize the loss function\", is the key point to understand here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing this in JAX/NumPy\n",
    "\n",
    "How do we optimize the parameters of our linear regression model using JAX?\n",
    "Let's explore how to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Define the linear regression model\n",
    "\n",
    "Firstly, let's define our model function.\n",
    "Write it out as a Python function,\n",
    "named `linear_model`,\n",
    "such that the parameters $\\theta$ are the first argument,\n",
    "and the data `x` are the second argument.\n",
    "It should return the model prediction.\n",
    "\n",
    "What should the data type of $\\theta$ be?\n",
    "You can decide, as long as it's a built-in Python data type,\n",
    "or NumPy data type, or some combination of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Define the model in this function\n",
    "def linear_model(theta, x):\n",
    "    pass\n",
    "\n",
    "from dl_workshop.answers import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Initialize linear regression model parameters using random numbers\n",
    "\n",
    "Using a random number generator,\n",
    "such as the `numpy.random.normal` function,\n",
    "write a function that returns \n",
    "random number starting points for each linear model parameter.\n",
    "Make sure it returns params in the form that are accepted by\n",
    "the `linear_model` function defined above.\n",
    "\n",
    "Hint: NumPy's random module (which is distinct from JAX's) has been imported for you in the namespace `npr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_linear_params():\n",
    "    pass\n",
    "\n",
    "# Comment this out if you fill in your answer above.\n",
    "from dl_workshop.answers import initialize_linear_params\n",
    "theta = initialize_linear_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Define the mean squared error loss function with linear model parameters as first argument\n",
    "\n",
    "Now, define the mean squared error loss function, called `mseloss`,\n",
    "such that \n",
    "1. the parameters $\\theta$ are accepted as the first argument,\n",
    "2. `model` function as the second argument,\n",
    "3. `x` as the third argument,\n",
    "4. `y` as the fourth argument, and\n",
    "5. returns a scalar valued result.\n",
    "\n",
    "This is the function we will be differentiating,\n",
    "and JAX's `grad` function will take the derivative of the function w.r.t. the first argument.\n",
    "Thus, $\\theta$ must be the first argument!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differentiable loss function w.r.t. 1st argument\n",
    "def mseloss(theta, model, x, y):\n",
    "    pass\n",
    "\n",
    "from dl_workshop.answers import mseloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we generate a new function called `dmseloss`, by calling `grad` on `mseloss`!\n",
    "The new function `dmseloss` will have the exact same signature\n",
    "as `mseloss`,\n",
    "but will instead return the value of the gradient\n",
    "evaluated at each of the parameters in $\\theta$,\n",
    "in the same data structure as $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your answer here.\n",
    "\n",
    "\n",
    "# The actual dmseloss function is also present in the answers,\n",
    "# but _seriously_, go fill the one-liner to get dmseloss defined!\n",
    "# If you fill out the one-liner above,\n",
    "# remember to comment out the answer below\n",
    "# so that mine doesn't clobber over yours!\n",
    "from dl_workshop.answers import dmseloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've provided an execution of the function below,\n",
    "so that you have an intuition of what's being returned.\n",
    "In my implementation,\n",
    "because theta are passed in as a 2-tuple,\n",
    "the gradients are returned as a 2-tuple as well.\n",
    "The return type will match up with how you pass in the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_workshop.answers import x, make_y, b_true, w_true\n",
    "\n",
    "# Create y by replacing my b_true and w_true with whatever you want\n",
    "y = make_y(x, w_true, b_true)\n",
    "dmseloss(dict(w=0.3, b=0.5), linear_model, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Write the optimization routine\n",
    "\n",
    "Finally, write the optimization routine!\n",
    "\n",
    "Make it run for 3,000 iterations,\n",
    "and record the loss on each iteration.\n",
    "Don't forget to update your parameters!\n",
    "(How you do so will depend on how you've set up the parameters.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your optimization routine below.\n",
    "\n",
    "\n",
    "# And if you implemented your optimization loop,\n",
    "# feel free to comment out the next two lines\n",
    "from dl_workshop.answers import model_optimization_loop\n",
    "losses, theta = model_optimization_loop(theta, linear_model, mseloss, x, y, n_steps=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot the loss score over time. It should be going downwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('mse');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect your parameters to see if they've become close to the true values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingredients of  Linear Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these first three sections, have seen how the following components play inside a linear model:\n",
    "\n",
    "1. Model specification (\"equations\", e.g. $y = wx + b$) and the parameters of the model to be optimized ($w$ and $b$, or more generally, $\\theta$).\n",
    "2. Loss function: tells us how wrong our model parameters are w.r.t. the data ($MSE$)\n",
    "3. Optimization routine (for-loop)\n",
    "\n",
    "Let's now explore a few pictorial representations of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression In Pictures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression can be expressed pictorially, not just in equation form. Here are two ways of visualizing linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression in one dimension looks like this:\n",
    "\n",
    "![](./figures/linreg-scalar.png)\n",
    "\n",
    "Linear regression in higher dimensions looks like this:\n",
    "\n",
    "![](./figures/linreg-matrices.png)\n",
    "\n",
    "This is also known in the statistical world as \"multiple linear regression\".\n",
    "The general idea, though, should be pretty easy to catch.\n",
    "You can do linear regression that projects any arbitrary number of input dimensions\n",
    "to any arbitrary number of output dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Diagram\n",
    "\n",
    "We can draw a \"neural diagram\" based on the matrix view, with the implicit \"identity\" function included in orange.\n",
    "\n",
    "![](./figures/linreg-neural.png) \n",
    "\n",
    "The neural diagram is one that we commonly see in the introductions to deep learning. As you can see here, linear regression, when visualized this way, can be conceptually thought of as the baseline model for understanding deep learning. \n",
    "\n",
    "The neural diagram also expresses the \"compute graph\" that transforms input variables to output variables."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-workshop",
   "language": "python",
   "name": "dl-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-showmarkdowntxt": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
