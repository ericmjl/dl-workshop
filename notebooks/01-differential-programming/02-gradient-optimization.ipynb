{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient-Based Optimization\n",
    "\n",
    "Implicit in what you were doing was something we formally call \"gradient-based optimization\". This is a very important point to understand. If you get this for a linear model, you will understand how this works for more complex models. Hence, we are going to go into a small crash-course detour on what gradient-based optimization is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Derivatives\n",
    "\n",
    "At the risk of ticking off mathematicians for a sloppy definition,\n",
    "for this book's purposes,\n",
    "a useful way of defining the derivative is:\n",
    "\n",
    "> How much our output changes as we take a small step on the inputs, taken in the limit of going to very small steps.\n",
    "\n",
    "If we have a function:\n",
    "\n",
    "$$f(w) = w^2 + 3w - 5$$\n",
    "\n",
    "What is the derivative of $f(x)$ with respect to $w$?\n",
    "From first-year undergraduate calculus,\n",
    "we should be able to calculate this:\n",
    "\n",
    "$$f'(w) = 2w + 3$$\n",
    "\n",
    "As a matter of style, we will use the apostrophe marks to indicate derivatives.\n",
    "1 apostrophe mark means first derivative, 2nd apostrophe mark means 2nd derivative, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing $f(w)$ Analytically\n",
    "\n",
    "What is the value of $w$ that minimizes $f(w)$?\n",
    "Again, from undergraduate calculus,\n",
    "we know that at a minima of a function (whether it is a global or local),\n",
    "the first derivative will be equal to zero, i.e. $f'(w) = 0$.\n",
    "By taking advantage of this property,\n",
    "we can analytically solve for the value of $w$ at the minima.\n",
    "\n",
    "$$2w + 3 = 0$$\n",
    "\n",
    "Hence, \n",
    "\n",
    "$$w = -\\frac{3}{2} = 1.5$$\n",
    "\n",
    "To check whether the value of $w$ at the place where $f'(w) = 0$ is a minima or maxima,\n",
    "we can use another piece of knowledge from 1st year undergraduate calculus:\n",
    "The sign of the second derivative will tell us whether this is a minima or maxima.\n",
    "\n",
    "- If the second derivative is positive regardless of the value of $w$, then the point is a minima. (Smiley faces are positive!)\n",
    "- If the second derivative is negative regardless of the value of $w$, then the point is a maxima. (Frowning faces are negative!)\n",
    "\n",
    "Hence, \n",
    "\n",
    "$$f''(w) = 2$$\n",
    "\n",
    "We can see that $f''(w) > 0$ for all $w$, hence the stationary point we find is going to be a local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimizing $f(w)$ Computationally\n",
    "\n",
    "An alternative way of looking at this is to take advantage of $f'(w)$, the gradient, evaluated at a particular $w$. A known property of the gradient is that if you take steps in the negative direction of the gradient, you will eventually reach a function's minima. If you take small steps in the positive direction of the gradient, you will reach a function's maxima (if it exists)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Implement gradient functions by hand\n",
    "\n",
    "Let's implement this using the function $f(w)$, done using NumPy.\n",
    "\n",
    "Firstly, implement the aforementioned function $f$ below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Write f(w) as a function.\n",
    "\n",
    "def f(w):\n",
    "    \"\"\"Your answer here.\"\"\"\n",
    "    return None\n",
    "\n",
    "from dl_workshop.answers import f\n",
    "\n",
    "f(2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is the **objective function** that we wish to optimize,\n",
    "where \"optimization\" means finding the minima or maxima.\n",
    "\n",
    "Now, implement the gradient function $\\frac{df}{dw}$ below in the function `df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Write df(w) as a function. \n",
    "def df(w):\n",
    "    \"\"\"Your answer here\"\"\"\n",
    "    return None\n",
    "\n",
    "from dl_workshop.answers import df\n",
    "df(2.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is the **gradient of the objective w.r.t. the parameter of interest**.\n",
    "It will help us find out the direction in which to change the parameter $w$\n",
    "in order to optimize the objective function.\n",
    "\n",
    "Now, pick a number at random to start with.\n",
    "You can specify a number explicitly,\n",
    "or use a random number generator to draw a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Pick a number to start w at.\n",
    "w = 10.0  # start with a float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a starting point for optimization.\n",
    "\n",
    "Finally, write an \"optimization loop\",\n",
    "in which you adjust the value of $w$\n",
    "in the negative direction of the gradient of $f$ w.r.t. $w$ (i.e. $\\frac{df}{dw}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, adjust the value of w 1000 times, taking small steps in the negative direction of the gradient.\n",
    "for i in range(1000):\n",
    "    w = w - df(w) * 0.01  # 0.01 is the size of the step taken.\n",
    "    \n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you have just implemented **gradient descent**!\n",
    "\n",
    "Gradient descent is an **optimization routine**: a way of programming a computer to do optimization for you so that you don't have to do it by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing $f(w)$ with `jax`\n",
    "\n",
    "`jax` is a Python package for automatically computing gradients; \n",
    "it provides what is known as an \"automatic differentiation\" system\n",
    "on top of the NumPy API.\n",
    "This way, we do not have to specify the gradient function by hand-calculating it;\n",
    "rather, `jax` will know how to automatically take the derivative of a Python function\n",
    "w.r.t. the first argument,\n",
    "leveraging the chain rule to help calculate gradients.\n",
    "With `jax`, our example above is modified in only a slightly different way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad\n",
    "import jax\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "# This is what changes: we use autograd's `grad` function to automatically return a gradient function.\n",
    "df = grad(f)\n",
    "\n",
    "# Exercise: Pick a number to start w at.\n",
    "w = -10.0\n",
    "\n",
    "# Now, adjust the value of w 1000 times, taking small steps in the negative direction of the gradient.\n",
    "for i in range(1000):\n",
    "    w = w - df(w) * 0.01  # 0.01 is the size of the step taken.\n",
    "\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this section, we saw one way to program a computer\n",
    "to automatically leverage _gradients_\n",
    "to find the optima of a polynomial function.\n",
    "This builds our knowledge and intuition for the next section,\n",
    "in which we find the optimal point of a linear regression loss function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-workshop",
   "language": "python",
   "name": "dl-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
