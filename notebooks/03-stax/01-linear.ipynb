{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing linear models with `stax`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I'll show the code for how to use JAX's `stax` submodule to write arbitrary models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "I'm assuming you have read through the `jax-programming.ipynb` notebook, as well as the `tutorial.ipynb` notebook.\n",
    "\n",
    "The main `tutorial.ipynb` notebook gives you a general introduction to differential programming using `grad`, while the `jax-programming.ipynb` notebook gives you a flavour of the other four main JAX idioms: `vmap`, `lax.scan`, `random.PRNGKey`, and `jit`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is `stax`?\n",
    "\n",
    "Most deep learning libraries use objects as the data structure for a neural network layer.\n",
    "As such, the tunable parameters of the layer, for example `w` and `b` for a linear (\"dense\") layer\n",
    "are class attributes associated with the forward function.\n",
    "\n",
    "In some sense, because a neural network layer is nothing more than a math function,\n",
    "specifying the layer in terms of a function might also make sense.\n",
    "`stax`, then, is a new take on writing neural network models using pure functions rather than objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does `stax` work?\n",
    "\n",
    "The way that `stax` layers work is as follows.\n",
    "Every neural network layer is nothing more than a math function with a \"forward\" pass.\n",
    "Neural network models typically have their parameters \n",
    "initialized into the right shapes using random number generators.\n",
    "Put these two together, and we have a _pair_ of functions that specify a layer:\n",
    "\n",
    "- An `init_fun` function, that _initializes_ parameters into the correct shapes, and\n",
    "- An `apply_fun` function, that _applies_ the specified math transformations onto incoming data, using parameters of the correct shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Linear layer\n",
    "\n",
    "Let's see an example of this in action, by studying the implementation of the linear (\"dense\") layer in `stax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental import stax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stax.Dense??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the `apply_fun` specifies the linear transformation.\n",
    "It accepts a parameter called `params`,\n",
    "which gets tuple-unpacked into the appropriate `W` and `b`.\n",
    "\n",
    "**Notice how the `params` argument matches up with the second output of `init_fun`!**\n",
    "The `init_fun` always accepts an `rng` parameter, which is returned from JAX's `jax.random.PRNGKey()`.\n",
    "It also accepts an `input_shape` parameter,\n",
    "which specifies what the elementary shape of one sample of data is.\n",
    "So if your entire dataset is of shape `(n_samples, n_columns)`,\n",
    "then you would put in `(n_columns,)` inside there,\n",
    "as you would want to ignore the sample dimension,\n",
    "thus allowing us to take advantage of `vmap` to map our model function\n",
    "over each and every i.i.d. sample in our dataset.\n",
    "The `init_fun` also returns the `output_shape`,\n",
    "which is used later when we chain layers together.\n",
    "\n",
    "Let's see how we can use the Dense layer to specify a linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the initialization and application function pairs\n",
    "\n",
    "Firstly, we create the `init_fun` and `apply_fun` pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_fun, apply_fun = stax.Dense(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the parameters\n",
    "\n",
    "Now, let's initialize parameters using the `init_fun`.\n",
    "\n",
    "Let's assume that we have data that is of 4 columns only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random, numpy as np\n",
    "\n",
    "key = random.PRNGKey(42)\n",
    "\n",
    "output_shape, params_initial = init_fun(key, input_shape=(4,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_initial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply parameters and data through function\n",
    "\n",
    "We'll create some randomly generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = random.normal(key, shape=(200, 4))\n",
    "X[0:5], X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's some `y_true` values that I've snuck in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.dot(X, np.array([1, 2, 3, 4])) + 5\n",
    "y_true = y_true.reshape(-1, 1)\n",
    "y_true[0:5], y_true.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll pass data through the linear model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_fun??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import vmap\n",
    "from functools import partial\n",
    "\n",
    "y_pred = vmap(partial(apply_fun, params_initial))(X)\n",
    "y_pred[0:5], y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voil√†! We have a simple linear model implemented just like that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "Next question: how do we *optimize* the parameters using JAX?\n",
    "\n",
    "Instead of writing a training loop on our own, we can take advantage of JAX's optimizers, which are also written in a functional paradigm!\n",
    "\n",
    "JAX's optimizers are constructed as a \"triplet\" set of functions:\n",
    "\n",
    "- `init`: Takes `params` and initializes them in as a `state`, which is structured in a fashion that `update` can operate on.\n",
    "- `update`: Takes in `i`, `g`, and `state`, which respectively are:\n",
    "    - `i`: The current loop iteration\n",
    "    - `g`: Gradients calculated from `grad`!\n",
    "    - `state`: The current state of the parameters.\n",
    "- `get_params`: Takes in the `state` at a given point, and returns the parameters structured correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jit, grad\n",
    "from jax.experimental.optimizers import adam\n",
    "\n",
    "init, update, get_params = adam(step_size=1e-1)\n",
    "update = jit(update)\n",
    "get_params = jit(get_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're still missing a piece here, that is the loss function.\n",
    "For illustration purposes, let's use the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mseloss(params, model, x, y_true):\n",
    "    y_preds = vmap(partial(model, params))(x)\n",
    "    return np.mean(np.power(y_preds - y_true, 2))\n",
    "\n",
    "dmseloss = grad(mseloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Step\" portion of update loop\n",
    "\n",
    "Now, we're going to define the \"step\" portion of the update loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_workshop.stax_models import step\n",
    "step??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JIT compilation\n",
    "\n",
    "Because it takes so many parameters (in order to remain pure, and not rely on notebook state),\n",
    "we're going to bind some of them using `functools.partial`.\n",
    "\n",
    "I'm also going to show you what happens when we JIT-compile vs. don't JIT-compile the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_partial = partial(step, get_params=get_params, dlossfunc=dmseloss, update=update, model=apply_fun, x=X, y_true=y_true)\n",
    "step_partial_jit = jit(step_partial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicit loops\n",
    "\n",
    "Firstly, let's see what kind of code we'd write if we _did_ write the loop explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "start = time()\n",
    "state = init(params_initial)\n",
    "for i in range(1000):\n",
    "    params = get_params(state)\n",
    "    g = dmseloss(params, apply_fun, X, y_true)\n",
    "    state = update(i, g, state)\n",
    "end = time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partialled out loop step\n",
    "\n",
    "Now, let's run the loop with the partialled out function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "state = init(params_initial)\n",
    "for i in range(1000):\n",
    "    state = step_partial(i, state)\n",
    "end = time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JIT-compiled loop!\n",
    "\n",
    "This is much cleaner of a loop, but we did have to do some work up-front.\n",
    "\n",
    "What happens if we now use the JIT-ed function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "state = init(params_initial)\n",
    "for i in range(1000):\n",
    "    state = step_partial_jit(i, state)\n",
    "end = time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa, holy smokes, that's fast! At least 10X faster using JIT-compilation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `lax.scan` loop\n",
    "\n",
    "Now we'll use some JAX trickery ot write a training loop without ever writing a for-loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_workshop.stax_models import make_scannable_step\n",
    "make_scannable_step??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import lax\n",
    "\n",
    "scannable_step = make_scannable_step(step_partial_jit)\n",
    "\n",
    "start = time()\n",
    "initial_state = init(params_initial)\n",
    "final_state, states_history = lax.scan(scannable_step, initial_state, np.arange(1000))\n",
    "end = time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_params(final_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `vmap`-ed training loop over multiple starting points\n",
    "\n",
    "Now, we're going to do the ultimate: we'll create at least 100 different parameter initializations and run our training loop over each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_workshop.stax_models import make_training_start\n",
    "make_training_start??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import lax\n",
    "\n",
    "train_linear = make_training_start(partial(init_fun, input_shape=(-1, 4)), init, scannable_step, 1000)\n",
    "\n",
    "start = time()\n",
    "N_INITIALIZATIONS = 100\n",
    "initialization_keys = random.split(key, N_INITIALIZATIONS)\n",
    "final_states, states_histories = vmap(train_linear)(initialization_keys)\n",
    "end = time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_final, b_final = vmap(get_params)(final_states)\n",
    "w_final.squeeze()[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_final.squeeze()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we were also able to run the whole optimization pretty fast, _and_ recover the correct parameters over multiple training starts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JIT-compiled training loop\n",
    "\n",
    "What happens if we JIT-compile the vmapped initialization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "N_INITIALIZATIONS = 100\n",
    "initialization_keys = random.split(key, N_INITIALIZATIONS)\n",
    "train_linear_jit = jit(train_linear)\n",
    "final_states, states_histories = vmap(train_linear_jit)(initialization_keys)\n",
    "vmap(get_params)(final_states)  # this line exists to just block the computation until it completes.\n",
    "end = time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HOOOOOLY SMOKES! Did you see that? With JIT-compilation, we essentially took the training time down to be identical to training on one starting point. Naturally, I don't expect this result to hold 100% of the time, but it's pretty darn rad to see that live. \n",
    "\n",
    "The craziest piece here is that we could `vmap` our training loop over multiple starting points and get massive speedups there."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-workshop",
   "language": "python",
   "name": "dl-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
