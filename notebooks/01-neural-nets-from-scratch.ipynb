{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as np\n",
    "from jax import jit\n",
    "import numpy.random as npr\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact, FloatSlider\n",
    "from pyprojroot import here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "Linear regression is foundational to deep learning. It should be a model that everybody has been exposed to before. However, it is important for us to go through this with a view to how we connect linear regression to the neural diagrams that are shown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "- In our machine learning toolkit, where do we use linear regression? \n",
    "- What are its advantages? \n",
    "- What are its disadvantages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equation Form\n",
    "\n",
    "Linear regression, as a model, is expressed as follows:\n",
    "\n",
    "$$y = wx + b$$\n",
    "\n",
    "Here:\n",
    "\n",
    "- The **model** is the equation, $y = wx + b$.\n",
    "- $y$ is the output data.\n",
    "- $x$ is our input data.\n",
    "- $w$ is a slope parameter.\n",
    "- $b$ is our intercept parameter.\n",
    "- Implicit in the model is the fact that we have transformed $y$ by another function, the \"identity\" function, $f(x) = x$.\n",
    "\n",
    "In this model, $y$ and $x$ are, in a sense, \"fixed\", because this is the data that we have obtained. On the other hand, $w$ and $b$ are the parameters of interest, and *we are interested in **learning** the parameter values for $w$ and $b$ that let our model best explain the data*.\n",
    "\n",
    "I will reveal the punchline early:\n",
    "\n",
    "> The **learning** in deep learning is about figuring out parameter values for a given model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Simulated Data\n",
    "\n",
    "To explore this idea in a bit more depth as applied to a linear regression model, let us start by making some simulated data with a bit of injected noise.\n",
    "\n",
    "You can specify a true $w$ and a true $b$ as you wish, or you can just follow along."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Simulate Data\n",
    "\n",
    "Fill in `w_true` and `b_true` with values that you like, or else leave them alone and follow along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 100)\n",
    "w_true = 2  # exercise: specify ground truth w.\n",
    "b_true = 20  # exercise: specify ground truth b.\n",
    "\n",
    "def noise(n):\n",
    "    return npr.normal(size=(n))\n",
    "\n",
    "# exercise: write the linear equation down.\n",
    "y = w_true * x + b_true + noise(len(x))\n",
    "\n",
    "# Plot ground truth data\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Take bad guesses\n",
    "\n",
    "Now, let's plot what would be a very bad estimate of $w$ and $b$.\n",
    "Replace the values assigned to `w` and `b` with something of your preference,\n",
    "or feel free to leave them alone and go on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a very bad estimate\n",
    "w = -5  # exercise: fill in a bad value for w\n",
    "b = 3   # exercise: fill in a bad value for b\n",
    "y_est = w * x + b  # exercise: fill in the equation.\n",
    "plt.plot(x, y_est, color='red', label='bad model')\n",
    "plt.scatter(x, y, label='data')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Loss Function\n",
    "\n",
    "How bad is our model?\n",
    "We can quantify this by looking at a metric called the \"mean squared error\".\n",
    "The mean squared error is defined as \"the average of the sum of squared errors\".\n",
    "\n",
    "\"Mean squared error\" is but one of many **loss functions**\n",
    "that are available in deep learning frameworks.\n",
    "It is commonly used for regression tasks.\n",
    "\n",
    "Loss functions are designed to quantify how bad our model is in predicting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Mean Squared Error\n",
    "\n",
    "Implement the mean squred error function in NumPy code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n",
    "    \"\"\"Implement the function here\"\"\"\n",
    "\n",
    "from dl_workshop.answers import mse\n",
    "\n",
    "# Calculate the mean squared error between \n",
    "print(mse(y, y_est))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activity: Optimize model by hand.\n",
    "\n",
    "Now, we're going to optimize this model by hand.\n",
    "(This is best done in a live notebook, and not on the website;\n",
    "use the sliders provided to adjust the model.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from ipywidgets import interact, FloatSlider\n",
    "import seaborn as sns\n",
    "\n",
    "@interact(w=FloatSlider(value=0, min=-10, max=10), b=FloatSlider(value=0, min=-10, max=30))\n",
    "def plot_model(w, b):\n",
    "    y_est = w * x + b\n",
    "    plt.scatter(x, y)\n",
    "    plt.plot(x, y_est)\n",
    "    sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "As you were optimizing the model, what did you notice about the MSE? score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detour: Gradient-Based Optimization\n",
    "\n",
    "Implicit in what you were doing was something we formally call \"gradient-based optimization\". This is a very important point to understand. If you get this for a linear model, you will understand how this works for more complex models. Hence, we are going to go into a small crash-course detour on what gradient-based optimization is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Derivatives\n",
    "\n",
    "At the risk of ticking off mathematicians for a sloppy definition, for this workshop's purposes, a useful way of defining the derivative is:\n",
    "\n",
    "> How much our output changes as we take a small step on the inputs, taken in the limit of going to very small steps.\n",
    "\n",
    "If we have a function:\n",
    "\n",
    "$$f(w) = w^2 + 3w - 5$$\n",
    "\n",
    "What is the derivative of $f(x)$ with respect to $w$? From first-year undergraduate calculus, we should be able to calculate this:\n",
    "\n",
    "$$f'(w) = 2w + 3$$\n",
    "\n",
    "(We will use the apostrophe marks to indicate derivatives. 1 apostrophe mark means first derivative, 2nd apostrophe mark means 2nd derivative.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing $f(w)$ Analytically\n",
    "\n",
    "What is the value of $w$ that minimizes $f(w)$? Again, from undergraduate calculus, we know that at a minima of a function (whether it is a global or local), the first derivative will be equal to zero, i.e. $f'(w) = 0$. By taking advantage of this property, we can analytically solve for the value of $w$ at the minima.\n",
    "\n",
    "$$2w + 3 = 0$$\n",
    "\n",
    "Hence, \n",
    "\n",
    "$$w = -\\frac{3}{2} = 1.5$$\n",
    "\n",
    "To check whether the value of $w$ at the place where $f'(w) = 0$ is a minima or maxima, we can use another piece of knowledge from 1st year undergraduate calculus: The sign of the second derivative will tell us whether this is a minima or maxima.\n",
    "\n",
    "- If the second derivative is positive regardless of the value of $w$, then the point is a minima. (Smiley faces are positive!)\n",
    "- If the second derivative is negative regardless of the value of $w$, then the point is a maxima. (Frowning faces are negative!)\n",
    "\n",
    "Hence, \n",
    "\n",
    "$$f''(w) = 2$$\n",
    "\n",
    "We can see that $f''(w) > 0$ for all $w$, hence the stationary point we find is going to be a local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing $f(w)$ Computationally\n",
    "\n",
    "An alternative way of looking at this is to take advantage of $f'(w)$, the gradient, evaluated at a particular $w$. A known property of the gradient is that if you take steps in the negative direction of the gradient, you will eventually reach a function's minima. If you take small steps in the positive direction of the gradient, you will reach a function's maxima (if it exists)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Implement gradient functions by hand\n",
    "\n",
    "Let's implement this using the function $f(w)$, done using NumPy.\n",
    "\n",
    "Firstly, implement the aforementioned function $f$ below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Write f(w) as a function.\n",
    "def f(w):\n",
    "    return w**2 + 3 * w - 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is the **objective function** that we wish to optimize,\n",
    "where \"optimization\" means finding the minima or maxima.\n",
    "\n",
    "Now, implement the gradient function $\\frac{df}{dw}$ below in the function `df`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Write df(w) as a function. \n",
    "def df(w):\n",
    "    \"\"\"\n",
    "    The derivative of f with respect to w.\n",
    "    \"\"\"\n",
    "    return 2 * w + 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function is the **gradient of the objective w.r.t. the parameter of interest**.\n",
    "It will help us find out the direction in which to change the parameter $w$\n",
    "in order to optimize the objective function.\n",
    "\n",
    "Now, pick a number at random to start with.\n",
    "You can specify a number explicitly,\n",
    "or use a random number generator to draw a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Pick a number to start w at.\n",
    "w = 10.0  # start with a float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gives us a starting point for optimization.\n",
    "\n",
    "Finally, write an \"optimization loop\",\n",
    "in which you adjust the value of $w$\n",
    "in the negative direction of the gradient of $f$ w.r.t. $w$ (i.e. $\\frac{df}{dw}$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, adjust the value of w 1000 times, taking small steps in the negative direction of the gradient.\n",
    "for i in range(1000):\n",
    "    w = w - df(w) * 0.01  # 0.01 is the size of the step taken.\n",
    "    \n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you have just implemented **gradient descent**!\n",
    "\n",
    "Gradient descent is an **optimization routine**: a way of programming a computer to do optimization for you so that you don't have to do it by hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimizing $f(w)$ with `jax`\n",
    "\n",
    "`jax` is a Python package for automatically computing gradients; \n",
    "it provides what is known as an \"automatic differentiation\" system\n",
    "on top of the NumPy API.\n",
    "This way, we do not have to specify the gradient function by hand-calculating it;\n",
    "rather, `jax` will know how to automatically take the derivative of a Python function\n",
    "w.r.t. the first argument,\n",
    "leveraging the chain rule to help calculate gradients.\n",
    "With `jax`, our example above is modified in only a slightly different way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad\n",
    "import jax\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "def f(w):\n",
    "    return w**2 + 3 * w - 5\n",
    "\n",
    "# This is what changes: we use autograd's `grad` function to automatically return a gradient function.\n",
    "df = grad(f)\n",
    "\n",
    "# Exercise: Pick a number to start w at.\n",
    "w = -10.0\n",
    "\n",
    "# Now, adjust the value of w 1000 times, taking small steps in the negative direction of the gradient.\n",
    "for i in tqdm(range(1000)):\n",
    "    w = w - df(w) * 0.01  # 0.01 is the size of the step taken.\n",
    "\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to Optimizing Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are we optimizing?\n",
    "\n",
    "In linear regression, we are:\n",
    "\n",
    "- minimizing (i.e. optimizing) the loss function \n",
    "- _with respect to_ the linear regression parameters.\n",
    "\n",
    "Here are the parallels to the example above:\n",
    "\n",
    "- In the example above, we minimized $f(w)$, the polynomial function. With linear regression, we are minimizing the mean squared error.\n",
    "- In the example above, we minimized $f(w)$ with respect to $w$, where $w$ is the key parameter of $f$. With linear regression, we minimize mean squared error of our model prediction with respect to the linear regression parameters. (Let's call the parameters collectively $\\theta$, such that $\\theta = (w, b)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingredients for \"Optimizing\" a Model\n",
    "\n",
    "At this point, we have learned what the ingredients are for optimizing a model:\n",
    "\n",
    "1. A model, which is a function that maps inputs $x$ to outputs $y$, and its parameters of the model. \n",
    "    1. Not to belabour the point, but in our linear regression case, this is $w$ and $b$; \n",
    "    1. Usually, in the literature, we call this **parameter set** $\\theta$, such that $\\theta$ encompasses all parameters of the model.\n",
    "2. Loss function, which tells us how bad our predictions are.\n",
    "3. Optimization routine, which tells the computer how to adjust the parameter values to minimize the loss function.\n",
    "\n",
    "**Keep note:** Because we are optimizing the loss w.r.t. two parameters, finding the $w$ and $b$ coordinates that minimize the loss is like finding the minima of a bowl.\n",
    "\n",
    "The latter point, which is \"how to adjust the parameter values to minimize the loss function\", is the key point to understand here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing this in JAX/NumPy\n",
    "\n",
    "How do we optimize the parameters of our linear regression model using JAX?\n",
    "Let's explore how to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Define the linear regression model\n",
    "\n",
    "Firstly, let's define our model function.\n",
    "Write it out as a Python function,\n",
    "named `linear_model`,\n",
    "such that the parameters $\\theta$ are the first argument,\n",
    "and the data `x` are the second argument.\n",
    "It should return the model prediction.\n",
    "\n",
    "What should the data type of $\\theta$ be?\n",
    "You can decide, as long as it's a built-in Python data type,\n",
    "or NumPy data type, or some combination of."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Define the model in this function\n",
    "def linear_model(theta, x):\n",
    "    pass\n",
    "\n",
    "from dl_workshop.answers import linear_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Initialize linear regression model parameters using random numbers\n",
    "\n",
    "Using a random number generator,\n",
    "such as the `numpy.random.normal` function,\n",
    "write a function that returns \n",
    "random number starting points for each linear model parameter.\n",
    "Make sure it returns params in the form that are accepted by\n",
    "the `linear_model` function defined above.\n",
    "\n",
    "Hint: NumPy's random module (which is distinct from JAX's) has been imported for you in the namespace `npr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_linear_params():\n",
    "    pass\n",
    "\n",
    "# Comment this out if you fill in your answer above.\n",
    "from dl_workshop.answers import initialize_linear_params\n",
    "theta = initialize_linear_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Define the mean squared error loss function with linear model parameters as first argument\n",
    "\n",
    "Now, define the mean squared error loss function, called `mseloss`,\n",
    "such that \n",
    "1. the parameters $\\theta$ are accepted as the first argument,\n",
    "2. `model` function as the second argument,\n",
    "3. `x` as the third argument,\n",
    "4. `y` as the fourth argument, and\n",
    "5. returns a scalar valued result.\n",
    "\n",
    "This is the function we will be differentiating,\n",
    "and JAX's `grad` function will take the derivative of the function w.r.t. the first argument.\n",
    "Thus, $\\theta$ must be the first argument!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Differentiable loss function w.r.t. 1st argument\n",
    "def mseloss(theta, model, x, y):\n",
    "    pass\n",
    "\n",
    "from dl_workshop.answers import mseloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we generate a new function called `dmseloss`, by calling `grad` on `mseloss`!\n",
    "The new function `dmseloss` will have the exact same signature\n",
    "as `mseloss`,\n",
    "but will instead return the value of the gradient\n",
    "evaluated at each of the parameters in $\\theta$,\n",
    "in the same data structure as $\\theta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put your answer here.\n",
    "\n",
    "\n",
    "# The actual dmseloss function is also present in the answers,\n",
    "# but seriously, go fill the one-liner to get dmseloss defined!\n",
    "# If you fill out the one-liner above,\n",
    "# remember to comment out the answer\n",
    "# so that mine doesn't clobber over yours!\n",
    "from dl_workshop.answers import dmseloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've provided an execution of the function below,\n",
    "so that you have an intuition of what's being returned.\n",
    "In my implementation,\n",
    "because theta are passed in as a 2-tuple,\n",
    "the gradients are returned as a 2-tuple as well.\n",
    "The return type will match up with how you pass in the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dmseloss(dict(w=0.3, b=0.5), linear_model, x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Write the optimization routine\n",
    "\n",
    "Finally, write the optimization routine!\n",
    "\n",
    "Make it run for 3,000 iterations,\n",
    "and record the loss on each iteration.\n",
    "Don't forget to update your parameters!\n",
    "(How you do so will depend on how you've set up the parameters.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your optimization routine below.\n",
    "\n",
    "\n",
    "# And if you implemented your optimization loop,\n",
    "# feel free to comment out the next two lines\n",
    "from dl_workshop.answers import model_optimization_loop\n",
    "losses, theta = model_optimization_loop(theta, linear_model, mseloss, x, y, n_steps=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot the loss score over time. It should be going downwards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('mse');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect your parameters to see if they've become close to the true values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap: Ingredients\n",
    "\n",
    "1. Model specification (\"equations\", e.g. $y = wx + b$) and the parameters of the model to be optimized ($w$ and $b$, or more generally, $\\theta$).\n",
    "2. Loss function: tells us how wrong our model parameters are w.r.t. the data ($MSE$)\n",
    "3. Opitmization routine (for-loop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression In Pictures\n",
    "\n",
    "Linear regression can be expressed pictorially, not just in equation form. Here are two ways of visualizing linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Form\n",
    "\n",
    "Linear regression in one dimension looks like this:\n",
    "\n",
    "![](../figures/linreg-scalar.png)\n",
    "\n",
    "Linear regression in higher dimensions looks like this:\n",
    "\n",
    "![](../figures/linreg-matrices.png)\n",
    "\n",
    "This is also known in the statistical world as \"multiple linear regression\".\n",
    "The general idea, though, should be pretty easy to catch.\n",
    "You can do linear regression that projects any arbitrary number of input dimensions\n",
    "to any arbitrary number of output dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Diagram\n",
    "\n",
    "We can draw a \"neural diagram\" based on the matrix view, with the implicit \"identity\" function included in orange.\n",
    "\n",
    "![](../figures/linreg-neural.png) \n",
    "\n",
    "The neural diagram is one that we commonly see in the introductions to deep learning. As you can see here, linear regression, when visualized this way, can be conceptually thought of as the baseline model for understanding deep learning. \n",
    "\n",
    "The neural diagram also expresses the \"compute graph\" that transforms input variables to output variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Break (10 min.)\n",
    "\n",
    "Let's take a break here, and reconvene in 10 minutes.\n",
    "\n",
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Modular Components of Modelling\n",
    "\n",
    "A key idea from this tutorial is to treat the aforementioned four ingredients (data, model, loss, parameters) as being **modular** components of machine learning. This means that we can swap out the model (and its associated parameters) to fit the problem that we have at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Logistic regression builds upon linear regression.\n",
    "We use logistic regression to perform **binary classification**, \n",
    "that is, distinguishing between two classes. \n",
    "Typically, we label one of the classes with the integer 0, \n",
    "and the other class with the integer 1.\n",
    "\n",
    "What does the model look like?\n",
    "To help you build intuition, let's visualize logistic regression using pictures again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Form\n",
    "\n",
    "Here is logistic regression in matrix form.\n",
    "\n",
    "![](../figures/logreg-matrices.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Diagram\n",
    "\n",
    "Now, here's logistic regression in a neural diagram:\n",
    "\n",
    "![](../figures/logreg-neural.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interactive Activity\n",
    "\n",
    "As should be evident from the pictures, \n",
    "logistic regression builds upon linear regression \n",
    "simply by **changing the activation function \n",
    "from an \"identity\" function to a \"logistic\" function**. \n",
    "In the one-dimensional case, \n",
    "it has the same two parameters as one-dimensional linear regression,  $w$ and $b$. \n",
    "Let's use an interactive visualization \n",
    "to visualize how the parameters $w$ and $b$ affect the shape of the curve.\n",
    "\n",
    "(Note: this exercise is best done in a live notebook!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_workshop.answers import logistic\n",
    "logistic??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(w=FloatSlider(value=0, min=-5, max=5), b=FloatSlider(value=0, min=-5, max=5))\n",
    "def plot_logistic(w, b):\n",
    "    x = np.linspace(-10, 10, 1000)\n",
    "    z = w * x + b  # linear transform on x\n",
    "    y = logistic(z)    \n",
    "    plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "1. What do $w$ and $b$ control, respectively? \n",
    "2. Where else do you see logistic-shaped curves?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make simulated data\n",
    "\n",
    "In this section, we're going to show that we can optimize a logistic regression model using the same set of ingredients. \n",
    "\n",
    "First off, let's start by simulating some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 100)\n",
    "w = 2\n",
    "b = 1\n",
    "z = w * x + b + npr.random(size=len(x))\n",
    "y_true = np.round(logistic(z))\n",
    "plt.scatter(x, y_true, alpha=0.3);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Function\n",
    "\n",
    "How would we quantify how good or bad our model is? In this case, we use the logistic loss function, also known as the binary cross entropy loss function.\n",
    "\n",
    "Expressed in equation form, it looks like this:\n",
    "\n",
    "$$L = -\\sum (y \\log(p) + (1-y)\\log(1-p)$$\n",
    "\n",
    "Here:\n",
    "\n",
    "- $y$ is the actual class, namely $1$ or $0$.\n",
    "- $p$ is the predicted class.\n",
    "\n",
    "If you're staring at this equation, and thinking that it looks a lot like the Bernoulli distribution log likelihood, you are right!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "Let's think about the loss function for a moment:\n",
    "\n",
    "- What happens to the term $y \\log(p)$ when $y=0$ and $y=1$? What about the $(1-y)\\log(1-p)$ term?\n",
    "- What happens to both terms when $p \\approx 0$ and when $p \\approx 1$ (but still bounded between 0 and 1)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Write down the logistic regression model\n",
    "\n",
    "Using the same patterns as you did before for the linear model,\n",
    "define a function called `logistic_model`,\n",
    "which accepts parameters `theta` and data `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Define logistic model\n",
    "def logistic_model(theta, x):\n",
    "    pass\n",
    "\n",
    "from dl_workshop.answers import logistic_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Write down the logistic loss function\n",
    "\n",
    "Now, write down the logistic loss function.\n",
    "It is defined as the negative binary cross entropy\n",
    "between the ground truth and the predicted.\n",
    "\n",
    "The binary cross entropy function is available for you to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_workshop.answers import binary_cross_entropy\n",
    "\n",
    "binary_cross_entropy??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Define logistic loss function, using flattened parameters\n",
    "def logistic_loss(p, model, x, y):\n",
    "    pass\n",
    "\n",
    "from dl_workshop.answers import logistic_loss\n",
    "logistic_loss??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now define the gradient of the loss function, using `grad`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise: Define gradient of loss function.\n",
    "dlogistic_loss = grad(logistic_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Initialize logistic regression model parameters using random numbers\n",
    "\n",
    "Because the parameters are identical to linear regression,\n",
    "you probably can use the same `initialize_linear_params` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_workshop.answers import initialize_linear_params\n",
    "\n",
    "theta = initialize_linear_params()\n",
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Write the training loop!\n",
    "\n",
    "This will look very similar to the linear model training loop,\n",
    "because the same two parameters are being optimized.\n",
    "The thing that should change is the loss function and gradient of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_workshop.answers import model_optimization_loop\n",
    "\n",
    "losses, theta = model_optimization_loop(\n",
    "    theta,\n",
    "    logistic_model,\n",
    "    logistic_loss,\n",
    "    x,\n",
    "    y_true,\n",
    "    n_steps=5000,\n",
    "    step_size=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice that the values are off from the true value. Why is this so? Partly it's because of the noise that we added, and we also rounded off values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y_true, alpha=0.3)\n",
    "plt.plot(x, logistic_model(theta, x), color='red');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "What if we did not round off the values, and did not add noise to the original data? Try re-running the model without those two."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "Neural networks are basically very powerful versions of logistic regressions. Like linear and logistic regression, they also take our data and map it to some output, but does so without ever knowing what the true equation form is.\n",
    "\n",
    "That's all a neural network model is: an arbitrarily powerful model.\n",
    "How do feed forward neural networks look like?\n",
    "To give you an intuition for this, let's see one example of a deep neural network in pictures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix diagram\n",
    "\n",
    "As usual, in a matrix diagram.\n",
    "\n",
    "![](../figures/deepnet_regressor-matrices.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural diagram\n",
    "\n",
    "And for correspondence, let's visualize this in a neural diagram.\n",
    "\n",
    "![](../figures/deepnet_regressor-neural.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion\n",
    "\n",
    "- When would we want to use a neural network? When would we not want to use a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Data\n",
    "\n",
    "We are going to try using some real data from the UCI Machine Learning Repository to something related to our work: QSAR modelling!\n",
    "\n",
    "With the dataset below, we want to predict whether a compound is biodegradable based on a series of 41 chemical descriptors.\n",
    "\n",
    "The dataset was taken from: https://archive.ics.uci.edu/ml/datasets/QSAR+biodegradation#. I have also prepared the data such that it is split into X (the predictors) and Y (the class that we are trying to predict), so that you do not have to worry about manipulating pandas DataFrames.\n",
    "\n",
    "Let's read in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X = pd.read_csv(here() / 'data/biodeg_X.csv', index_col=0)\n",
    "y = pd.read_csv(here() / 'data/biodeg_y.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network model definition\n",
    "\n",
    "Now, let's write a neural network model. \n",
    "This neural network model starts with 41 input nodes,\n",
    "has 1 hidden layer with 20 nodes, and 1 output layer with 1 node.\n",
    "Because this is a classification problem,\n",
    "we will use a logistic activation function right at the end.\n",
    "\n",
    "The parameter shapes define the size of the neural network,\n",
    "while the model function determines the transformations.\n",
    "\n",
    "Let's start with the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noise(size):\n",
    "    return npr.normal(size=size)\n",
    "\n",
    "params = dict()\n",
    "params['w1'] = noise((41, 20))\n",
    "params['b1'] = noise((20,))\n",
    "params['w2'] = noise((20, 1))\n",
    "params['b2'] = noise((1,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's define the model as a Python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_network_model(theta, x):\n",
    "    # \"a1\" is the activation from layer 1\n",
    "    a1 = np.tanh(np.dot(x, theta['w1']) + theta['b1'])\n",
    "    # \"a2\" is the activation from layer 2.\n",
    "    # Explain why we need logistic at the end.\n",
    "    a2 = logistic(np.dot(a1, theta['w2']) + theta['b2'])\n",
    "    return a2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization loop\n",
    "\n",
    "Now, write the optimization loop!\n",
    "It will look very similar to the optimization loop\n",
    "that we wrote for the logistic regression classification model.\n",
    "The difference here is the model that is used,\n",
    "as well as the initialized set of parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, params = model_optimization_loop(\n",
    "    params, \n",
    "    neural_network_model, \n",
    "    logistic_loss,\n",
    "    X.values,\n",
    "    y.values,\n",
    "    step_size=0.0001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.title(f\"final loss: {losses[-1]:.2f}\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize trained model performance\n",
    "\n",
    "We can use a confusion matrix to see how \"confused\" a model was.\n",
    "Read more on [Wikipedia](https://en.wikipedia.org/wiki/Confusion_matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "y_pred = neural_network_model(params, X.values)\n",
    "confusion_matrix(y, np.round(y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(confusion_matrix(y, np.round(y_pred)))\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('actual');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recap\n",
    "\n",
    "Deep learning, and more generally any modelling, has the following ingredients:\n",
    "\n",
    "1. A model and its associated parameters to be optimized.\n",
    "2. A loss function against which we are optimizing parameters.\n",
    "3. An optimization routine.\n",
    "\n",
    "You have seen these three ingredients at play with 3 different models: a linear regression model, a logistic regression model, and a deep feed forward neural network model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Pictures\n",
    "\n",
    "Here is a summary of what we've learned in this tutorial!\n",
    "\n",
    "![](../figures/infographic.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caveats of this tutorial\n",
    "\n",
    "Deep learning is an active field of research. I have only shown you the basics here. In addition, I have also intentionally omitted certain aspects of machine learning practice, such as \n",
    "\n",
    "- splitting our data into training and testing sets, \n",
    "- performing model selection using cross-validation\n",
    "- tuning hyperparameters, such as trying out optimizers\n",
    "- regularizing the model, using L1/L2 regularization or dropout\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parting Thoughts\n",
    "\n",
    "Deep learning is nothing more than optimization of a model with a really large number of parameters.\n",
    "\n",
    "In its current state, it is not artificial intelligence.\n",
    "You should not be afraid of it;\n",
    "it is nothing more than a really powerful model that maps X to Y."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-workshop",
   "language": "python",
   "name": "dl-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
