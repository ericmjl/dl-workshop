{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we'll introduce you to some idiomatic JAX tools that will help you write performant numerical array programs. The main takeaways you should get from this notebook are how you can:\n",
    "\n",
    "1. Replace slow Python for loop constructs with fast, just-in-time compiled JAX loop constructs,\n",
    "2. Create deterministic random numbers (and yes, this is not an oxymoron!) for reproducibility,\n",
    "3. Freely mix-and-match through this idea of _composable transforms_.\n",
    "\n",
    "Because contrasts to what we might be used to doing are the the most effective way to teach and learn, in each section, we'll be explicit about what exactly we're replacing when we write these numerical array programs. In doing so, my hope is that you'll see very clearly that structuring your array programs in a composable and atomic fashion will help you take advantage of JAX's composable function transforms to write really fast and compiled functions. And for good measure, we'll contrast this against pure Python programs, so you can witness for yourself how powerful JAX's ideas are... and appreciate how much effort has gone into making the whole thing NumPy compatible!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "To get the most out of this notebook, you need only be familiar with the NumPy API, and writing functions. Having an appreciation of `functools.partial`, will help a bit, because we use it a lot in writing JAX programs. However, I know that not everybody has had prior experience with `partial`-ed functions, so we will introduce the idea mid-way, in a _just-in-time_ fashion as well.\n",
    "\n",
    "If you've gone through `tutorial.ipynb`, which is the main tutorial notebook for this repository, then you'll have some appreciation of JAX's composable transforms. You'll also see how we wrote some loops in there, and hopefully have an appreciation of how much faster things will run when we use JAX's looping constructs instead.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing simple for-loops with `vmap`\n",
    "\n",
    "The first JAX thing we will look at is the `vmap` function. What does `vmap` do? From the [JAX docs on `vmap`](https://jax.readthedocs.io/en/latest/jax.html#jax.vmap):\n",
    "\n",
    "> Vectorizing map. Creates a function which maps fun over argument axes.\n",
    "\n",
    "What does that mean? Well, let's take a look at a few classic examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping an elementwise function over an array's leading axis\n",
    "\n",
    "The first example is mapping a function over an array axis. The simplest example, which is a bit trivial, is doing elementwise application of a function. Say we have uniformly spaced numbers from 0 to 1 in an array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as np\n",
    "from jax import vmap\n",
    "from time import time\n",
    "\n",
    "arr = np.linspace(0, 1, 10000)\n",
    "arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to apply an exponential transform on every element, the \"dumb\", pure Python way to do so is to write a for-loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "new_arr = []\n",
    "for element in arr:\n",
    "    new_arr.append(np.exp(element))\n",
    "new_arr = np.array(new_arr)\n",
    "end = time()\n",
    "print(f\"{end - start:.2f} seconds\")\n",
    "new_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because `np.exp` is a NumPy `ufunc` that operates on individual elements, we can call `np.exp` on `arr` directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "new_arr = np.exp(arr)\n",
    "end = time()\n",
    "print(f\"{end - start:.4f} seconds\")\n",
    "new_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this is much faster.\n",
    "\n",
    "This, incidentally, is equivalent to using `vmap` to map the function across all elements in the array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "new_arr = vmap(np.exp)(arr)\n",
    "end = time()\n",
    "print(f\"{end - start:.4f} seconds\")\n",
    "new_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a bit slower, but one thing we gain from using `vmap` is the ability to ignore the leading (first) array axis of every element that is passed into the `vmap`-ed function. To see that, we're going to look at another example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping a row-wise function across an array's leading axis\n",
    "\n",
    "In this example let's say we have a matrix of values that we measured in an experiment. There were `n_samples` measured, and `3` unique properties that we collected, thereby giving us a matrix of shape `(n_samples, 3)`. If we needed to find their sum, we could do the following in pure NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_sum(data):\n",
    "    \"\"\"Given one dataset, calculate row-wise sum of data.\"\"\"\n",
    "    return np.sum(data, axis=1)\n",
    "\n",
    "data = np.array([\n",
    "    [1, 3, 1,],\n",
    "    [3, 5, 1,],\n",
    "    [1, 2, 5,],\n",
    "    [7, 1, 3,],\n",
    "    [11, 2, 3,],\n",
    "])\n",
    "\n",
    "start = time()\n",
    "result = row_sum(data)\n",
    "end = time()\n",
    "print(f\"{end - start:.4f} seconds\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would give us the correct answer... but we had to worry about the \"axis\" argument, which is a bit irritating. Instead, we could use first transform `np.sum` into a vmapped function that is mapped across the leading axis of `data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_sum_one_data(data):\n",
    "    \"\"\"Given one dataset, calculate row-wise sum of data.\"\"\"\n",
    "    return vmap(np.sum)(data)\n",
    "\n",
    "start = time()\n",
    "result = row_sum_one_data(data)\n",
    "end = time()\n",
    "print(f\"{end - start:.4f} seconds\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thereby giving us the exact same result. While the syntax does take some time to get used to, it does more explicitly and clearly expresses the idea that _we don't really care about summing over the leading axis_.\n",
    "\n",
    "Now, let's say we had multiple datasets for which we wanted to calculate the row-wise sum. How would we do this in pure NumPy?\n",
    "\n",
    "Well, let's first create this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = np.array([\n",
    "    [1, 3, 7,],\n",
    "    [3, 5, 11,],\n",
    "    [3, 2, 5,],\n",
    "    [7, 5, 3,],\n",
    "    [11, 5, 3,],\n",
    "])\n",
    "\n",
    "combined_data = np.moveaxis(np.dstack([data, data2], ), 2, 0)\n",
    "combined_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our shapes tell us that we have 2 stacks of data, each with 5 rows and 3 columns.\n",
    "\n",
    "Since we want row-wise summation, but want to preserve the 2 stacks of data, we have to now worry about which axes to collapse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(combined_data, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all cool, but we now have a \"magic number\" in our program. We can eliminate this magic number by instead doing vmapping `row_sum_over_data` across the `combined_data` array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_sum_all_data(data):\n",
    "    return vmap(row_sum_one_data)(data)\n",
    "    \n",
    "row_sum_all_data(combined_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And voil√†, just like that, magic numbers were removed from our program, and the hierarchical structure of our functions are a bit more explicit:\n",
    "\n",
    "- The elementary function, `np.sum`, operates on a per-row basis.\n",
    "- We map the elementary function across all rows of a single dataset, giving us a higher-order function that calculates row-wise summation for a single dataset, `row_sum_one_data`.\n",
    "- We then map the `row_sum_one_data` across all of the datasets that have been stacked together in a single 3D array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping a function over two arrays simultaneously\n",
    "\n",
    "Let's look at another example. Say we are given two arrays, and we wanted to elementwise multiply them together. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = np.array([1, 2, 3, 4,])\n",
    "a2 = np.array([2, 3, 4, 5,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the NumPy-idiomatic option, we could do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 * a2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option is that we can define a function called `multiply`, which multiplies two scalars together and gives us back another scalar, which we then apply over each element in a `zip` of the two arrays. This is the _extremely_ naive way of handling the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "def multiply(a, b):\n",
    "    return a * b\n",
    "\n",
    "for val1, val2 in zip(a1, a2):\n",
    "    result.append(multiply(val1, val2))\n",
    "np.array(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, if we consider this to be the elementary operation of our function, we could instead multiply them pairwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmap(multiply)(a1, a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we are able to not care about the leading array axis for each array. Once again, we also broke down the problem into its elementary components, and then leveraged `vmap` to build _out_ the program to do what we wanted it to do. (This general pattern will show up!)\n",
    "\n",
    "In general, `vmap`-ing over the _leading_ array axis is the idiomatic thing to do with JAX. It's possibleto `vmap` over other axes, but those are not the defaults. The implication is that we are nudged towards writing programs that at their core begin with an \"elementary\" function that operate \"elementwise\", where the definition of an \"element\" is not necessarily an array element, but problem-dependent. We then progressively `vmap` them outwards on array data structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: `vmap`-ing a dot product over square matrices\n",
    "\n",
    "Let's try getting some practice with the following exercises.\n",
    "\n",
    "The first one is to `vmap` a dot product of a square matrix against itself across a stack of square matrices.\n",
    "\n",
    "An example square matrix called `sq_matrix` is provided for you to jog your memory on how dot products work if you need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random\n",
    "\n",
    "key = random.PRNGKey(42)\n",
    "data = random.normal(key, shape=(11, 5, 5))\n",
    "sq_matrix = random.normal(key, shape=(5, 5))\n",
    "\n",
    "vmap(np.dot)(data, data).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Constructing a more complex program\n",
    "\n",
    "We're going to try our hand at constructing a program that first calculates a cumulative product vector for each row in each dataset, sums them up column-wise across each dataset, and applies this same operation across all datasets stacked together. This one is a bit more challenging!\n",
    "\n",
    "To help you along here, the shape of the data are such:\n",
    "\n",
    "- There are 11 stacks of data.\n",
    "- Each stack of data has 31 rows, and 7 columns.\n",
    "\n",
    "The result of this program still should have 11 stacks and 31 rows, but now each column is not the original data, but the cumulative product of the previous columns.\n",
    "\n",
    "To get this answer write, no magic numbers are allows (e.g. for accessing particular axes). At least two `vmap`s are necessary here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = random.normal(key, shape=(11, 31, 7))\n",
    "\n",
    "def row_wise_cumprod(row):\n",
    "    return np.cumprod(row)\n",
    "\n",
    "def dataset_wise_sum_cumprod(data):\n",
    "    row_cumprods = vmap(row_wise_cumprod)(data)\n",
    "    return vmap(np.sum)(row_cumprods)\n",
    "\n",
    "vmap(dataset_wise_sum_cumprod)(data).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partially evaluating a function\n",
    "\n",
    "We're going to take a quick detour and look at this idea of \"partially evaluating a function\". This is going to be important, as it'll allow us to construct functions that are compatible with the requirements of `vmap` and `lax.scan` and others in JAX, i.e. they have the correct function signature, but still allow us the flexibility to put in arbitrary things that might be needed for the function to work correctly.\n",
    "\n",
    "There are two ways to do this: you can either use `functools.partial`, or you can use function closures. Let's see how to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partially evaluating a function using `functools.partial`\n",
    "\n",
    "For simplicity's sake, let's explore the idea using a function that adds two numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(a, b):\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's say we wanted to fix `b` to the value `3`, thus generating an `add_three` function. We can do this two ways. The first is by `functools.partial`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "add_three = partial(add, b=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now call `add_three` on any value of `a`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_three(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we inspect the function `add_three`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_three?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that `add_three` accepts one _positional_ argument, `a`, and its value of `b` has been set to a default of `3`.\n",
    "\n",
    "What if we wanted to fix `a` to `3` instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_three_v2 = partial(add, a=3)\n",
    "add_three_v2?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how now the function signature has changed, such that `b` is not set while `a` has been. This has implications for how we use the function.\n",
    "\n",
    "Calling the function this way will error out:\n",
    "\n",
    "```python\n",
    ">>> add_three_v2(3)\n",
    "---------------------------------------------------------------------------\n",
    "TypeError                                 Traceback (most recent call last)\n",
    "<ipython-input-109-e78f540eb25e> in <module>\n",
    "----> 1 add_three_v2(3)\n",
    "\n",
    "TypeError: add() got multiple values for argument 'a'\n",
    "```\n",
    "\n",
    "That is because when we pass in the argument with no keyword specified, it is interpreted as the first positional argument, which as you can see, has already been set.\n",
    "\n",
    "On the other hand, calling the function this way will not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_three_v2(b=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partially evaluating a function using closures\n",
    "\n",
    "Another pattern that we can use is to use closures. Closures are functions that return a closed function that contains information from the closing function. Confused? Let me illustrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def closing_function(a):\n",
    "    def closed_function(b):\n",
    "        return a + b\n",
    "    return closed_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this pattern, we can rewrite `add_three` using closures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_add_something(value):\n",
    "    def closed_function(b):\n",
    "        return b + value\n",
    "    return closed_function\n",
    "\n",
    "add_three_v3 = make_add_something(3)\n",
    "add_three_v3(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_three_v3?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you'll notice that the signature of `add_three_v3` follows that exactly of the closed function. \n",
    "\n",
    "When writing array programs using JAX, this is the key design pattern you'll want to implement: Always return a function that has the function signature that you need.\n",
    "\n",
    "Naming things is the hardest activity in programming, because we are giving categorical names to things, and sometimes their category of thing isn't always clear. Fret not: the pattern I'll give you is the following:\n",
    "\n",
    "```python\n",
    "def SOME_FUNCTION_generator(argument1, argument2, keyword_arugment1=default_value1):\n",
    "    \"\"\"To simplify things, just give the name of the closing function <some_function>_generator.\"\"\"\n",
    "    def inner(arg1, arg2, kwarg1=default_value1):\n",
    "        \"\"\"This function should follow the API that is neeed.\"\"\"\n",
    "        return something\n",
    "    return inner\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliminating for-loops that have carry-over using `lax.scan`\n",
    "\n",
    "We are now going to see how we can eliminate for-loops that have carry-over using `lax.scan`.\n",
    "\n",
    "From the JAX docs, `lax.scan` replaces a for-loop with carry-over:\n",
    "\n",
    "> Scan a function over leading array axes while carrying along state.\n",
    "> \n",
    "> ...\n",
    "> \n",
    "> ```python\n",
    "> def scan(f, init, xs, length=None):\n",
    "    if xs is None:\n",
    "         xs = [None] * length\n",
    "    carry = init\n",
    "    ys = []\n",
    "    for x in xs:\n",
    "        carry, y = f(carry, x)\n",
    "        ys.append(y)\n",
    "    return carry, np.stack(ys)\n",
    "> ```\n",
    "\n",
    "A key requirement of the function `f` is that it must have only two positional arguments in there, one for `carry` and one for `x`. You'll see how we can thus apply `functools.partial` to construct functions that have this signature from other functions that have other \n",
    "\n",
    "Let's see some concrete examples of this in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updating a variable with new info on each loop iteration\n",
    "\n",
    "One classic case where we might use a for-loop is in the cumulative sum or product. Here, we need the current loop information to update the information from the previous loop. Let's see it in action for the cumulative sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1, 2, 3, 5, 7, 11, 13, 17])\n",
    "\n",
    "result = []\n",
    "res = 0\n",
    "for el in a:\n",
    "    res += el\n",
    "    result.append(res)\n",
    "np.array(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is identical to the cumulative sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cumsum(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's write it using `lax.scan`, so we can see the pattern in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import lax\n",
    "def scanfunc(res, el):\n",
    "    res = res + el\n",
    "    return res, res  # (\"carryover\", \"accumulated\")\n",
    "\n",
    "result_init = 0\n",
    "final, result = lax.scan(scanfunc, result_init, a)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, scanned function has to return two things:\n",
    "\n",
    "- One object that gets carried over to the next loop (`carryover`), and\n",
    "- Another object that gets \"accumulated\" into an array (`accumulated`).\n",
    "\n",
    "The starting initial value, `result_init`, is passed into the `scanfunc` as `res` on the first call of the `scanfunc`. On subsequent calls, the first `res` is passed back into the `scanfunc` as the new `res`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Simulating compound interest\n",
    "\n",
    "We can use `lax.scan` to generate data that simulates the generation of wealth by compound interest. Here's an implementation using a plain vanilla for-loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wealth_record = []\n",
    "starting_wealth = 100.\n",
    "interest_factor = 1.01\n",
    "\n",
    "prev_wealth = starting_wealth\n",
    "for t in range(100):\n",
    "    new_wealth = prev_wealth * interest_factor\n",
    "    wealth_record.append(prev_wealth)\n",
    "    prev_wealth = new_wealth\n",
    "\n",
    "np.array(wealth_record)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try implementing it in a `lax.scan` form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "starting_wealth = 100.\n",
    "interest_factor = 1.01\n",
    "\n",
    "timesteps = np.arange(100)\n",
    "\n",
    "def make_wealth_at_time_func(interest_factor):\n",
    "    def wealth_at_time(prev_wealth, time):\n",
    "        new_wealth = prev_wealth * interest_factor\n",
    "        return new_wealth, prev_wealth\n",
    "    return wealth_at_time\n",
    "\n",
    "wealth_func = make_wealth_at_time_func(interest_factor)\n",
    "\n",
    "final, result = lax.scan(wealth_func, init=starting_wealth, xs=timesteps)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two are equivalent, so we know we have the `lax.scan` implementation right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Compose `vmap` and `lax.scan` together\n",
    "\n",
    "That was one simulation of wealth generation by compound interest for one individual. Now, let's simulate the wealth generation for different starting wealth levels (you may choose any 300 starting points however you'd like). To do so, you'll likely want to start with a function that accepts a scalar starting wealth and generates the simulated time series from there, and then `vmap` that function across multiple starting points (which is an array itself)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_simulation_func(timesteps):\n",
    "    def inner(starting_wealth):\n",
    "        final, result = lax.scan(wealth_func, init=starting_wealth, xs=timesteps)\n",
    "        return final, result\n",
    "    return inner\n",
    "\n",
    "simulation_func = make_simulation_func(timesteps=np.arange(200))\n",
    "starting_wealth = np.arange(300).astype(float)\n",
    "\n",
    "final, growth = vmap(simulation_func)(starting_wealth)\n",
    "growth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Reproducible Random Number Generation\n",
    "\n",
    "In this section, we'll explore how to create programs that use random number generation in a fashion that is fully deterministic conditioned on a single starting random number generation key.\n",
    "\n",
    "But first, let's explore what happens when we use NumPy's vanilla random number generation protocol to generate numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as onp  # original numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's draw a random number from a Gaussian in NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "onp.random.seed(42)\n",
    "a = onp.random.normal()\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for good measure, let's draw another one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = onp.random.normal()\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is intuitive behaviour, because we expect that each time we call on a random number generator, we should get back a different number from before.\n",
    "\n",
    "However, this behaviour is problematic when we are trying to debug programs, which essentially are deterministic. This is because _stochastically_, we might hit a setting where we encounter an error in our program, and we are unable to reproduce it because we are relying on a random number generator that relies on global state, and hence that doesn't behave in a _fully_ controllable fashion.\n",
    "\n",
    "How then can we get \"the best of both worlds\": random number generation that is controllable?\n",
    "\n",
    "The way that JAX's developers went about doing this is to use pseudo-random number generators that require explicit passing in of a pseudo-random number generation key, rather than relying on a global state being set. Each unique key will deterministically give a unique drawn value explicitly. Let's see that in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = random.PRNGKey(42)\n",
    "\n",
    "a = random.normal(key=key)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show you that passing in the same key gives us the same values as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = random.normal(key=key)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That should already be a stark difference from what you're used to with vanilla NumPy, and this is one key crucial difference between JAX's random module and NumPy's random module. Everything else is very similar, but this is a key difference, and for good reason -- this should hint to you the idea that we can have explicity reproducibility, rather than merely implicit, over our stochastic programs within the same session.\n",
    "\n",
    "How do we get a new draw? Well, we can either create a new key manually, or we can programmatically split the key into two, and use one of the newly split keys to generate a new random number. Let's see that in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1, k2 = random.split(key)\n",
    "c = random.normal(key=k2)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k3, k4, k5 = random.split(k2, num=3)\n",
    "d = random.normal(key=k3)\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By splitting the key into two, three, or even 1000 parts, we can get new keys that are derived from a parent key that generate different random numbers from the same random number generating function.\n",
    "\n",
    "Let's explore how we can use this in the generation of a Gaussian random walk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulating a Gaussian random walk\n",
    "\n",
    "A Gaussian random walk is one where we start at a point that is drawn from a Gaussian, and then we draw another point from a Gausian using the first point as the starting Gaussian point.\n",
    "\n",
    "Does that loop structure sound familiar? Well... yeah, it sounds like a classic `lax.scan` setup!\n",
    "\n",
    "Here's how we might set it up.\n",
    "\n",
    "Firstly, JAX's `random.normal` function doesn't allow us to specify the location and scale, and only gives us a draw from a unit Gaussian. We can work around this, because any unit Gaussian draw can be shifted and scaled to a $N(\\mu, \\sigma)$ by multiplying the draw by $\\sigma$ and adding $\\mu$. \n",
    "\n",
    "To get a length 1000 random draw, we can split the key 1000 ways, and use `lax.scan` to scan a new Gaussian generator across the keys, thereby giving us 1000 unique draws. We then add the old value of the Gaussian to the new draw.\n",
    "\n",
    "We return the tuple (`new_gaussian, old_gaussian`), as we want to have the new gaussian passed into the next iteration, and accumulate the history of the old gaussians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_gaussian(old_gaussian, key):\n",
    "    new_gaussian = random.normal(key) + old_gaussian\n",
    "    return new_gaussian, old_gaussian\n",
    "\n",
    "keys = random.split(key, num=1000)\n",
    "final, result = lax.scan(generate_new_gaussian, 0., keys)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we did it! Definitely looks like a proper Gaussian random walk to me. Let's encapsulate this inside a funciton generator, because the next thing we're going to do is to generate multiple realizations of the Gaussian random walk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gaussian_random_walk_func(num_steps):\n",
    "    def gaussian_random_walk(key):\n",
    "        keys = random.split(key, num=num_steps)\n",
    "        final, result = lax.scan(generate_new_gaussian, 0., keys)\n",
    "        return final, result\n",
    "    return gaussian_random_walk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what if we wanted to generate multiple realizations of the Gaussian random walk? Does this sound familiar? If so... yeah, it's a vanilla for-loop, which directly brings us to `vmap`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_realizations = 200\n",
    "keys = random.split(key, num_realizations)\n",
    "grw_1000_steps = make_gaussian_random_walk_func(1000)\n",
    "final, trajectories = vmap(grw_1000_steps)(keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did it! We have 200 trajectories of a 1000-step Gaussian random walk. Notice also how the program is structured very nicely: Each layer of abstraction in the program corresponds to a new axis dimension along which we are working. The onion layering of the program has very _natural_ structure for the problem at hand.\n",
    "\n",
    "Enough prosyletizing from me, let's visualize the Gaussian random walk to make sure it genuinely is a GRW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for trajectory in trajectories[0:20]:\n",
    "    ax.plot(trajectory)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, note how if you were to re-run the entire program from top-to-bottom again, you would get _exactly the same plot_. This is what we mean by \"reproducible\". Traditional array programs are not fully reproducible, they are only \"kind of\" reproducible in the limit of many runs of the same program. With JAX's random number generation paradigm, any random number generation program is 100% reproducible, down to the level of the exact sequence of random number draws, as long as the seed(s) controlling the program are 100% identical. When an error shows up in a program, as long as its stochastic components are controlled by hand-set seeds, that error is 100% reproducible. For those who have tried working with stochastic programs before, this is an extremely desirable property, as it means we gain the ability to reliably debug our program -- absolutely crucial especially when it comes to working with probabilistic models.\n",
    "\n",
    "Also notice how we finally wrote our first productive for-loop -- but it was only to plot something, not for some form of calculations :)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What do we gain doing this kind of composition?\n",
    "\n",
    "To help us get a handle over what kind of gains we get, I'm going to do a comparison between composed `lax.scan` and `vmaps` against a program that we might write in pure Python versus our compiled version."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Writing a Gaussian random walk in pure Python\n",
    "\n",
    "Let's start with a pure Python implementation of a Gaussian random walk, leveraging vanilla NumPy's random module for API convenience only (and not for performance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_random_walk_python(num_realizations, num_timesteps):\n",
    "    rws = []\n",
    "    for i in range(num_realizations):\n",
    "        rw = []\n",
    "        prev_draw = 0\n",
    "        for t in range(num_timesteps):\n",
    "            prev_draw = onp.random.normal(loc=prev_draw)\n",
    "            rw.append(prev_draw)\n",
    "        rws.append(rw)\n",
    "    return rws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_REALIZATIONS = 500\n",
    "N_TIMESTEPS = 10_000\n",
    "start = time()\n",
    "trajectories_python = gaussian_random_walk_python(N_REALIZATIONS, N_TIMESTEPS)\n",
    "end = time()\n",
    "print(f\"{end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trajectory in trajectories_python[:20]:\n",
    "    plt.plot(trajectory)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison against our JAX program\n",
    "\n",
    "Let's now compare the program against the version we wrote above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_random_walk_jax(num_realizations, num_timesteps):\n",
    "    keys = random.split(key, num_realizations)\n",
    "    grw_k_steps = make_gaussian_random_walk_func(num_timesteps)\n",
    "    final, trajectories = vmap(grw_k_steps)(keys)\n",
    "    return final, trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "final_jax, trajectories_jax = gaussian_random_walk_jax(N_REALIZATIONS, N_TIMESTEPS)\n",
    "trajectories_jax.block_until_ready()\n",
    "end = time()\n",
    "print(f\"{end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trajectory in trajectories_jax[:20]:\n",
    "    plt.plot(trajectory)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare against a JIT-compiled version of our JAX program\n",
    "\n",
    "Now we're going to JIT-compile our Gaussian Random Walk function and see how long it takes for the program to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jit\n",
    "\n",
    "def gaussian_random_walk_jit(num_realizations, num_timesteps):\n",
    "    keys = random.split(key, num_realizations)\n",
    "    grw_k_steps = make_gaussian_random_walk_func(num_timesteps)\n",
    "    grw_k_steps = jit(grw_k_steps)\n",
    "    final, trajectories = vmap(grw_k_steps)(keys)\n",
    "    return final, trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "final_jit, trajectories_jit = gaussian_random_walk_jit(N_REALIZATIONS, N_TIMESTEPS)\n",
    "trajectories_jit.block_until_ready()\n",
    "end = time()\n",
    "print(f\"{end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for trajectory in trajectories_jit[:20]:\n",
    "    plt.plot(trajectory)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JIT-compilation gave us about a 1-2X speedup over non-JIT compiled code, and was about 20X faster than the pure Python version. That shouldn't surprise you one bit :)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A few pointers on syntax\n",
    "\n",
    "Firstly, if we subscribe to the Zen of Python's notion that \"flat is better than nested\", then following the idioms listed here -- closures/partials, `vmap` and `lax.scan`, then we'll likely only ever go one closure deep into our programs. Notice how we basically never wrote any for-loops in our array code; they were handled elegantly by the looping constructs `vmap` and `lax.scan`. \n",
    "\n",
    "Secondly, using `jit`, we get further optimizations on our code for free. A pre-requisite of `jit` is that the _every_ function call made in the program function being `jit`-ed is required to be written in a \"pure functional\" style, i.e. there are no side effects, no mutation of global state. If you write a program using the idioms used here (closures to wrap state, `vmap`/`lax.scan` in lieu of loops, explicit random number generation using PRNGKeys), then you will be able to JIT compile the program with ease."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-workshop",
   "language": "python",
   "name": "dl-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
