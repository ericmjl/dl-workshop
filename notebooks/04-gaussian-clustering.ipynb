{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian mixture model-based clustering\n",
    "\n",
    "In this notebook, we are going to take a look at how to cluster Gaussian-distributed data.\n",
    "\n",
    "Imagine you have data that are multi-modal.\n",
    "A task of interest, naturally, is to cluster that data.\n",
    "Let's see how we can accomplish this using nothing but the NumPy API and some gradients from JAX. (And maybe some vmaps too!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate mixture gaussians\n",
    "\n",
    "As always, when exploring a method,\n",
    "we start with a highly simplified version of it\n",
    "that contains a ton of constraints that we know of,\n",
    "which we can always break later.\n",
    "Thus, we'll start with a simple version of our problem:\n",
    "the setting where we have bimodal Gaussian data,\n",
    "and we want to identify the cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import vmap, grad\n",
    "import jax.numpy as np\n",
    "from jax.scipy import stats\n",
    "from jax import random\n",
    "from jax.scipy.special import logsumexp\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by describing how the data are generated. We'll set it up such that there are two cluster centers: one at -2 and one at 3. We'll also impose a 1:5 ratio of data between those two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "weights_true = np.array([1, 5])\n",
    "locs_true = np.array([-2., 5.])\n",
    "scale_true = np.array([1.1, 2])\n",
    "\n",
    "base_n_draws = 1000\n",
    "key = random.PRNGKey(100)\n",
    "\n",
    "k1, k2 = random.split(key)\n",
    "\n",
    "draws_1 = scale_true[0] * random.normal(k1, shape=(base_n_draws * weights_true[0],)) + locs_true[0]\n",
    "draws_2 = scale_true[1] * random.normal(k2, shape=(base_n_draws * weights_true[1],)) + locs_true[1]\n",
    "data_mixture = np.concatenate([draws_1, draws_2])\n",
    "plt.hist(data_mixture);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our learning task at hand is thus to learn the two cluster centers, and their relative weighting.\n",
    "\n",
    "When faced with probabilistically-generated data, it is oftentimes desirable to impose a likelihood distribution\n",
    "on the observed data.\n",
    "Doing so gives us a quantitative measure of \"goodness of fit\" for our parameters.\n",
    "Here, because we observe bimodal data, we might hazard a guess that\n",
    "a two-component mixture distribution likelihood would be good. \n",
    "\n",
    "Here, the likelihood of the each data point is the sum of the likelihood of each data point under each of the components. In other words, for each data point, we calculate the likelihood of observing that datum under each component's distribution, adjust the likelihood by multiplying it by the weight, and sum up the component weights. Because we assume that each data point is independently drawn, we therefore multiply each datum's likelihood to get the joint likelihood of all data observed.\n",
    "\n",
    "To see some of the JAX programming idioms in action, we are going to build things up from the core as usual.\n",
    "\n",
    "Let's write the log likelihood of one datum under one component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_workshop.gaussian_mixture import loglike_one_component\n",
    "\n",
    "loglike_one_component??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summation here is because we are operating in logarithmic space.\n",
    "\n",
    "You might ask, why do we use \"log\" of the component scale,\n",
    "and why do we use the \"logit\" of the component probability?\n",
    "This is a math trick that helps us whenever we are doing computations in an unbounded space.\n",
    "When doing gradient descent,\n",
    "we can never guarantee that a gradient update on a parameter that ought to be positive-only\n",
    "will give us a positive number.\n",
    "Thus, for positive numbers, we operate in logarithmic space.\n",
    "\n",
    "\n",
    "We can quickly write a test here. If the component probability is 1.0, the component $\\mu$ is 0, and the observed datum is also 0, it should equal to the log-likelihood of 0 under a unit Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loglike_one_component(\n",
    "    component_weight=1.0, \n",
    "    component_mu=0., \n",
    "    log_component_scale=1., \n",
    "    datum=0.) == (\n",
    "    stats.norm.logpdf(x=0, loc=0, scale=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leveraging what we know now, let's write a function that calculates the total log likelihood of our data\n",
    "under all of the component probability distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.special import logit\n",
    "\n",
    "def normalize_weights(weights):\n",
    "    \"\"\"Normalize a weights vector to sum to 1.\"\"\"\n",
    "    return weights / np.sum(weights)\n",
    "\n",
    "def loglike_across_components(\n",
    "    log_component_weights,\n",
    "    component_mus,\n",
    "    log_component_scales,\n",
    "    datum\n",
    "):\n",
    "    \"\"\"Log likelihood of datum under all components of the mixture.\"\"\"\n",
    "    component_weights = normalize_weights(\n",
    "        np.exp(log_component_weights)\n",
    "    )\n",
    "    loglike_components = vmap(\n",
    "        partial(\n",
    "            loglike_one_component,\n",
    "            datum=datum\n",
    "        )\n",
    "    )(component_weights, component_mus, log_component_scales)\n",
    "    return logsumexp(loglike_components)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside that function, we first calculated elementwise the log-likelihood of observing that data under each component.\n",
    "That only gives us per-component log-likelihoods though.\n",
    "Because our data could have been drawn from any of those components,\n",
    "the total likelihood is a _sum_ of the per-component likelihoods.\n",
    "Thus, we have to elementwise exponentiate the log-likelihoods, \n",
    "Now, we have sum up each of those probability components together,\n",
    "so we have to use the [logsumexp](https://en.wikipedia.org/wiki/LogSumExp) function,\n",
    "which first exponentiates each of the probabilities,\n",
    "sums them up,\n",
    "and then takes their log again.\n",
    "(We could have written our own version of the function,\n",
    "but I think it makes a ton of sense\n",
    "to trust the numerically-stable,\n",
    "professionally-implemented version provided\n",
    "in SciPy!\n",
    "Let us now test-drive this function,\n",
    "which should give us a scalar value at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_bad = np.array([1 - 0.0001, 0.0001])\n",
    "weights_bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loglike_across_components(\n",
    "    log_component_weights=np.log(weights_true),\n",
    "    component_mus=locs_true,\n",
    "    log_component_scales=np.log(scale_true),\n",
    "    datum=data_mixture[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, that worked!\n",
    "\n",
    "Now that we've got the log-likelihood of each datum under each component,\n",
    "we can now `vmap` the function across all data given to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mixture_loglike(log_component_weights, component_mus, log_component_scales, data):\n",
    "    \"\"\"Log likelihood of data (not datum!) under all components of the mixture.\"\"\"\n",
    "    ll_per_data = vmap(\n",
    "        partial(\n",
    "            loglike_across_components,\n",
    "            log_component_weights,\n",
    "            component_mus,\n",
    "            log_component_scales\n",
    "        )\n",
    "    )(data)\n",
    "    return np.sum(ll_per_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_weights_true = np.log(weights_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixture_loglike(\n",
    "    log_component_weights=np.log(weights_true),\n",
    "    component_mus=locs_true,\n",
    "    log_component_scales=np.log(scale_true),\n",
    "    data=data_mixture,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we play around with the mixture loglike though, we'll notice that it isn't the end of the story.\n",
    "The component weights can be \"hacked\" to produce higher log-likelihood values,\n",
    "by minimizing one of the components.\n",
    "We need thus need to postulate a generative story for the weights,\n",
    "which will provide an anchoring distribution.\n",
    "One reasonable thing is to postulate that it came from a Dirichlet distribution\n",
    "that gave equal weight across each of the components.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_loglike(log_component_weights):\n",
    "    component_weights = np.exp(log_component_weights)\n",
    "    component_weights = normalize_weights(component_weights)\n",
    "    return stats.dirichlet.logpdf(x=component_weights, alpha=2 * np.ones_like(component_weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_loglike(log_weights_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_bad = np.array([3., 2.])\n",
    "log_weights_bad = np.log(weights_bad)\n",
    "weights_loglike(log_weights_bad), weights_loglike(log_weights_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have composed together our generative story for the data,\n",
    "let's pause for a moment and break down our model a bit.\n",
    "This will serve as a review of what we've done.\n",
    "\n",
    "Firstly, we have our \"model\", i.e. the log-likelihood of our data\n",
    "conditioned on some parameter set and their values.\n",
    "\n",
    "Secondly, our parameters to tweak and adjust to find maximum likelihood values for are:\n",
    "\n",
    "1. Component weights.\n",
    "2. Component central tendencies/means\n",
    "3. Component scales/variances.\n",
    "\n",
    "What we're going to attempt next is to optimize those parameters, leveraging what we've learned before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent to find maximum likelihood values\n",
    "\n",
    "Using JAX's optimizers, we're always interested in finding the minima of a function.\n",
    "However, we're faced with a _maximum_ likelihood problem.\n",
    "We can get around the problem by simply inverting the sign of our problem.\n",
    "Let's see this in action.\n",
    "\n",
    "As always, we begin with a loss function to minimize, and its derivative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad\n",
    "\n",
    "def loss(params, data):\n",
    "    log_component_weights, component_mus, log_component_scales = params\n",
    "    loglike_mixture = mixture_loglike(\n",
    "        log_component_weights,\n",
    "        component_mus,\n",
    "        log_component_scales,\n",
    "        data\n",
    "    )\n",
    "    loglike_weights = weights_loglike(log_component_weights)\n",
    "    \n",
    "    total = loglike_mixture + loglike_weights\n",
    "    return -total\n",
    "\n",
    "dloss = grad(loss)\n",
    "dloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we initialize our three parameters with random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_MIXTURE_COMPONENTS = 2\n",
    "\n",
    "k1, k2, k3, k4 = random.split(key, 4)\n",
    "log_component_weights_init = random.normal(k1, shape=(N_MIXTURE_COMPONENTS,))\n",
    "component_mus_init = random.normal(k2, shape=(N_MIXTURE_COMPONENTS,))\n",
    "log_component_scales_init = random.normal(k3, shape=(N_MIXTURE_COMPONENTS,))\n",
    "\n",
    "params_init = log_component_weights_init, component_mus_init, log_component_scales_init\n",
    "params_true = np.log(weights_true), locs_true, np.log(scale_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now test-drive the functions to make sure they execute properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(params_true, data_mixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(params_init, data_mixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dloss(params_init, data_mixture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, in contrast to vanilla `grad`, `value_and_grad` also gives us\n",
    "the loss as the first element in the tuple.\n",
    "\n",
    "Now, we are going to use JAX's optimizers inside a `lax.scan`-ed training loop\n",
    "to get fast training going.\n",
    "\n",
    "We begin with the elementary \"step\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(i, state, get_params_func, dloss_func, update_func, data):\n",
    "    params = get_params_func(state)\n",
    "    g = dloss_func(params, data)\n",
    "    state = update_func(i, g, state)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then make the elementary step function a scannable one using `lax.scan`.\n",
    "This will allow us to \"scan\" the function across an array\n",
    "that represents the number of optimization steps we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_step_scannable(get_params_func, dloss_func, update_func, data):\n",
    "    def inner(previous_state, iteration):\n",
    "        new_state = step(\n",
    "            i=iteration,\n",
    "            state=previous_state,\n",
    "            get_params_func=get_params_func,\n",
    "            dloss_func=dloss_func,\n",
    "            update_func=update_func,\n",
    "            data=data,\n",
    "        )\n",
    "        return new_state, previous_state\n",
    "    return inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we actually instantiate the scannable step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental.optimizers import adam\n",
    "\n",
    "adam_init, adam_update, adam_get_params = adam(0.5)\n",
    "\n",
    "step_scannable = make_step_scannable(\n",
    "    get_params_func=adam_get_params,\n",
    "    dloss_func=dloss,\n",
    "    update_func=adam_update,\n",
    "    data=data_mixture, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we `lax.scan` `step_scannable` over 1000 iterations (constructed as an `np.arange()` array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import lax\n",
    "\n",
    "initial_state = adam_init(params_init)\n",
    "\n",
    "final_state, state_history = lax.scan(step_scannable, initial_state, np.arange(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now unpack our parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt = adam_get_params(final_state)\n",
    "log_component_weights_opt, component_mus_opt, log_component_scales_opt = params_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first check that we indeed have \"learned\". The loss function value should be pretty darn close to the loss function when we put in true params.\n",
    "Keep in mind that because we have data that are an imperfect sample of the ground truth distribution,\n",
    "it is possible that our optimized params' negative log likelihood will be lower than that of the true params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss(params_opt, data_mixture), loss(params_true, data_mixture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up: what do the component probabilities look like? Do they reflect what we expect?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(log_component_weights_opt), weights_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's so rad! We're at the 1:5 ratio that was prescribed at the beginning!\n",
    "\n",
    "And how about the component means?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_mus_opt, locs_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also really close! And finally, the component scales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(log_component_scales_opt), scale_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very nice, really close to the ground truth too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's visualize the mixture distributions.\n",
    "\n",
    "We're going to visualize the mixture distributions to help us get a handle over what exactly happened during training.\n",
    "\n",
    "To start, we need a function that plots the mixture distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy.stats import norm\n",
    "\n",
    "def plot_component_norm_pdfs(log_component_weights, component_mus, log_component_scales, xmin, xmax, ax, title):\n",
    "    component_weights = normalize_weights(np.exp(log_component_weights))\n",
    "    component_scales = np.exp(log_component_scales)\n",
    "    x = np.linspace(xmin, xmax, 1000).reshape(-1,1)\n",
    "    pdfs = component_weights * norm.pdf(x, loc=component_mus, scale=component_scales)\n",
    "    for component in range(pdfs.shape[1]):\n",
    "        ax.plot(x, pdfs[:, component])\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
    "# Plot ground truth\n",
    "plot_component_norm_pdfs(np.log(weights_true), locs_true, np.log(scale_true), -7.5, 10, axes[0], title=\"Ground Truth\")\n",
    "\n",
    "# Plot initialized\n",
    "plot_component_norm_pdfs(\n",
    "    log_component_weights_init,\n",
    "    component_mus_init,\n",
    "    log_component_scales_init,\n",
    "    xmin=-7.5,\n",
    "    xmax=10,\n",
    "    ax=axes[1],\n",
    "    title=\"Initialized\",\n",
    ")\n",
    "\n",
    "# Plot optimized\n",
    "plot_component_norm_pdfs(\n",
    "    log_component_weights_opt, \n",
    "    component_mus_opt,\n",
    "    log_component_scales_opt,\n",
    "    xmin=-7.5,\n",
    "    xmax=10,\n",
    "    ax=axes[2],\n",
    "    title=\"Optimized\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning over history\n",
    "\n",
    "Let's also take a look at the mixture PDFs over training iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    log_component_weights_history,\n",
    "    component_mus_history,\n",
    "    log_component_scales_history\n",
    ") = adam_get_params(state_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from celluloid import Camera\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "cam = Camera(fig)\n",
    "\n",
    "for w, m, s in zip(log_component_weights_history[::10], component_mus_history[::10], log_component_scales_history[::10]):\n",
    "    ax.hist(data_mixture, bins=40, normed=True, color=\"blue\")\n",
    "    plot_component_norm_pdfs(\n",
    "        w, m, s, xmin=-20, xmax=20, ax=ax, title=None,\n",
    "    )\n",
    "    cam.snap()\n",
    "    \n",
    "animation = cam.animate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(animation.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's some comments to be said on the dynamics here:\n",
    "\n",
    "1. At first, one Gaussian is used to approximate over the entire distribution. It's not a good fit, but approximates it fine enough.\n",
    "1. However, our optimization routine continues to push forward, eventually finding the bimodal pattern. Once this happens, the PDFs fit very nicely to the data samples.\n",
    "\n",
    "This phenomena is also reflected in the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loss(state):\n",
    "    params = adam_get_params(state)\n",
    "    loss_score = loss(params, data_mixture)\n",
    "    return loss_score\n",
    "\n",
    "losses = vmap(get_loss)(state_history)\n",
    "plt.plot(losses)\n",
    "plt.yscale(\"log\");\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice the first plateau, followed by the second plateau.\n",
    "This corresponds to the two phases of learning.\n",
    "\n",
    "Now, thus far, we have set up the problem in a fashion that is essentially \"trivial\".\n",
    "What if, however, we wanted to try fitting a mixture Gaussian where we didn't know exactly how many mixture components there _ought_ to be?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generalizing this to \"unknown\" numbers of modes\n",
    "\n",
    "We're going to see how we can generalize this to an \"unknown\" number of modes.\n",
    "\n",
    "To make the problem a bit harder, we'll start by expanding our data to have more mixture components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_true = np.array([1, 5, 0.9, 3])\n",
    "locs_true = np.array([-2., -5., 3., 8.])\n",
    "scale_true = np.array([1.1, 2, 1., 1.5,])\n",
    "\n",
    "base_n_draws = 1000\n",
    "\n",
    "keys = random.split(key, 4)\n",
    "\n",
    "draws = []\n",
    "for i in range(4):\n",
    "    shape = int(base_n_draws * weights_true[i]),\n",
    "    draw = scale_true[i] * random.normal(keys[i], shape=shape) + locs_true[i]\n",
    "    draws.append(draw)\n",
    "data_mixture = np.concatenate(draws)\n",
    "plt.hist(data_mixture);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the histogram, it should be easy to tell that this is not going to be an easy problem to solve.\n",
    "Firstly, the mixture distributions in _reality_ have 4 components.\n",
    "But what we get looks more like 2 components... or really?\n",
    "Could it be that we're lying by using a histogram?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data_mixture, bins=40);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha! The case against histograms reveals itself. Turns out there's lots of problems using histograms, and I shan't go deeper into them here, but obscuring data is one of those issues. To learn more, I wrote [a blog post on the matter](https://ericmjl.github.io/blog/2018/7/14/ecdfs/).\n",
    "\n",
    "Let us now go back to pretending that we don't know the _actual_ number of mixture components. How would we handle this situation?\n",
    "\n",
    "One practical way to handle this is to provide a very large number of possible component weights, and then let the optimization routine figure out how to get us to the maximum likelihood estimation of each Gaussian components' weights, means, and variances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_MIXTURE_COMPONENTS = 20\n",
    "\n",
    "k1, k2, k3 = random.split(key, 3)\n",
    "log_component_weights_init = random.normal(k1, shape=(N_MIXTURE_COMPONENTS,))\n",
    "component_mus_init = random.normal(k2, shape=(N_MIXTURE_COMPONENTS,))\n",
    "log_component_scales_init = random.normal(k3, shape=(N_MIXTURE_COMPONENTS,))\n",
    "\n",
    "params_init = log_component_weights_init, component_mus_init, log_component_scales_init\n",
    "params_true = np.log(weights_true), locs_true, np.log(scale_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jit\n",
    "initial_state = adam_init(params_init)\n",
    "\n",
    "final_state, state_history = lax.scan(jit(step_scannable), initial_state, np.arange(10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt = adam_get_params(final_state)\n",
    "log_component_weights_opt, component_mus_opt, log_component_scales_opt = params_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
    "# Plot ground truth\n",
    "plot_component_norm_pdfs(np.log(weights_true), locs_true, np.log(scale_true), -10, 10, axes[0], title=\"Ground Truth\")\n",
    "\n",
    "# Plot initialized\n",
    "plot_component_norm_pdfs(\n",
    "    log_component_weights_init,\n",
    "    component_mus_init,\n",
    "    log_component_scales_init,\n",
    "    xmin=-10,\n",
    "    xmax=10,\n",
    "    ax=axes[1],\n",
    "    title=\"Initialized\",\n",
    ")\n",
    "\n",
    "# Plot optimized\n",
    "plot_component_norm_pdfs(\n",
    "    log_component_weights_opt, \n",
    "    component_mus_opt,\n",
    "    log_component_scales_opt,\n",
    "    xmin=-10,\n",
    "    xmax=10,\n",
    "    ax=axes[2],\n",
    "    title=\"Optimized\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    log_component_weights_history,\n",
    "    component_mus_history,\n",
    "    log_component_scales_history\n",
    ") = adam_get_params(state_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from celluloid import Camera\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "cam = Camera(fig)\n",
    "\n",
    "for w, m, s in zip(log_component_weights_history[::100], component_mus_history[::100], log_component_scales_history[::100]):\n",
    "    ax.hist(data_mixture, bins=40, normed=True, color=\"blue\")\n",
    "    plot_component_norm_pdfs(\n",
    "        w, m, s, xmin=-20, xmax=20, ax=ax, title=None,\n",
    "    )\n",
    "    cam.snap()\n",
    "    \n",
    "animation = cam.animate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(animation.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = vmap(get_loss)(state_history)\n",
    "plt.plot(losses)\n",
    "plt.yscale(\"log\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When I look at the mixture distribution PDFs generated from the optimized weights,\n",
    "I see something a tad unsatisfactory.\n",
    "Our optimization routine has given us a mix of Gaussians that struggle to model the ground truth data convincingly.\n",
    "By occam's razor, we would want to find the _parsimonious_ set of mixture components that give us our data,\n",
    "i.e. assign those components the largest amount of weight, and assign vanishingly small weights to the rest.\n",
    "In other words, we should be able to do better on this learning task.\n",
    "\n",
    "If that sounds appealing to you, then read on. We're going to walk into the world of Bayesian non-parametrics!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirichlet Process Priors\n",
    "\n",
    "From the previous section, it appeared that simply providing a large number of Gaussian mixture components, initialized with random weighting, was insufficient for learning the true number of Gaussian components (at least in simulated data). We need a better way of approaching the problem.\n",
    "\n",
    "We'll try formulating the problem slightly differently. Earlier on, we evaluated the likelihood of our weights matrix under a Dirichlet distribution with equally-distributed concentrations. The prior of equally distributed concentrations reflects our belief that a every component could contribute more or less equally to the observed data. If instead we wanted to express the prior belief that a constrained set of components were responsible for the data, we need a Dirichlet process prior with a concentration term that governs how many components to give weighting to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dirichlet Processes\n",
    "\n",
    "A Dirichlet process expresses the idea that there are an infinite number of possible states. It is governed by a \"concentration\" parameter, which specifies how \"concentrated\" probability mass is assigned across the infinite number of states. From a practical perspective, though, we don't use \"infinite\" states, but rather a \"countably large number\" of states, just as we did above. \n",
    "\n",
    "Let's first explore how to generate a Dirichlet-distributed set of weights by using the \"stick-breaking\" process.\n",
    "The key idea is simple.\n",
    "We take a length 1 stick, draw a probability value from a Beta distribution, break the length 1 stick into two at the point drawn, and record the left side's value. We then take the right side, draw another probability value from a Beta distribution again, break that stick into two portions at the point drawn. and record the absolute length of the left side's value, and break the right side again. We repeat this until we have the countably large number of states that we desire. In code, the process looks like a `lax.scan`-ed function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stick_breaking_weights(beta_draws):\n",
    "    \"\"\"Return weights from a stick breaking process.\n",
    "    \n",
    "    :param beta_draws: i.i.d draws from a Beta distribution.\n",
    "        This should be a row vector.\n",
    "    \"\"\"\n",
    "    def weighting(occupied_probability, beta_i):\n",
    "        \"\"\"\n",
    "        :param occupied_probability: The cumulative occupied probability taken up.\n",
    "        :param beta_i: Current value of beta to consider.\n",
    "        \"\"\"\n",
    "        weight = (1 - occupied_probability) * beta_i\n",
    "        return occupied_probability + weight, weight\n",
    "    \n",
    "    occupied_probability, weights = lax.scan(weighting, np.array(0.), beta_draws)\n",
    "    \n",
    "    weights = weights / np.sum(weights)\n",
    "    return occupied_probability, weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize what one draw with 50 possible slots looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concentration = 3\n",
    "beta_draws = random.beta(key=key, a=1, b=concentration, shape=(50,))\n",
    "occupied_probability, weights = stick_breaking_weights(beta_draws)\n",
    "plt.plot(weights);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see here, we have most of the probability mass concentrated on the first few states.\n",
    "\n",
    "If we plotted multiple draws from the same concentration value, what might it look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_draws = random.beta(key=key, a=1, b=concentration, shape=(20, 50))\n",
    "occupied_probability, weights = vmap(stick_breaking_weights)(beta_draws)\n",
    "\n",
    "import seaborn as sns\n",
    "sns.heatmap(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is visible, over 20 realizations, most of the probability mass is concentrated in the first few states.\n",
    "\n",
    "Now, what if we wanted to see the effect of varying concentration? This is another `vmap`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concentrations = np.array([0.5, 1, 3, 5, 10, 20])\n",
    "\n",
    "def weights_one_concentration(concentration, num_draws, num_components):\n",
    "    beta_draws = random.beta(key=key, a=1, b=concentration, shape=(num_draws, num_components))\n",
    "    occupied_probability, weights = vmap(stick_breaking_weights)(beta_draws)\n",
    "    return occupied_probability, weights\n",
    "\n",
    "occupied_probabilities, weights = vmap(partial(weights_one_concentration, num_draws=20, num_components=50))(concentrations)\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`weights` is now a matrix of size (6, 20, 50), which corresponds to 6 concentrations, 20 i.i.d draws each, with 50 component weights available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(3*3, 3*2))\n",
    "\n",
    "for ax, weights_mat, conc in zip(axes.flatten(), weights, concentrations):\n",
    "    sns.heatmap(weights_mat, ax=ax)\n",
    "    ax.set_title(f\"Concentration = {conc}\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we increase the concentration value, the probabilities get more diffuse.\n",
    "\n",
    "From this forward process of generating Dirichlet-distributed weights,\n",
    "instead of evaluating the log likelihood of the component weights\n",
    "under a \"fixed\" Dirichlet distribution prior,\n",
    "we can instead evaluate it under a Dirichlet process with a \"concentration\" prior.\n",
    "The requirement here is that we be able to recover correctly the i.i.d. Beta draws\n",
    "that generated the Dirichlet process weights.\n",
    "\n",
    "Let's try that out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beta_draw_from_weights(weights, tol=1e-8):\n",
    "    def beta_from_w(accounted_probability, weights_i):\n",
    "        \"\"\"\n",
    "        :param accounted_probability: The cumulative probability acounted for.\n",
    "        :param weights_i: Current value of weights to consider.\n",
    "        \"\"\"\n",
    "        denominator = 1 - accounted_probability\n",
    "        log_denominator = np.log(denominator)\n",
    "        \n",
    "        log_beta_i = np.log(weights_i) - log_denominator\n",
    "\n",
    "        newly_accounted_probability = accounted_probability + weights_i\n",
    "        \n",
    "        return newly_accounted_probability, np.exp(log_beta_i)\n",
    "    final, betas = lax.scan(beta_from_w, np.array(0.), weights)\n",
    "    return final, betas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concentration = 3\n",
    "beta_draws = random.beta(key=key, a=1, b=concentration, shape=(50,))\n",
    "occupied_probability, weights = stick_breaking_weights(beta_draws)\n",
    "final, beta_hat = beta_draw_from_weights(weights)\n",
    "plt.plot(beta_draws, label=\"original\")\n",
    "plt.plot(beta_hat, label=\"inferred\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is visible from the plot above, we were able to recover about 1/2 to 2/3 of the weights\n",
    "before the divergence in the two curves shows up.\n",
    "\n",
    "One of the difficulties that we have is that when we get back the observed weights in real life,\n",
    "we have no access to how much of the length 1 \"stick\" is leftover.\n",
    "This alongside numerical underflow issues arising from small numbers\n",
    "means we can only use about 1/2 of the drawn weights\n",
    "to recover the Beta-distributed draws\n",
    "from which we can evaluate our log likelihoods.\n",
    "Let's try performing that evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def component_probs_loglike(log_component_probs, log_concentration):\n",
    "    \"\"\"\n",
    "    :param log_concentration: Real-valued scalar.\n",
    "    \"\"\"\n",
    "    concentration = np.exp(log_concentration)\n",
    "    component_probs = normalize_weights(np.exp(log_component_probs))\n",
    "    _, beta_draws = beta_draw_from_weights(component_probs)\n",
    "    num_components = np.floor(len(beta_draws) / 2).astype(np.int32)\n",
    "    return np.sum(stats.beta.logpdf(x=beta_draws[:num_components], a=1, b=concentration))\n",
    "\n",
    "component_probs_loglike(np.log(weights), log_concentration=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, let's build up our understanding by seeing how the log likelihood of our weights\n",
    "under an assumed Dirichlet process from a Beta distribution\n",
    "changes as we vary the concentration parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_concentrations = np.linspace(-3, 3, 10000)\n",
    "logps = vmap(partial(component_probs_loglike, np.log(weights)))(log_concentrations)\n",
    "plt.plot(np.exp(log_concentrations), logps)\n",
    "plt.xlabel(\"concentration\")\n",
    "plt.ylabel(\"logp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks quite good. Let's see if we can visualize how the log probability changes with multiple weights draws from a Dirichlet process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_draws = 20\n",
    "num_components = 50\n",
    "concentration = 3\n",
    "beta_draws = random.beta(key=key, a=1, b=concentration, shape=(num_draws, num_components))\n",
    "occupied_probability, weights = vmap(stick_breaking_weights)(beta_draws)\n",
    "\n",
    "def logp_curve(log_weights_vector, log_concentrations):\n",
    "    \"\"\"Logp curve for one weights vector.\"\"\"\n",
    "    logps = vmap(partial(component_probs_loglike, log_weights_vector))(log_concentrations)\n",
    "    return logps\n",
    "\n",
    "logps = vmap(partial(logp_curve, log_concentrations=log_concentrations))(np.log(weights))\n",
    "\n",
    "for logp in logps:\n",
    "    plt.plot(np.exp(log_concentrations), logp)\n",
    "    plt.xlabel(\"concentration\")\n",
    "    plt.ylabel(\"logp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Over multiple realizations of weights, we see that we should be able to approximately recover the true concentration value\n",
    "if we used gradient descent.\n",
    "This gives us hope!\n",
    "\n",
    "Let us now write down the log likelihood for the full probabilistic model.\n",
    "We can leverage some of the components we have already written before.\n",
    "`mixture_loglike` is the one that we want to start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_loglike(log_component_weights, log_concentration, component_mus, log_component_scales, observed_data):\n",
    "    \n",
    "    # logpdf of weights under concentrations prior\n",
    "    logp_weights = component_probs_loglike(log_component_weights, log_concentration)\n",
    "    \n",
    "    logp_observed_data = mixture_loglike(log_component_weights, component_mus, log_component_scales, observed_data)\n",
    "    return logp_weights + logp_observed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_loss(params, data):\n",
    "    log_component_weights, log_concentration, component_mus, log_component_scales = params\n",
    "    \n",
    "    nll = -joint_loglike(*params, observed_data=data)\n",
    "    \n",
    "    return nll + np.squeeze(log_concentration ** 2)\n",
    "\n",
    "djoint_loss = grad(joint_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1, k2, k3, k4 = random.split(key, 4)\n",
    "n_components = 50\n",
    "\n",
    "log_component_weights_init = random.normal(k1, shape=(n_components,))\n",
    "log_concentration_init = random.normal(k2, shape=(1,))\n",
    "component_mus_init = random.normal(k3, shape=(n_components,))\n",
    "log_component_scales_init = random.normal(k4, shape=(n_components,))\n",
    "\n",
    "params_init = log_component_weights_init, log_concentration_init, component_mus_init, log_component_scales_init\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_loss(params_init, data_mixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_init, adam_get_params, adam_update = adam(0.005)\n",
    "step_scannable = make_step_scannable(\n",
    "    get_params_func=adam_get_params,\n",
    "    dloss_func=djoint_loss,\n",
    "    update_func=adam_update,\n",
    "    data=data_mixture, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_state = adam_init(params_init)\n",
    "\n",
    "final_state, state_history = lax.scan(step_scannable, initial_state, np.arange(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_history = adam_get_params(state_history)\n",
    "params_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'd like to learn the concentration parameter for the component probs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we can calculate the component logpdfs, let's jointly look at them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_loss(params, data):\n",
    "    log_component_probs, log_concentration, component_mus = params\n",
    "    component_probs = np.exp(log_component_probs)\n",
    "    \n",
    "    # component probability distribution logpdf against beta distribution\n",
    "    comp_probs_logp = component_probs_logpdf(component_probs, log_concentration)\n",
    "    \n",
    "    # mixture distribution logpdf\n",
    "    mixture_logp = mixture_loglike(component_probs, component_mus, data)\n",
    "    \n",
    "    total_logp = comp_probs_logp + mixture_logp\n",
    "    regularization = np.power(log_concentration, 4)\n",
    "    return -total_logp + regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "djoint_loss = grad(joint_loss)\n",
    "\n",
    "concentration_init = 3.\n",
    "\n",
    "params_init = log_component_probs_init, np.log(concentration_init), component_mus_init\n",
    "joint_loss(params_init, observed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt = optimize_params(params_init, data_mixture, djoint_loss, n_iter=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_component_probs_opt, log_concentration_opt, component_mus_opt = params_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_probs_opt = np.exp(log_component_probs_opt)\n",
    "component_probs_opt = component_probs_opt / component_probs_opt.sum()\n",
    "component_probs_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_mus_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concentration_opt = np.exp(log_concentration_opt)\n",
    "concentration_opt, concentration_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_component_norm_pdfs(component_probs_opt, component_mus_opt, -10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_component_norm_pdfs(np.exp(log_component_probs_init), component_mus_init, -10, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-workshop",
   "language": "python",
   "name": "dl-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
