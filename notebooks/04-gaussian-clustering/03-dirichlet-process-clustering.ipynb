{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying Dirichlet-processes to mixture-model clustering\n",
    "\n",
    "Over the previous two sections, we learned about Dirichlet processes and Gaussian Mixture Model-based clustering. In this section, we're going to put the two concepts together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A data problem with unclear number of modes\n",
    "\n",
    "Let's start with a data problem that is a bit trickier to solve: one that has multiple numbers of modes,\n",
    "but for which the mixture distribution visually obscures the true number of modes present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import numpy as np, random, vmap, jit, grad, lax\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_true = np.array([2, 10, 1, 6])\n",
    "locs_true = np.array([-2., -5., 3., 8.])\n",
    "scale_true = np.array([1.1, 2, 1., 1.5,])\n",
    "\n",
    "base_n_draws = 1000\n",
    "key = random.PRNGKey(42)\n",
    "keys = random.split(key, 4)\n",
    "\n",
    "draws = []\n",
    "for i in range(4):\n",
    "    shape = int(base_n_draws * weights_true[i]),\n",
    "    draw = scale_true[i] * random.normal(keys[i], shape=shape) + locs_true[i]\n",
    "    draws.append(draw)\n",
    "data_mixture = np.concatenate(draws)\n",
    "plt.hist(data_mixture);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the histogram, it should be easy to tell that this is not going to be an easy problem to solve.\n",
    "Firstly, the mixture distributions in _reality_ have 4 components.\n",
    "But what we get looks more like 2 components... or really?\n",
    "Could it be that we're lying by using a histogram?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data_mixture, bins=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha! The case against histograms reveals itself. Turns out there's lots of problems using histograms, and I shan't go deeper into them here, but obscuring data is one of those issues. To learn more, I wrote [a blog post on the matter](https://ericmjl.github.io/blog/2018/7/14/ecdfs/).\n",
    "\n",
    "In any case, this situation is a clear one where the distribution shape clearly masks the number of mixture components. How can we get around this?\n",
    "\n",
    "Here, we can turn to Dirichlet processes as a tool to help us.\n",
    "Because DPs don't impose an _exact_ number of _significant_ categories on us,\n",
    "but instead allow us to control their number probabilistically with a single \"concentration\" parameter,\n",
    "we can instead write down a model to learn the:\n",
    "\n",
    "1. concentration parameters,\n",
    "1. optimal relative weighting of components, conditioned on concentration parameters,\n",
    "1. distribution parameters for each component, conditioned on data.\n",
    "\n",
    "This effectively forms a **Dirichlet-Process Gaussian Mixture Model**.\n",
    "\n",
    "Let's see this in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirichlet-Process Gaussian Mixture Model (DP-GMM)\n",
    "\n",
    "The DP-GMM model presumes an infinite (or countably large) number of states,\n",
    "with one Gaussian available per state.\n",
    "The first thing we need to do is to write down the joint log-likelihood of every parameter in our model.\n",
    "As always, before we write down that joint log-likelihood,\n",
    "the first thing we must do is correctly specify what the data generating process is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generating process for a DP-GMM\n",
    "\n",
    "This could be our data generating process:\n",
    "\n",
    "1. We start with a large number of states, and for each one, their likelihood of ocurring is goverened by a concentration parameter.\n",
    "2. With each state and their corresponding probabilities, we draw a number from the corresponding mixture Gaussian.\n",
    "3. That number's likelihood is proportional to the state from which it was drawn.\n",
    "\n",
    "With this idea in hand, we can start composing together the joint log-likelihood of the model, conditioned on its parameters and data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-likelihood for the component weights\n",
    "\n",
    "The first piece we need to compose together is the component weights.\n",
    "We have that already defined!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_workshop.gaussian_mixture import component_probs_loglike\n",
    "\n",
    "component_probs_loglike??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To quickly recap what this is: it's the log likelihood of a categorical probability vector under a Dirichlet process with a specified concentration parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-likelihood for the Gaussian mixture\n",
    "\n",
    "The second piece we need is the Gaussian mixture log-likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_workshop.gaussian_mixture import mixture_loglike\n",
    "\n",
    "mixture_loglike??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to recap this one really quickly: this is the log likelihood of the observed data under each of the component weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint log-likelihood\n",
    "\n",
    "Put together, the joint log-likelihood of the Gaussian mixture model is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_loglike(\n",
    "    log_component_weights,\n",
    "    log_concentration,\n",
    "    num_components,\n",
    "    component_mus,\n",
    "    log_component_scales,\n",
    "    data\n",
    "):\n",
    "    component_probs = np.exp(log_component_weights)\n",
    "    probs_ll = component_probs_loglike(\n",
    "        log_component_weights,\n",
    "        log_concentration,\n",
    "        num_components\n",
    "    )\n",
    "\n",
    "    mix_ll = mixture_loglike(\n",
    "        log_component_weights,\n",
    "        component_mus,\n",
    "        log_component_scales,\n",
    "        data\n",
    "    )\n",
    "\n",
    "    return probs_ll + mix_ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through log likelihood function, \n",
    "we are expressing the dependence of the mixture Gaussians on the component probs,\n",
    "and the dependence of the component probs on the concentration parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "We can now begin optimizing our mixture model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "As always, we define the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_joint_loss(num_components):\n",
    "    def inner(params, data):\n",
    "        (\n",
    "            log_component_weights,\n",
    "            log_concentration,\n",
    "            component_mus,\n",
    "            log_component_scales\n",
    "        ) = params\n",
    "\n",
    "        ll = joint_loglike(\n",
    "            log_component_weights,\n",
    "            log_concentration,\n",
    "            num_components,\n",
    "            component_mus,\n",
    "            log_component_scales,\n",
    "            data,\n",
    "        )\n",
    "        return -ll\n",
    "    return inner\n",
    "\n",
    "joint_loss = make_joint_loss(num_components=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The closure pattern is here, so that we can set the number of components to use for Dirichlet estimation\n",
    "without making it part of the params to optimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient function\n",
    "\n",
    "We then define the gradient function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "djoint_loss = grad(joint_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because I know these work, I am going to skip over test-driving them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "We'll now start by initializing our parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1, k2, k3, k4 = random.split(key, 4)\n",
    "n_components = 50\n",
    "\n",
    "log_component_weights_init = random.normal(k1, shape=(n_components,))\n",
    "log_concentration_init = random.normal(k2, shape=(1,))\n",
    "component_mus_init = random.normal(k3, shape=(n_components,))\n",
    "log_component_scales_init = random.normal(k4, shape=(n_components,))\n",
    "\n",
    "params_init = log_component_weights_init, log_concentration_init, component_mus_init, log_component_scales_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop\n",
    "\n",
    "Now we write the training loop, leveraging the functions we had before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental.optimizers import adam\n",
    "from dl_workshop.gaussian_mixture import make_step_scannable\n",
    "\n",
    "adam_init, adam_update, adam_get_params = adam(0.05)\n",
    "step_scannable = make_step_scannable(\n",
    "    get_params_func=adam_get_params,\n",
    "    dloss_func=djoint_loss,\n",
    "    update_func=adam_update,\n",
    "    data=data_mixture, \n",
    ")\n",
    "step_scannable = jit(step_scannable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run training\n",
    "\n",
    "Finally, we train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "start = time()\n",
    "initial_state = adam_init(params_init)\n",
    "N_STEPS = 10000\n",
    "\n",
    "final_state, state_history = lax.scan(step_scannable, initial_state, np.arange(N_STEPS))\n",
    "end = time()\n",
    "print(f\"Time taken: {end - start:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize training\n",
    "\n",
    "We're going to make the money figure first. Let's visualize the evolution of the mixture Gaussians over training iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_history = adam_get_params(state_history)\n",
    "log_component_weights_history, log_concentration_history, component_mus_history, log_component_scales_history = params_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_workshop.gaussian_mixture import animate_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "params_for_plotting = [log_component_weights_history, component_mus_history, log_component_scales_history]\n",
    "animation = animate_training(params_for_plotting, int(N_STEPS / 200), data_mixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(animation.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for the losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "joint_loss = jit(joint_loss)\n",
    "losses = []\n",
    "for w, c, m, s in zip(log_component_weights_history, log_concentration_history, component_mus_history, log_component_scales_history):\n",
    "    prm = (w, c, m, s)\n",
    "    l = joint_loss(prm, data_mixture)\n",
    "    losses.append(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_workshop.gaussian_mixture import normalize_weights\n",
    "params_opt = adam_get_params(final_state)\n",
    "log_component_weights_opt = params_opt[0]\n",
    "component_weights_opt = np.exp(log_component_weights_opt)\n",
    "plt.plot(normalize_weights(component_weights_opt), marker=\"o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we are able to recover the major components, in the correct proportions!\n",
    "\n",
    "If you remembered what the data looked like in 1 dimension, there were basically only 3 majorly-identifiable components. Given enough training iterations (we had to go to 10,000 iterations), our trained model was able to identify all of them, while assigning insignificant probability mass to the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some caveats\n",
    "\n",
    "While the main point of this chapter was to show you that it _is_ possible\n",
    "to use gradient-based optimization to cluster data,\n",
    "the same caveats that apply to GMM-based clustering also apply here.\n",
    "\n",
    "For example, label switching is prominent: the components that are prominent may switch\n",
    "at any time during the gradient descent process.\n",
    "If you observed the video carefully, you would see that in action too.\n",
    "When it comes to MCMC for fully Bayesian inference, this is a problem.\n",
    "With maximum likelihood estimation using gradient descent, however,\n",
    "this is less of an issue, as we usually only end up taking the final optimized parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "The primary purpose of this notebook was to show you\n",
    "that gradient descent is not _only_ for supervised machine learning,\n",
    "but also for unsupervised learning.\n",
    "More generally,\n",
    "gradients can be used anywhere there is an \"optimization\" problem setup.\n",
    "In this case, identifying clusters of data in a mixture model\n",
    "is a classic _unsupervised_ machine learning problem,\n",
    "but because we cast it in the form of a log-likelihood optimization problem,\n",
    "we were able to leverage gradients to solve this problem.\n",
    "\n",
    "Aside from that, we saw the JAX idioms in action: `vmap`, `lax.scan`, `grad`, `jit` and more.\n",
    "Once again, `vmap` and `lax.scan` replaced\n",
    "many of the for-loops that we might have otherwise written,\n",
    "`grad` gave us easy access to gradients,\n",
    "and `jit` gave us the advantage of compilation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-workshop",
   "language": "python",
   "name": "dl-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
