{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian mixture model-based clustering\n",
    "\n",
    "In this notebook, we are going to take a look at how to cluster Gaussian-distributed data.\n",
    "\n",
    "Imagine you have data that are multi-modal, something that looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as np\n",
    "from jax import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "weights_true = np.array([1, 5])  # 1:5 ratio\n",
    "locs_true = np.array([-2., 5.])  # different means\n",
    "scale_true = np.array([1.1, 2])  # different variances\n",
    "\n",
    "base_n_draws = 1000\n",
    "key = random.PRNGKey(100)\n",
    "\n",
    "k1, k2 = random.split(key)\n",
    "\n",
    "draws_1 = scale_true[0] * random.normal(k1, shape=(base_n_draws * weights_true[0],)) + locs_true[0]\n",
    "draws_2 = scale_true[1] * random.normal(k2, shape=(base_n_draws * weights_true[1],)) + locs_true[1]\n",
    "data_mixture = np.concatenate([draws_1, draws_2])\n",
    "plt.hist(data_mixture, bins=40);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihoods of Mixture Data\n",
    "\n",
    "We might look at this data and say, \"I think there's two clusters of data here.\" One that belongs to the left mode, and one that belongs to the right mode. By visual inspection, the relative weighting might be about 1:3 to 1:6, or somewhere in between."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What might be the \"data generating process\" here?\n",
    "\n",
    "Well, we could claim that when a data point is drawn from the mixture distribution, it could have come from _either_ of the modes.\n",
    "By basic probability logic, the joint likelihood of observing the data point is:\n",
    "\n",
    "- The likelihood that the datum came from the left Gaussian, times the probability of drawing a number from the left Gaussian, plus...\n",
    "- The likelihood that the datum came from the right Gaussian, times the probability of drawing a number from the right Gaussian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phrased more generally:\n",
    "\n",
    "> The sum over \"components $j$ of the likelihood that the datum $x_i$ came from Gaussian $j$ with parameters $\\mu_j, \\sigma_j$ times the likelihood of observing a draw from component $j$.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In math, we would need to calculate:\n",
    "\n",
    "$$\\sum_j P(x_i|\\mu_j, \\sigma_j) P(\\mu_j, \\sigma_j|w_j) P(w_j)$$\n",
    "\n",
    "Now, we can make the middle term $P(\\mu_j, \\sigma_j|w_j)$ is always 1, by assuming that the $\\mu_j$ and $\\sigma_j$ chosen are always fixed given the component weight chosen. The expression then simplifies to:\n",
    "\n",
    "$$\\sum_j P(x_i|\\mu_j, \\sigma_j) P(w_j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Likelihood of One Datum under One Component\n",
    "\n",
    "Because this is a summation, let's work out the elementary steps first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_workshop.gaussian_mixture import loglike_one_component\n",
    "\n",
    "loglike_one_component??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summation here is because we are operating in logarithmic space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might ask, why do we use \"log\" of the component scale?\n",
    "This is a math trick that helps us whenever we are doing computations in an unbounded space.\n",
    "When doing gradient descent,\n",
    "we can never guarantee that a gradient update on a parameter that ought to be positive-only\n",
    "will give us a positive number.\n",
    "Thus, for positive numbers, we operate in logarithmic space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can quickly write a test here. \n",
    "If the component probability is 1.0, \n",
    "the component $\\mu$ is 0, and the observed datum is also 0, \n",
    "it should equal to the log-likelihood of 0 \n",
    "under a unit Gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.scipy import stats\n",
    "\n",
    "our_test = loglike_one_component(\n",
    "    component_weight=1.0, \n",
    "    component_mu=0., \n",
    "    log_component_scale=np.log(1.), \n",
    "    datum=0.)\n",
    "\n",
    "ground_truth = (\n",
    "    stats.norm.logpdf(x=0, loc=0, scale=1)\n",
    ")\n",
    "\n",
    "our_test, ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Likelihood of One Datum under All Components\n",
    "\n",
    "Now that we are done with the elementary computation of one datum under one component, we can `vmap` the log-likelihood calculation over all components, thereby giving us the loglikelihood of a datum under any of the possible given components.\n",
    "\n",
    "Firstly, we need a function that normalizes component weights to sum to 1. This is enforced just in case during the gradient descent procedure, we end up with weights that do not sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_workshop.gaussian_mixture import normalize_weights, loglike_across_components\n",
    "\n",
    "normalize_weights??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we leverage the `normalize_weights` function inside a `loglike_across_components` function, which `vmap`s the log likelihood calculation across components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loglike_across_components??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside that function, we first calculated elementwise the log-likelihood of observing that data under each component.\n",
    "That only gives us per-component log-likelihoods though.\n",
    "Because our data could have been drawn from any of those components,\n",
    "the total likelihood is a _sum_ of the per-component likelihoods.\n",
    "Thus, we have to elementwise exponentiate the log-likelihoods first.\n",
    "Because we have sum up each of those probability components together,\n",
    "a shortcut function we have access to is the [logsumexp](https://en.wikipedia.org/wiki/LogSumExp) function,\n",
    "which first exponentiates each of the probabilities,\n",
    "sums them up,\n",
    "and then takes their log again,\n",
    "thereby accomplishing what we need.\n",
    "\n",
    "We could have written our own version of the function,\n",
    "but I think it makes a ton of sense\n",
    "to trust the numerically-stable,\n",
    "professionally-implemented version provided\n",
    "in SciPy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice to pass in `log_component_weights` rather than `weights` is because the `normalize_weights` function assumes that all numbers in the vector are positive, but in gradient descent, we operate in an unbounded space, which may bring us into negative numbers. To make things safe, we assume the numbers come to us from an unbounded space, and then use an exponential transform first before normalizing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now test-drive our `loglike_across_components` function,\n",
    "which should give us a scalar value at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loglike_across_components(\n",
    "    log_component_weights=np.log(weights_true),\n",
    "    component_mus=locs_true,\n",
    "    log_component_scales=np.log(scale_true),\n",
    "    datum=data_mixture[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, that worked!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Likelihood of All Data under All Components\n",
    "\n",
    "Now that we've got the log-likelihood of each datum under each component,\n",
    "we can now `vmap` the function across all data given to us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mathematically, this would be:\n",
    "\n",
    "$$\\prod_i \\sum_j P(x_i|\\mu_j, \\sigma_j) P(w_j)$$\n",
    "\n",
    "Or in prose:\n",
    "\n",
    "> The total likelihood of all datum $x_i$ together under all components $j$ is given by first summing the likelihoods of each datum $x_i$ under each component $j$, and then taking the product of likelihoods for each data point $x_i$, assuming data are i.i.d. from the mixture distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_workshop.gaussian_mixture import mixture_loglike\n",
    "\n",
    "mixture_loglike??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we `vmap`-ed the `loglike_across_components` function over all data points provided in the function above. This helped us eliminate a for-loop, basically!\n",
    "\n",
    "If we execute the function, we should get a scalar value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixture_loglike(\n",
    "    log_component_weights=np.log(weights_true),\n",
    "    component_mus=locs_true,\n",
    "    log_component_scales=np.log(scale_true),\n",
    "    data=data_mixture,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log Likelihood of Weighting\n",
    "\n",
    "The final thing we are missing is a generative story for the weights. In other words, we are asking the question, \"How did the weights come about?\"\n",
    "\n",
    "We might say that the weights were drawn from a Dirichlet distribution (the generalization of a Beta distribution to multiple dimensions), and as a na√Øve first pass, were drawn with equal probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_workshop.gaussian_mixture import weights_loglike\n",
    "\n",
    "weights_loglike??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_prior = 2 * np.ones_like(weights_true)\n",
    "weights_loglike(np.log(weights_true), alpha_prior=alpha_prior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review thus far\n",
    "\n",
    "Now that we have composed together our generative story for the data,\n",
    "let's pause for a moment and break down our model a bit.\n",
    "This will serve as a review of what we've done.\n",
    "\n",
    "Firstly, we have our \"model\", i.e. the log-likelihood of our data\n",
    "conditioned on some parameter set and their values.\n",
    "\n",
    "Secondly, our parameters of the model are:\n",
    "\n",
    "1. Component weights.\n",
    "2. Component central tendencies/means\n",
    "3. Component scales/variances.\n",
    "\n",
    "What we're going to attempt next is to use gradient based optimization to learn what those parameters are, conditioned on data, leveraging the JAX idioms that we've learned before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent to find maximum likelihood values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a mixture Gaussian dataset, one natural task we might want to do is estimate the weights, central tendencies/means and scales/variances from data. This corresponds naturally to a maximum likelihood estimation task.\n",
    "\n",
    "Now, one thing we know is that JAX's optimizers assume we are _minimizing_ a function, so to use JAX's optimizers with a maximum likelihood function, we simply take the negative of the log likelihood and minimize that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "Let's first take a look at the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_workshop.gaussian_mixture import loss_mixture_weights\n",
    "\n",
    "loss_mixture_weights??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, our function is designed to be compatible with JAX's `grad`. We are taking derivatives w.r.t. the first argument, the parameters, which we unpack into our likelihood function parameters.\n",
    "\n",
    "The two likelihood functions are used inside there too:\n",
    "\n",
    "- `mixture_loglike`\n",
    "- `weights_loglike`\n",
    "\n",
    "The `alpha_prior` is hard-coded; it's not the most ideal. For convenience, I have just hard-coded it, but the principled way to handle this is to add it as a keyword argument that gets passed in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient of loss function\n",
    "\n",
    "As usual, we now define the gradient function of `loss_mixture_weights` by calling `grad` on it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad\n",
    "\n",
    "dloss_mixture_weights = grad(loss_mixture_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Initialization\n",
    "\n",
    "Next up, we initialize our parameters randomly. For convenience, we'll use Gaussian draws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_MIXTURE_COMPONENTS = 2\n",
    "\n",
    "k1, k2, k3, k4 = random.split(key, 4)\n",
    "log_component_weights_init = random.normal(k1, shape=(N_MIXTURE_COMPONENTS,))\n",
    "component_mus_init = random.normal(k2, shape=(N_MIXTURE_COMPONENTS,))\n",
    "log_component_scales_init = random.normal(k3, shape=(N_MIXTURE_COMPONENTS,))\n",
    "\n",
    "params_init = log_component_weights_init, component_mus_init, log_component_scales_init\n",
    "params_true = np.log(weights_true), locs_true, np.log(scale_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, you see JAX's controllable handling of random numbers. Our parameters are always going to be initialized in exactly the same way on each notebook cell re-run, since we have explicit keys passed in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-drive functions\n",
    "\n",
    "Let's test-drive the functions to make sure that they work correctly.\n",
    "\n",
    "For the loss function, we should expect to get back a scalar. If we pass in initialized parameters, it should also have a higher value (corresponding to more lower log likelihood) than if we pass in true parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mixture_weights(params_true, data_mixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mixture_weights(params_init, data_mixture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, both criteria are satisfied. \n",
    "\n",
    "Test-driving the gradient function should give us a tuple of gradients evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dloss_mixture_weights(params_init, data_mixture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining performant training loops\n",
    "\n",
    "Now, we are going to use JAX's optimizers inside a `lax.scan`-ed training loop\n",
    "to get fast training going.\n",
    "\n",
    "We begin with the elementary \"step\" function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_workshop.gaussian_mixture import step\n",
    "\n",
    "step??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should look familiar to you. At each step of the loop, we unpack params from a JAX optimizer state, obtain gradients, and then update the state using the gradients.\n",
    "\n",
    "We then make the elementary step function a scannable one using `lax.scan`.\n",
    "This will allow us to \"scan\" the function across an array\n",
    "that represents the number of optimization steps we will be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_workshop.gaussian_mixture import make_step_scannable\n",
    "\n",
    "make_step_scannable??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the inner function that gets returned here has the API that we require for using `lax.scan`: \n",
    "\n",
    "- `previous_state` corresponds to the `carry`, and\n",
    "- `iteration` corresponds to the `x`.\n",
    "\n",
    "Now we actually instantiate the scannable step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental.optimizers import adam\n",
    "\n",
    "adam_init, adam_update, adam_get_params = adam(0.5)\n",
    "\n",
    "step_scannable = make_step_scannable(\n",
    "    get_params_func=adam_get_params,\n",
    "    dloss_func=dloss_mixture_weights,\n",
    "    update_func=adam_update,\n",
    "    data=data_mixture, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we `lax.scan` `step_scannable` over 1000 iterations (constructed as an `np.arange()` array)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import lax\n",
    "\n",
    "initial_state = adam_init(params_init)\n",
    "\n",
    "final_state, state_history = lax.scan(step_scannable, initial_state, np.arange(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity-checking whether learning has happened\n",
    "\n",
    "We can sanity check whether learning has happened.\n",
    "\n",
    "The loss function value for optimized parameters should be pretty close to the loss function when we put in true params.\n",
    "(Do keep in mind that because we have data that are an imperfect sample of the ground truth distribution,\n",
    "it is possible that our optimized params' negative log likelihood will be different than that of the true params.)\n",
    "\n",
    "Firstly, we unpack the parameters of the final state:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt = adam_get_params(final_state)\n",
    "log_component_weights_opt, component_mus_opt, log_component_scales_opt = params_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we look at the loss for the optimized params:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mixture_weights(params_opt, data_mixture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should be lower than the loss for the initialized params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_mixture_weights(params_init, data_mixture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed that is so!\n",
    "\n",
    "And if we inspect the component weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(log_component_weights_opt), weights_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, we have optimized our parameters such that they are close to the original 1:5 ratio!\n",
    "\n",
    "And for our component means?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_mus_opt, locs_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Really close too!\n",
    "\n",
    "Finally, for the component scales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(log_component_scales_opt), scale_true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very nice, really close to the ground truth too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing training dynamics\n",
    "\n",
    "Let's now visualize how training went.\n",
    "\n",
    "I have created a function called `animate_training`, which will provide for us a visual representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "animate_training??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`animate_training` leverages [`celluloid`][celluloid] to make easy matplotlib animations. You can check out the package [here][celluloid].\n",
    "\n",
    "[celluloid]: https://github.com/jwkvam/celluloid\n",
    "\n",
    "We can now call on `animate_training` to give us an animation of the mixture Gaussian PDFs as we trained the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from dl_workshop.gaussian_mixture import animate_training\n",
    "\n",
    "params_history = adam_get_params(state_history)\n",
    "\n",
    "animation = animate_training(params_history, 10, data_mixture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML(animation.to_html5_video())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's some comments to be said on the dynamics here:\n",
    "\n",
    "1. At first, one Gaussian is used to approximate over the entire distribution. It's not a good fit, but approximates it fine enough.\n",
    "1. However, our optimization routine continues to push forward, eventually finding the bimodal pattern. Once this happens, the PDFs fit very nicely to the data samples.\n",
    "\n",
    "This phenomena is also reflected in the loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_workshop.gaussian_mixture import get_loss\n",
    "get_loss??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because `states_history` is the result of `lax.scan`-ing, we can `vmap` our `get_loss` function over the `states_history` object to get back an array of losses that can then be plotted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import vmap\n",
    "from functools import partial\n",
    "\n",
    "losses = vmap(partial(get_loss, get_params_func=adam_get_params, loss_func=loss_mixture_weights, data=data_mixture))(state_history)\n",
    "plt.plot(losses)\n",
    "plt.yscale(\"log\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice the first plateau, followed by the second plateau.\n",
    "This corresponds to the two phases of learning.\n",
    "\n",
    "Now, thus far, we have set up the problem in a fashion that is essentially \"trivial\".\n",
    "What if, however, we wanted to try fitting a mixture Gaussian where we didn't know exactly how many mixture components there _ought_ to be?\n",
    "\n",
    "To check that out, head over to the next section in this chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-workshop",
   "language": "python",
   "name": "dl-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
