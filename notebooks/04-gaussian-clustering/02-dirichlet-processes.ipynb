{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dirichlet Processes: A simulated guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In the previous section, we saw how we could fit a two-component Gaussian mixture model\n",
    "to data that looked like it had just two components.\n",
    "In many real-world settings, though, we oftentimes do not know\n",
    "_exactly_ how many components are present,\n",
    "so one way we can approach the problem is to assume\n",
    "that there are an _infinite_ (or \"countably large\") number of components available\n",
    "for our model to pick from,\n",
    "but we \"guide\" our model to focus its attention on only a small number of components provided.\n",
    "\n",
    "Does that sound magical? It sure did for me when I first heard about this possibility.\n",
    "The key modelling component that we need\n",
    "is a process for creating _infinite_ numbers of mixture weight components\n",
    "from a single controllable parameter,\n",
    "and that naturally gives us a **Dirichlet process**,\n",
    "which we will look at in this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are Dirichlet processes?\n",
    "\n",
    "To quote from [Wikipedia's article on DPs][wikidp]:\n",
    "\n",
    "[wikidp]: https://en.wikipedia.org/wiki/Dirichlet_process\n",
    "\n",
    "> In probability theory, Dirichlet processes (after Peter Gustav Lejeune Dirichlet) are a family of stochastic processes whose realizations are probability distributions.\n",
    "\n",
    "Hmm, now that doesn't look very concrete.\n",
    "Is there a more concrete way to think about DPs?\n",
    "Turns out, the answer is yes!\n",
    "\n",
    "At its core, each realization/draw from a DP provides an infinite (or, in computing world, a \"large\") set of weights that sum to 1.\n",
    "Remember that: A long vector of numbers that sum to 1,\n",
    "which we can interpret as a probability distribution over sets of weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulating a Dirichlet Process using \"stick-breaking\"\n",
    "\n",
    "We're going to look at one way to construct a probability vector, the \"stick-breaking\" process.\n",
    "\n",
    "How does it work? At its core, it looks like this, a very simple idea.\n",
    "\n",
    "1. We take a length 1 stick, draw a probability value from a Beta distribution, break the length 1 stick into two at the point drawn, and record the left side's value.\n",
    "1. We then take the right side, draw another probability value from a Beta distribution again, break that stick proportionally into two portions at the point drawn, and record the absolute length of the left side's value\n",
    "1. We then braek the right side again, using the same process.\n",
    "\n",
    "We repeat this until we have the countably large number of states that we desire. \n",
    "\n",
    "In code, this looks like a loop with a carryover from the previous iteration, which means it is a `lax.scan`-able function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_workshop.gaussian_mixture import stick_breaking_weights\n",
    "stick_breaking_weights??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, in the inner function `weighting`,\n",
    "we first calculate the `weight` associated with the \"left side\" of the stick,\n",
    "which we record down and accumulate as the \"history\" (second tuple element of the return).\n",
    "Our `carry` is the `occupied_probability + weight`,\n",
    "which we can use to calculate the length of the right side of the stick (`1 - occupied_probability`).\n",
    "\n",
    "Because each `beta_i` is an i.i.d. draw from `beta_draws`,\n",
    "we can pre-instantiate a vector of `beta_draws`\n",
    "and then `lax.scan` the `weighting` function over the vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beta distribution crash-course\n",
    "\n",
    "Because on computers it's hard to deal with _infinitely-long_ arrays,\n",
    "we can instead instantiate a \"countably large\" array of `beta_draws`.\n",
    "\n",
    "Now, the `beta_draws`, need to be i.i.d. from a source Beta distribution,\n",
    "which has two parameters, `a` and `b`,\n",
    "and gives us a continuous distribution over the interval $(0, 1)$.\n",
    "Because of the nature of `a` and `b` corresponding to `success` and `failure` weights:\n",
    "\n",
    "- higher `a` at constant `b` shifts the distribution closer to 1,\n",
    "- higher `b` at constant `a` shifts the distribution closer to 0,\n",
    "- higher magnitudes of `a` and `b` narrow the distribution width."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing stick-breaking\n",
    "\n",
    "For our purposes, we are going to hold `a` constant at 1.0 while varying `b`.\n",
    "We'll then see how our weight vectors are generated as a function of `b`.\n",
    "As you will see, `b` becomes a \"concentration\" parameter,\n",
    "which governs how \"concentrated\" our probability mass is allocated.\n",
    "\n",
    "Let's see how one draw from a Dirichlet process looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dp_draw(key, concentration, vector_length):\n",
    "    beta_draws = random.beta(key=key, a=1, b=concentration, shape=(vector_length,))\n",
    "    occupied_probability, weights = stick_breaking_weights(beta_draws)\n",
    "    return occupied_probability, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "key = random.PRNGKey(42)\n",
    "\n",
    "occupied_probability, weights = dp_draw(key, 3, 50)\n",
    "plt.plot(weights)\n",
    "plt.xlabel(\"Vector slot\")\n",
    "plt.ylabel(\"Probability\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, what if we took 20 draws from the Dirichlet process?\n",
    "\n",
    "To do so, we can `vmap` `dp_draw` over split `PRNGKey`s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import vmap\n",
    "from functools import partial\n",
    "import seaborn as sns\n",
    "keys = random.split(key, 20)\n",
    "occupied_probabilities, weights_draws = vmap(partial(dp_draw, concentration=3, vector_length=50))(keys)\n",
    "\n",
    "sns.heatmap(weights_draws);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect of concentration on Dirichlet weights draws\n",
    "\n",
    "As is visible here, when `concentration = 3`,\n",
    "most of our probability mass is _concentrated_\n",
    "across roughly the first 5-8 states.\n",
    "\n",
    "What happens if we varied the concentration?\n",
    "How does that parameter affect the distribution of weights?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as np\n",
    "\n",
    "concentrations = np.array([0.5, 1, 3, 5, 10, 20])\n",
    "\n",
    "def dirichlet_one_concentration(key, concentration, num_draws):\n",
    "    keys = random.split(key, num_draws)\n",
    "    occupied_probabilities, weights_draws = vmap(partial(dp_draw, concentration=concentration, vector_length=50))(keys)\n",
    "    return occupied_probabilities, weights_draws\n",
    "\n",
    "keys = random.split(key, len(concentrations))\n",
    "\n",
    "occupied_probabilities, weights_draws = vmap(partial(dirichlet_one_concentration, num_draws=20))(keys, concentrations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_draws.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(3*3, 3*2), sharex=True, sharey=True)\n",
    "\n",
    "for ax, weights_mat, conc in zip(axes.flatten(), weights_draws, concentrations):\n",
    "    sns.heatmap(weights_mat, ax=ax)\n",
    "    ax.set_title(f\"Concentration = {conc}\")\n",
    "    ax.set_xlabel(\"Component\")\n",
    "    ax.set_ylabel(\"Draw\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we increase the concentration value, the probabilities get more diffuse.\n",
    "This is evident from the above heatmaps in the following ways.\n",
    "\n",
    "1. Over each draw, as we increase the value of the concentration parameter, the probability mass allocated to the components that have significant probability mass decreases.\n",
    "2. Additionally, more components have \"significant\" amounts of probability mass allocated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running stick-breaking backwards\n",
    "\n",
    "From this forward process of generating Dirichlet-distributed weights,\n",
    "instead of evaluating the log likelihood of the component weights\n",
    "under a \"fixed\" Dirichlet distribution prior,\n",
    "we can instead evaluate it under a Dirichlet process with a \"concentration\" prior.\n",
    "The requirement here is that we be able to recover correctly the i.i.d. Beta draws\n",
    "that generated the Dirichlet process weights.\n",
    "\n",
    "Let's try that out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_workshop.gaussian_mixture import beta_draw_from_weights\n",
    "beta_draw_from_weights??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We essentially run the process backwards,\n",
    "taking advantage of the fact\n",
    "that we know the first weight exactly.\n",
    "Let's try to see how well we can recover the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "concentration = 3\n",
    "beta_draws = random.beta(key=key, a=1, b=concentration, shape=(50,))\n",
    "occupied_probability, weights = stick_breaking_weights(beta_draws)\n",
    "final, beta_hat = beta_draw_from_weights(weights)\n",
    "plt.plot(beta_draws, label=\"original\")\n",
    "plt.plot(beta_hat, label=\"inferred\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Component\")\n",
    "plt.ylabel(\"Beta Draw\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is visible from the plot above,\n",
    "we were able to recover about 1/2 to 2/3 of the weights\n",
    "before the divergence in the two curves shows up.\n",
    "\n",
    "One of the difficulties that we have is that\n",
    "when we get back the observed weights in real life,\n",
    "we have no access to how much of the length 1 \"stick\" is leftover.\n",
    "This, alongside numerical underflow issues arising from small numbers,\n",
    "means we can only use about 1/2 of the drawn weights\n",
    "to recover the Beta-distributed draws\n",
    "from which we can evaluate our log likelihoods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating log-likelihood of recovered Beta-distributed weights\n",
    "\n",
    "So putting things all together, we can take a weights vector,\n",
    "run the stick-breaking process backwards (up to a certain point)\n",
    "to recover Beta-distributed draws that would have generated the weights vector,\n",
    "and then evaluate the log-likelihood of the Beta-disributed draws\n",
    "under a Beta distribution.\n",
    "\n",
    "Let's see that in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_workshop.gaussian_mixture import component_probs_loglike\n",
    "\n",
    "component_probs_loglike??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And evaluating our draws should give us a scalar likelihood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_probs_loglike(np.log(weights), log_concentration=1.0, num_components=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log likelihood as a function of concentration\n",
    "\n",
    "Once again, let's build up our understanding\n",
    "by seeing how the log likelihood of our weights\n",
    "under an assumed Dirichlet process from a Beta distribution\n",
    "changes as we vary the concentration parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_concentration = np.linspace(-3, 3, 1000)\n",
    "\n",
    "def make_vmappable_loglike(log_component_probs, num_components):\n",
    "    def inner(log_concentration):\n",
    "        return component_probs_loglike(log_component_probs, log_concentration, num_components)\n",
    "    return inner\n",
    "\n",
    "component_probs_loglike_vmappable = make_vmappable_loglike(log_component_probs=np.log(weights), num_components=25)\n",
    "\n",
    "lls = vmap(component_probs_loglike_vmappable)(log_concentration)\n",
    "plt.plot(log_concentration, lls)\n",
    "plt.xlabel(\"Concentration\")\n",
    "plt.ylabel(\"Log likelihood\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above,\n",
    "we first constructed the vmappable log-likelihood function using a closure.\n",
    "The shape of the curve tells us that it is an optimizable problem with one optimal point,\n",
    "at least within bounds of possible concentrations that we're interested in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing the log-likelihood\n",
    "\n",
    "Once again, we're going to see how we can use gradient-based optimization\n",
    "to see how we can identify the most likely concentration value\n",
    "that generated a Dirichlet process weights vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss function\n",
    "\n",
    "As always, we start with the loss function definition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because our `component_probs_loglike` function operates only on a single draw,\n",
    "we need a function that will allow us to operate on multiple draws.\n",
    "We can do this by using a closure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad\n",
    "def make_loss_dp(num_components):\n",
    "    def loss_dp(log_concentration, log_component_probs):\n",
    "        \"\"\"Log-likelihood of component_probabilities of dirichlet process.\n",
    "        \n",
    "        :param log_concentration: Scalar value.\n",
    "        :param log_component_probs: One or more component probability vectors.\n",
    "        \"\"\"\n",
    "        vm_func = partial(\n",
    "            component_probs_loglike,\n",
    "            log_concentration=log_concentration,\n",
    "            num_components=num_components,\n",
    "        )\n",
    "        ll = vmap(vm_func, in_axes=0)(log_component_probs)\n",
    "        return -np.sum(ll)\n",
    "    return loss_dp\n",
    "\n",
    "loss_dp = make_loss_dp(num_components=25)\n",
    "\n",
    "dloss_dp = grad(loss_dp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_dp(np.log(3), log_component_probs=np.log(weights_draws[3] + 1e-6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have opted for a closure pattern here\n",
    "because we are going to require that\n",
    "the Dirichlet-process log likelihood loss function\n",
    "accept `log_concentration` (parameter to optimize) as the first argument,\n",
    "and `log_component_probs` (data) as the second.\n",
    "However, we need to specify the number of components we are going to allow\n",
    "for evaluating the Beta-distributed log likelihood,\n",
    "so that goes on the outside.\n",
    "\n",
    "Moreover, we are assuming i.i.d. draws of weights,\n",
    "therefore, we also `vmap` over all of the `log_component_probs`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define training loop\n",
    "\n",
    "Just as with the previous sections, we are going to define the training loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_workshop.gaussian_mixture import make_step_scannable\n",
    "\n",
    "make_step_scannable??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our demonstration here, we are going to use draws from the `weights_draws` matrix defined above,\n",
    "specifically the one at index 3, which had a concentration value of 5.\n",
    "Just to remind ourselves what that heatmapt looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(weights_draws[3]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we set up the scannable step function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental.optimizers import adam\n",
    "\n",
    "adam_init, adam_update, adam_get_params = adam(0.05)\n",
    "\n",
    "step_scannable = make_step_scannable(\n",
    "    get_params_func=adam_get_params,\n",
    "    dloss_func=dloss_dp,\n",
    "    update_func=adam_update,\n",
    "    data=np.log(weights_draws[3] + 1e-6), \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we initialize our parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_concentration_init = random.normal(key)\n",
    "params_init = log_concentration_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we run the training loop as a `lax.scan` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import lax\n",
    "\n",
    "initial_state = adam_init(params_init)\n",
    "\n",
    "final_state, state_history = lax.scan(step_scannable, initial_state, np.arange(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can calculate the losses over history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dl_workshop.gaussian_mixture import get_loss\n",
    "get_loss??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import vmap\n",
    "from functools import partial\n",
    "\n",
    "losses = vmap(\n",
    "    partial(\n",
    "        get_loss, \n",
    "        get_params_func=adam_get_params, \n",
    "        loss_func=loss_dp, \n",
    "        data=np.log(weights_draws[1] + 1e-6)\n",
    "    )\n",
    ")(state_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the final value that we obtain?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_opt = adam_get_params(final_state)\n",
    "params_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(params_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is pretty darn close to what we started with!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "Here, we took a detour through Dirichlet processes to help you get a grounding onto how its math works.\n",
    "Through code, we saw how to:\n",
    "\n",
    "1. Use the Beta distribution,\n",
    "1. Write the stick-breaking process using Beta-distributed draws to generate large vectors of weights that correspond to categorical probabilities,\n",
    "1. Run the stick-breaking process backwards from a vector of categorical probabilities to get back Beta-distributed draws\n",
    "1. Infer the maximum likelihood concentration value given a set of draws.\n",
    "\n",
    "The primary purpose of this section was to get you primed for the next section,\n",
    "in which we try to simulatenously infer the number of prominent mixture components\n",
    "and their distribution parameters.\n",
    "A (ahem!) _derivative_ outcome here was that I hopefully showed you how it is possible\n",
    "to use gradient-based optimization on seemingly discrete problems."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-workshop",
   "language": "python",
   "name": "dl-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
