{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deterministic Randomness\n",
    "\n",
    "In this section, we'll explore how to create programs \n",
    "that use random number generation in a fashion that is fully deterministic.\n",
    "If that sounds weird to you, fret not: \n",
    "it sounded weird to me too when I first started using random numbers.\n",
    "My goal here is to demystify this foundational piece for you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random number generation before JAX\n",
    "\n",
    "Before JAX came along, we used NumPy's stateful random number generation system.\n",
    "Let's quickly recap how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as onp  # original numpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's draw a random number from a Gaussian in NumPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "onp.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "a = onp.random.normal()\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And for good measure, let's draw another one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b = onp.random.normal()\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is intuitive behaviour,\n",
    "because we expect that each time we call on a random number generator,\n",
    "we should get back a different number from before.\n",
    "\n",
    "However, this behaviour is problematic when we are trying to debug programs.\n",
    "When debugging, one desirable property is determinism.\n",
    "Executing the same line of code twice _should_ produce exactly the same result.\n",
    "Otherwise, debugging what happens at that particular line would be extremely difficult.\n",
    "The core problem here is that stochastically,\n",
    "we might hit a setting where we encounter an error in our program,\n",
    "and we are unable to reproduce it because we are relying on\n",
    "a random number generator that relies on global state,\n",
    "and hence that doesn't behave in a _fully_ controllable fashion.\n",
    "\n",
    "I don't know about you, but if I am going to encounter problems,\n",
    "I'd like to encounter them _reliably_!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random number generation with JAX\n",
    "\n",
    "How then can we get \"the best of both worlds\": random number generation that is controllable?\n",
    "\n",
    "### Explicit PRNGKeys control random number generation\n",
    "\n",
    "The way that JAX's developers went about doing this\n",
    "is to use pseudo-random number generators\n",
    "that require explicit passing in of a pseudo-random number generation key,\n",
    "rather than relying on a global state being set.\n",
    "Each unique key will deterministically give a unique drawn value explicitly.\n",
    "Let's see that in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from jax import random\n",
    "\n",
    "key = random.PRNGKey(42)\n",
    "\n",
    "a = random.normal(key=key)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show you that passing in the same key gives us the same values as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b = random.normal(key=key)\n",
    "b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That should already be a stark difference from what you're used to with vanilla NumPy,\n",
    "and this is one key crucial difference between JAX's random module and NumPy's random module.\n",
    "Everything else about the API is very similar,\n",
    "but this is a key difference, and for good reason -- \n",
    "this should hint to you the idea that we can have explicit reproducibility,\n",
    "rather than merely implicit,\n",
    "over our stochastic programs within the same session."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting keys to generate new draws\n",
    "\n",
    "How, then, do we get a new draw from JAX?\n",
    "Well, we can either create a new key manually,\n",
    "or we can programmatically split the key into two,\n",
    "and use one of the newly split keys to generate a new random number. Let's see that in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k1, k2 = random.split(key)\n",
    "c = random.normal(key=k2)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k3, k4, k5 = random.split(k2, num=3)\n",
    "d = random.normal(key=k3)\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating multiple draws from a Gaussian, two ways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show you how we can combine random keys together with `vmap`, here's two ways we can generate random draws from a Normal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first way is to split the key into K (say, 20) pieces and then vmap `random.normal` over the split keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from jax import vmap\n",
    "key = random.PRNGKey(44)\n",
    "ks = random.split(key, 20)  # we want to generate 20 draws\n",
    "draws = vmap(random.normal)(ks)\n",
    "draws\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the second way is to simply specify the shape of the draws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.normal(key, shape=(20,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By splitting the key into two, three, or even 1000 parts, we can get new keys that are derived from a parent key that generate different random numbers from the same random number generating function.\n",
    "\n",
    "Let's explore how we can use this in the generation of a Gaussian random walk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Simulating a Gaussian random walk\n",
    "\n",
    "A Gaussian random walk is one where we start at a point that is drawn from a Gaussian,\n",
    "and then we draw another point from a Gausian using the first point as the starting Gaussian point.\n",
    "Does that loop structure sound familiar? \n",
    "Well... yeah, it sounds like a classic `lax.scan` setup!\n",
    "\n",
    "Here's how we might set it up.\n",
    "\n",
    "Firstly, JAX's `random.normal` function doesn't allow us to specify the location and scale,\n",
    "and only gives us a draw from a unit Gaussian.\n",
    "We can work around this, because any unit Gaussian draw can be shifted and scaled to a $N(\\mu, \\sigma)$\n",
    "by multiplying the draw by $\\sigma$ and adding $\\mu$. \n",
    "\n",
    "Knowing this, let's see how we can write a Gaussian random walk using JAX's idioms, building up from a vanilla Python implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vanilla Python implementation\n",
    "\n",
    "For those who might not be too familiar with Gaussian random walks,\n",
    "here is an annotated version in vanilla Python code\n",
    "(plus some use of the JAX PRNGKey system added in)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_timesteps = 100\n",
    "\n",
    "mu = 0.0  # starting mean.\n",
    "observations = [mu]\n",
    "\n",
    "key = random.PRNGKey(44)\n",
    "# Split the key num_timesteps number of times\n",
    "keys = random.split(key, num_timesteps)\n",
    "\n",
    "# Gaussian Random Walk goes here\n",
    "for k in keys:\n",
    "    mu = mu + random.normal(k)\n",
    "    observations.append(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation using JAX\n",
    "\n",
    "Now, let's see how we can write a Gaussian random walk\n",
    "using `lax.scan`.\n",
    "The strategy we'll go for is as follows:\n",
    "\n",
    "1. We'll instantiate an array of PRNG keys.\n",
    "2. We'll then scan a function across the PRNG keys.\n",
    "3. We'll finally collect the observations together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import lax\n",
    "\n",
    "def new_draw(prev_val, key):\n",
    "    new = prev_val + random.normal(key)\n",
    "    return new, prev_val\n",
    "\n",
    "\n",
    "final, draws = lax.scan(new_draw, 0.0, keys)\n",
    "plt.plot(draws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we did it!\n",
    "Definitely looks like a proper Gaussian random walk to me. \n",
    "Let's encapsulate the code inside a function\n",
    "that gives us _one_ random walk draw,\n",
    "as I will show you how next to generate multiple random walk draws."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grw_draw(key, num_steps):\n",
    "    keys = random.split(key, num_steps)\n",
    "    final, draws = lax.scan(new_draw, 0.0, keys)\n",
    "    return final, draws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final, draw = grw_draw(key, num_steps=100)\n",
    "plt.plot(draw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on reproducibility\n",
    "\n",
    "Now, note how if you were to re-run the entire program from top-to-bottom again, \n",
    "you would get _exactly the same plot_. \n",
    "This is what we might call _strictly reproducible_.\n",
    "Traditional array programs are not always written in a strictly reproducible way;\n",
    "the sloppy programmer would set a global state at the top of a notebook and then call it a day.\n",
    "\n",
    "By contrast, with JAX's random number generation paradigm, \n",
    "any random number generation program is 100% reproducible, \n",
    "down to the level of the exact sequence of random number draws, \n",
    "as long as the seed(s) controlling the program are 100% identical.\n",
    "Because JAX's stochastic programs _always_ require an explicit key to be provided,\n",
    "as long as you write your stochastic programs to depend on keys passed into it,\n",
    "rather than keys instantiated from within it,\n",
    "any errors you get can be fully reproduced by passing in exactly the same key.\n",
    "\n",
    "When an error shows up in a program, \n",
    "as long as its stochastic components are controlled by explicitly passed in seeds,\n",
    "that error is 100% reproducible. \n",
    "For those who have tried working with stochastic programs before,\n",
    "this is an extremely desirable property,\n",
    "as it means we gain the ability to reliably debug our program --\n",
    "absolutely crucial especially when it comes to working with probabilistic models.\n",
    "\n",
    "Also notice how we finally wrote our first productive for-loop --\n",
    "but it was only to plot something, not for some form of calculations :)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Brownian motion on a grid\n",
    "\n",
    "In this exercise, the goal is to simulate the random walk of a single particle on a 2D grid.\n",
    "The particle's (x, y) position can be represented by a vector of length 2.\n",
    "At each time step, the particle moves either in the x- or y- direction,\n",
    "and when it moves, it either goes +1 or -1 along that axis.\n",
    "Here is the NumPy + Python loopy equivalent that you'll be simulating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jax.numpy as np\n",
    "\n",
    "starting_position = onp.array([0, 0])\n",
    "n_steps = 1000\n",
    "\n",
    "positions = [starting_position]\n",
    "keys = random.split(key, n_steps)\n",
    "for k in keys:\n",
    "    k1, k2 = random.split(k)\n",
    "    axis = random.choice(k1, np.array([0, 1]))\n",
    "    direction = random.choice(k2, np.array([-1, 1]))\n",
    "    x, y = positions[-1]\n",
    "    if axis == 0:\n",
    "        x += direction\n",
    "    else:\n",
    "        y += direction\n",
    "    new_position = np.array([x, y])\n",
    "    positions.append(new_position)\n",
    "positions = np.stack(positions)\n",
    "plt.plot(positions[:, 0], positions[:, 1], alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your challenge is to replicate the brownian motion on a grid using JAX's random module.\n",
    "Some hints that may help you get started include:\n",
    "\n",
    "1. JAX arrays are immutable, so you definitely cannot do `arr[:, 0] += 1`.\n",
    "2. `random.permutation` can be used to identify which axis to move.\n",
    "3. `random.choice` can be used to identify which direction to go in.\n",
    "4. Together, the axis to move in and the direction to proceed can give you something to loop over...\n",
    "5. ...but without looping explicitly :), for which you have all of the tricks in the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def randomness_ex_1(keys):\n",
    "    # Your answer here!\n",
    "    pass\n",
    "\n",
    "\n",
    "from dl_workshop.jax_idioms import randomness_ex_1\n",
    "\n",
    "final, history = randomness_ex_1(keys, starting_position)\n",
    "plt.plot(history[:, 0], history[:, 1], alpha=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Stochastic stick breaking\n",
    "\n",
    "In the previous notebook, we introduced you to the stick-breaking process,\n",
    "and we asked you to write it in a non-stochastic fashion.\n",
    "We're now going to have you write it using a stochastic draw.\n",
    "\n",
    "To do so, however, you need to be familiar with the [Beta distribution][betadist],\n",
    "which models a random draw from the interval $x \\in (0, 1)$.\n",
    "\n",
    "[betadist]: https://en.wikipedia.org/wiki/Beta_distribution\n",
    "\n",
    "Here is how you can draw numbers from the Beta distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "betadraw = random.beta(key, a=1, b=2)\n",
    "betadraw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, I'm going to show you the NumPy + Python equivalent of the _real_ (i.e. stochastic) stick-breaking process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as np\n",
    "\n",
    "num_breaks = 30\n",
    "keys = random.split(key, num_breaks)\n",
    "concentration = 5\n",
    "\n",
    "sticks = []\n",
    "stick_length = 1.0\n",
    "for k in keys:\n",
    "    breaking_fraction = random.beta(k, a=1, b=concentration)\n",
    "    stick = stick_length * breaking_fraction\n",
    "    sticks.append(stick)\n",
    "    stick_length = stick_length - stick\n",
    "result = np.array(sticks)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, your task is to implement it using `lax.scan`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def randomness_ex_2(key, num_breaks, concentration: float):\n",
    "    # Your answer here!\n",
    "    pass\n",
    "\n",
    "\n",
    "# Comment out the import to test your answer!\n",
    "from dl_workshop.jax_idioms import randomness_ex_2\n",
    "\n",
    "final, sticks = randomness_ex_2(key, num_breaks, concentration)\n",
    "assert np.allclose(sticks, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Multiple GRWs\n",
    "\n",
    "Now, what if we wanted to generate multiple realizations of the Gaussian random walk? \n",
    "Does this sound familiar? \n",
    "If so... yeah, it's a vanilla for-loop, which directly brings us to `vmap`!\n",
    "And that's what we're going to try to implement in this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from jax import vmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The key idea here is to `vmap` the `grw_draw` function across multiple PRNGKeys.\n",
    "That way, you can avoid doing a for-loop, which is the goal of this exercise too.\n",
    "You get to decide how many realizations of the GRW you'd like to create."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def randomness_ex_3(key, num_realizations=20, grw_draw=grw_draw):\n",
    "    # Your answer here\n",
    "    pass\n",
    "\n",
    "from dl_workshop.jax_idioms import randomness_ex_3\n",
    "\n",
    "final, trajectories = randomness_ex_3(key, num_realizations=20, grw_draw=grw_draw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trajectories.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We did it! We have 20 trajectories of a 1000-step Gaussian random walk. \n",
    "Notice also how the program is structured very nicely: \n",
    "Each layer of abstraction in the program corresponds to a new axis dimension along which we are working. \n",
    "The onion layering of the program has very _natural_ structure for the problem at hand.\n",
    "Effectively, we have planned out, or perhaps staged out, our computation using Python\n",
    "before actually executing it.\n",
    "\n",
    "Let's visualize the trajectories to make sure they are really GRW-like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for trajectory in trajectories[0:20]:\n",
    "    ax.plot(trajectory)\n",
    "sns.despine()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-workshop",
   "language": "python",
   "name": "dl-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
