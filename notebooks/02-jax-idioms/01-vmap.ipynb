{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing simple for-loops with `vmap`\n",
    "\n",
    "The first JAX thing we will look at is the `vmap` function. What does `vmap` do? From the [JAX docs on `vmap`](https://jax.readthedocs.io/en/latest/jax.html#jax.vmap):\n",
    "\n",
    "> Vectorizing map. Creates a function which maps fun over argument axes.\n",
    "\n",
    "What does that mean? Well, let's take a look at a few classic examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping an elementwise function over an array's leading axis\n",
    "\n",
    "The first example is mapping a function over an array axis. The simplest example, which is a bit trivial, is doing elementwise application of a function. Say we have uniformly spaced numbers from 0 to 1 in an array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as np\n",
    "from jax import vmap\n",
    "from time import time\n",
    "\n",
    "arr = np.linspace(0, 1, 10000)\n",
    "arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to apply an exponential transform on every element, the \"dumb\", pure Python way to do so is to write a for-loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "new_arr = []\n",
    "for element in arr:\n",
    "    new_arr.append(np.exp(element))\n",
    "new_arr = np.array(new_arr)\n",
    "end = time()\n",
    "print(f\"{end - start:.2f} seconds\")\n",
    "new_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because `np.exp` is a NumPy `ufunc` that operates on individual elements, we can call `np.exp` on `arr` directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "new_arr = np.exp(arr)\n",
    "end = time()\n",
    "print(f\"{end - start:.4f} seconds\")\n",
    "new_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this is much faster.\n",
    "\n",
    "This, incidentally, is equivalent to using `vmap` to map the function across all elements in the array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "new_arr = vmap(np.exp)(arr)\n",
    "end = time()\n",
    "print(f\"{end - start:.4f} seconds\")\n",
    "new_arr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a bit slower, but one thing we gain from using `vmap` is the ability to ignore the leading (first) array axis of every element that is passed into the `vmap`-ed function. To see that, we're going to look at another example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping a row-wise function across an array's leading axis\n",
    "\n",
    "In this example let's say we have a matrix of values that we measured in an experiment. There were `n_samples` measured, and `3` unique properties that we collected, thereby giving us a matrix of shape `(n_samples, 3)`. If we needed to find their sum, we could do the following in pure NumPy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_sum(data):\n",
    "    \"\"\"Given one dataset, calculate row-wise sum of data.\"\"\"\n",
    "    return np.sum(data, axis=1)\n",
    "\n",
    "data = np.array([\n",
    "    [1, 3, 1,],\n",
    "    [3, 5, 1,],\n",
    "    [1, 2, 5,],\n",
    "    [7, 1, 3,],\n",
    "    [11, 2, 3,],\n",
    "])\n",
    "\n",
    "start = time()\n",
    "result = row_sum(data)\n",
    "end = time()\n",
    "print(f\"{end - start:.4f} seconds\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This would give us the correct answer... but we had to worry about the \"axis\" argument, which is a bit irritating. Instead, we could use first transform `np.sum` into a vmapped function that is mapped across the leading axis of `data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_sum_one_data(data):\n",
    "    \"\"\"Given one dataset, calculate row-wise sum of data.\"\"\"\n",
    "    return vmap(np.sum)(data)\n",
    "\n",
    "start = time()\n",
    "result = row_sum_one_data(data)\n",
    "end = time()\n",
    "print(f\"{end - start:.4f} seconds\")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thereby giving us the exact same result. While the syntax does take some time to get used to, it does more explicitly and clearly expresses the idea that _we don't really care about summing over the leading axis_.\n",
    "\n",
    "Now, let's say we had multiple datasets for which we wanted to calculate the row-wise sum. How would we do this in pure NumPy?\n",
    "\n",
    "Well, let's first create this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2 = np.array([\n",
    "    [1, 3, 7,],\n",
    "    [3, 5, 11,],\n",
    "    [3, 2, 5,],\n",
    "    [7, 5, 3,],\n",
    "    [11, 5, 3,],\n",
    "])\n",
    "\n",
    "combined_data = np.moveaxis(np.dstack([data, data2], ), 2, 0)\n",
    "combined_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our shapes tell us that we have 2 stacks of data, each with 5 rows and 3 columns.\n",
    "\n",
    "Since we want row-wise summation, but want to preserve the 2 stacks of data, we have to now worry about which axes to collapse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(combined_data, axis=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is all cool, but we now have a \"magic number\" in our program. We can eliminate this magic number by instead doing vmapping `row_sum_over_data` across the `combined_data` array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def row_sum_all_data(data):\n",
    "    return vmap(row_sum_one_data)(data)\n",
    "    \n",
    "row_sum_all_data(combined_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And voil√†, just like that, magic numbers were removed from our program, and the hierarchical structure of our functions are a bit more explicit:\n",
    "\n",
    "- The elementary function, `np.sum`, operates on a per-row basis.\n",
    "- We map the elementary function across all rows of a single dataset, giving us a higher-order function that calculates row-wise summation for a single dataset, `row_sum_one_data`.\n",
    "- We then map the `row_sum_one_data` across all of the datasets that have been stacked together in a single 3D array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping a function over two arrays simultaneously\n",
    "\n",
    "Let's look at another example. Say we are given two arrays, and we wanted to elementwise multiply them together. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = np.array([1, 2, 3, 4,])\n",
    "a2 = np.array([2, 3, 4, 5,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the NumPy-idiomatic option, we could do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 * a2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option is that we can define a function called `multiply`, which multiplies two scalars together and gives us back another scalar, which we then apply over each element in a `zip` of the two arrays. This is the _extremely_ naive way of handling the problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = []\n",
    "\n",
    "def multiply(a, b):\n",
    "    return a * b\n",
    "\n",
    "for val1, val2 in zip(a1, a2):\n",
    "    result.append(multiply(val1, val2))\n",
    "np.array(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, if we consider this to be the elementary operation of our function, we could instead multiply them pairwise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vmap(multiply)(a1, a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we are able to not care about the leading array axis for each array. Once again, we also broke down the problem into its elementary components, and then leveraged `vmap` to build _out_ the program to do what we wanted it to do. (This general pattern will show up!)\n",
    "\n",
    "In general, `vmap`-ing over the _leading_ array axis is the idiomatic thing to do with JAX. It's possibleto `vmap` over other axes, but those are not the defaults. The implication is that we are nudged towards writing programs that at their core begin with an \"elementary\" function that operate \"elementwise\", where the definition of an \"element\" is not necessarily an array element, but problem-dependent. We then progressively `vmap` them outwards on array data structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: `vmap`-ing a dot product over square matrices\n",
    "\n",
    "Let's try getting some practice with the following exercises.\n",
    "\n",
    "The first one is to `vmap` a dot product of a square matrix against itself across a stack of square matrices.\n",
    "\n",
    "An example square matrix called `sq_matrix` is provided for you to jog your memory on how dot products work if you need to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random\n",
    "\n",
    "key = random.PRNGKey(42)\n",
    "data = random.normal(key, shape=(11, 5, 5))\n",
    "sq_matrix = random.normal(key, shape=(5, 5))\n",
    "\n",
    "vmap(np.dot)(data, data).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Constructing a more complex program\n",
    "\n",
    "We're going to try our hand at constructing a program that first calculates a cumulative product vector for each row in each dataset, sums them up column-wise across each dataset, and applies this same operation across all datasets stacked together. This one is a bit more challenging!\n",
    "\n",
    "To help you along here, the shape of the data are such:\n",
    "\n",
    "- There are 11 stacks of data.\n",
    "- Each stack of data has 31 rows, and 7 columns.\n",
    "\n",
    "The result of this program still should have 11 stacks and 31 rows, but now each column is not the original data, but the cumulative product of the previous columns.\n",
    "\n",
    "To get this answer write, no magic numbers are allows (e.g. for accessing particular axes). At least two `vmap`s are necessary here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = random.normal(key, shape=(11, 31, 7))\n",
    "\n",
    "def row_wise_cumprod(row):\n",
    "    return np.cumprod(row)\n",
    "\n",
    "def dataset_wise_sum_cumprod(data):\n",
    "    row_cumprods = vmap(row_wise_cumprod)(data)\n",
    "    return vmap(np.sum)(row_cumprods)\n",
    "\n",
    "vmap(dataset_wise_sum_cumprod)(data).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-workshop",
   "language": "python",
   "name": "dl-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
