{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "JAX has a `jit` function that allows us to just-in-time compile functions\n",
    "written using JAX's NumPy and SciPy-wrapped functions.\n",
    "JIT stands for \"just-in-time\" compilation,\n",
    "which stands in contrast to AOT (ahead-of-time).\n",
    "Using `jit` should give you speed-ups compared to not using it.\n",
    "\n",
    "In this notebook, we are going to explore the gains that we expect to get\n",
    "by using JAX's just-in-time compilation function `jit`.\n",
    "Because JIT compilation is usually simply applied _on top of_ existing functions,\n",
    "we'll explore its primarily by examples rather than by exercises."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JIT example from the JAX docs\n",
    "\n",
    "Coming up with an example where JIT compilation could be useful is quite a challenge,\n",
    "so let's start off with an examplee from the JAX docs.\n",
    "\n",
    "The function in question is the SELU function,\n",
    "which is an activation function applied elementwise\n",
    "to the outputs of a neural network layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import jax.numpy as np\n",
    "\n",
    "\n",
    "def selu(x, alpha=1.67, lmbda=1.05):\n",
    "    return lmbda * np.where(x > 0, x, alpha * np.exp(x) - alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timing the function _without_ JIT compilation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "from jax import random\n",
    "\n",
    "key = random.PRNGKey(44)\n",
    "x = random.normal(key, (1000000,))\n",
    "%timeit selu(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try JIT-compiling the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from jax import jit\n",
    "\n",
    "selu_jit = jit(selu)\n",
    "\n",
    "%timeit selu_jit(x).block_until_ready()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the JIT-compiled function is about 3X faster than the non-JIT compiled function.\n",
    "\n",
    "More importantly, any function that you write using JAX-wrapped NumPy,\n",
    "JAX-wrapped SciPy,\n",
    "and its own provided `lax` submodule,\n",
    "can be JIT-compiled to gain speed-ups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-examining the Gaussian random walk\n",
    "\n",
    "Let's revisit the Gaussian random walk that we implemented\n",
    "as a case study in what happens when we use JAX's idioms to write our code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pure Python version of the Gaussian random walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as onp\n",
    "\n",
    "\n",
    "def gaussian_random_walk_python(num_realizations, num_timesteps):\n",
    "    rws = []\n",
    "    for i in range(num_realizations):\n",
    "        rw = []\n",
    "        prev_draw = 0\n",
    "        for t in range(num_timesteps):\n",
    "            prev_draw = onp.random.normal(loc=prev_draw)\n",
    "            rw.append(prev_draw)\n",
    "        rws.append(rw)\n",
    "    return rws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "N_REALIZATIONS = 1_000\n",
    "N_TIMESTEPS = 10_000\n",
    "start = time()\n",
    "trajectories_python = gaussian_random_walk_python(N_REALIZATIONS, N_TIMESTEPS)\n",
    "end = time()\n",
    "print(f\"{end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "for trajectory in trajectories_python[:20]:\n",
    "    plt.plot(trajectory)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JAX implementation without JIT\n",
    "\n",
    "Now, let's take a look at the JAX-based implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from jax import lax, random\n",
    "\n",
    "key = random.PRNGKey(44)\n",
    "keys = random.split(key, N_TIMESTEPS)\n",
    "\n",
    "\n",
    "def new_draw(prev_val, key):\n",
    "    \"\"\"lax.scannable function for drawing a new draw from the GRW.\"\"\"\n",
    "    new = prev_val + random.normal(key)\n",
    "    return new, prev_val\n",
    "\n",
    "\n",
    "def grw_draw(key, num_steps):\n",
    "    \"\"\"One GRW draw over a bunch of steps.\"\"\"\n",
    "    keys = random.split(key, num_steps)\n",
    "    final, draws = lax.scan(new_draw, 0.0, keys)\n",
    "    return final, draws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "from jax import vmap\n",
    "\n",
    "\n",
    "def gaussian_random_walk_jax(num_realizations, num_timesteps):\n",
    "    \"\"\"Multiple GRW draws.\"\"\"\n",
    "    keys = random.split(key, num_realizations)\n",
    "    grw_k_steps = partial(grw_draw, num_steps=num_timesteps)\n",
    "    final, trajectories = vmap(grw_k_steps)(keys)\n",
    "    return final, trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from jax import random\n",
    "\n",
    "key = random.PRNGKey(42)\n",
    "start = time()\n",
    "final_jax, trajectories_jax = gaussian_random_walk_jax(\n",
    "    N_REALIZATIONS, N_TIMESTEPS\n",
    ")\n",
    "trajectories_jax.block_until_ready()\n",
    "end = time()\n",
    "print(f\"{end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%timeit gaussian_random_walk_jax(N_REALIZATIONS, N_TIMESTEPS)[1].block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for trajectory in trajectories_jax[:20]:\n",
    "    plt.plot(trajectory)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JAX implementation _with_ JIT compilation\n",
    "\n",
    "Now we're going to JIT-compile our Gaussian Random Walk function and see how long it takes for the program to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from jax import jit\n",
    "\n",
    "\n",
    "def gaussian_random_walk_jit(num_realizations, num_timesteps):\n",
    "    keys = random.split(key, num_realizations)\n",
    "    grw_k_steps = jit(partial(grw_draw, num_steps=num_timesteps))\n",
    "    final, trajectories = vmap(grw_k_steps)(keys)\n",
    "    return final, trajectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start = time()\n",
    "final_jit, trajectories_jit = gaussian_random_walk_jit(\n",
    "    N_REALIZATIONS, N_TIMESTEPS\n",
    ")\n",
    "trajectories_jit.block_until_ready()\n",
    "end = time()\n",
    "print(f\"{end - start:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%timeit gaussian_random_walk_jit(N_REALIZATIONS, N_TIMESTEPS)[1].block_until_ready()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for trajectory in trajectories_jit[:20]:\n",
    "    plt.plot(trajectory)\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may appear that JIT-compilation doesn't appear to do much,\n",
    "but we can assure you that there's a great explanation for this phenomena.\n",
    "\n",
    "Within the Gaussian random walk, we used `lax.scan`,\n",
    "which itself gives us a fairly compiled operation already.\n",
    "The docs spell it out in jargon:\n",
    "\n",
    "> Also unlike that Python version, scan is a JAX primitive and is lowered to a single XLA While HLO. \n",
    "> That makes it useful for reducing compilation times for jit-compiled functions, \n",
    "> since native Python loop constructs in an @jit function are unrolled, \n",
    "> leading to large XLA computations.\n",
    "\n",
    "If we were to use a for-loop instead of `lax.scan`,\n",
    "then we would be missing out on te performance gain.\n",
    "So when we add in JIT-compilation _on top of_ using `lax.scan`,\n",
    "the added gain is not as much as if we didn't use `lax.scan`.\n",
    "\n",
    "In both cases, the runtime is essentially constant\n",
    "\n",
    "JIT-compilation gave us about a 1-2X speedup over non-JIT compiled code,\n",
    "and was approximately at least 20X faster than the pure Python version.\n",
    "That shouldn't surprise you one bit :)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A few pointers on syntax\n",
    "\n",
    "Firstly, if we subscribe to the Zen of Python's notion that \"flat is better than nested\",\n",
    "then by following the idioms listed here -- closures/partials, `vmap` and `lax.scan`,\n",
    "we'll likely only ever go one closure deep into our programs.\n",
    "Notice how we basically never wrote any for-loops in our array code;\n",
    "they were handled elegantly by the looping constructs `vmap` and `lax.scan`. \n",
    "\n",
    "Secondly, using `jit`, we get further optimizations on our code for free.\n",
    "A pre-requisite of `jit` is that the _every_ function call made in the program function being `jit`-ed\n",
    "is required to be written in a \"pure functional\" style,\n",
    "i.e. there are no side effects, no mutation of global state.\n",
    "Put plainly, everything that you use _inside_ the function should be passed in\n",
    "(with the exception of imports, of course).\n",
    "If you write a program using the idioms used here\n",
    "(closures to wrap state, `vmap`/`lax.scan` in lieu of loops,\n",
    "explicit random number generation using PRNGKeys),\n",
    "then you will be able to JIT compile the program with ease."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-workshop",
   "language": "python",
   "name": "dl-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
