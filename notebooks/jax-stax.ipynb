{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, I'll show the code for how to use JAX's `stax` submodule to write arbitrary models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "I'm assuming you have read through the `jax-programming.ipynb` notebook, as well as the `tutorial.ipynb` notebook.\n",
    "\n",
    "The main `tutorial.ipynb` notebook gives you a general introduction to differential programming using `grad`, while the `jax-programming.ipynb` notebook gives you a flavour of the other four main JAX idioms: `vmap`, `lax.scan`, `random.PRNGKey`, and `jit`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is `stax`?\n",
    "\n",
    "Most deep learning libraries use objects as the data structure for a neural network layer.\n",
    "As such, the tunable parameters of the layer, for example `w` and `b` for a linear (\"dense\") layer\n",
    "are class attributes associated with the forward function.\n",
    "\n",
    "In some sense, because a neural network layer is nothing more than a math function,\n",
    "specifying the layer in terms of a function might also make sense.\n",
    "`stax`, then, is a new take on writing neural network models using pure functions rather than objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does `stax` work?\n",
    "\n",
    "The way that `stax` layers work is as follows.\n",
    "Every neural network layer is nothing more than a math function with a \"forward\" pass.\n",
    "Neural network models typically have their parameters \n",
    "initialized into the right shapes using random number generators.\n",
    "Put these two together, and we have a _pair_ of functions that specify a layer:\n",
    "\n",
    "- An `init_fun` function, that _initializes_ parameters into the correct shapes, and\n",
    "- An `apply_fun` function, that _applies_ the specified math transformations onto incoming data, using parameters of the correct shape."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Linear layer\n",
    "\n",
    "Let's see an example of this in action, by studying the implementation of the linear (\"dense\") layer in `stax`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental import stax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stax.Dense??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the `apply_fun` specifies the linear transformation.\n",
    "It accepts a parameter called `params`,\n",
    "which gets tuple-unpacked into the appropriate `W` and `b`.\n",
    "\n",
    "**Notice how the `params` argument matches up with the second output of `init_fun`!**\n",
    "The `init_fun` always accepts an `rng` parameter, which is returned from JAX's `jax.random.PRNGKey()`.\n",
    "It also accepts an `input_shape` parameter,\n",
    "which specifies what the elementary shape of one sample of data is.\n",
    "So if your entire dataset is of shape `(n_samples, n_columns)`,\n",
    "then you would put in `(n_columns,)` inside there,\n",
    "as you would want to ignore the sample dimension,\n",
    "thus allowing us to take advantage of `vmap` to map our model function\n",
    "over each and every i.i.d. sample in our dataset.\n",
    "The `init_fun` also returns the `output_shape`,\n",
    "which is used later when we chain layers together.\n",
    "\n",
    "Let's see how we can use the Dense layer to specify a linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the initialization and application function pairs\n",
    "\n",
    "Firstly, we create the `init_fun` and `apply_fun` pair:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_fun, apply_fun = stax.Dense(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the parameters\n",
    "\n",
    "Now, let's initialize parameters using the `init_fun`.\n",
    "\n",
    "Let's assume that we have data that is of 4 columns only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import random, numpy as np\n",
    "\n",
    "key = random.PRNGKey(42)\n",
    "\n",
    "output_shape, params_initial = init_fun(key, input_shape=(4,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_initial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply parameters and data through function\n",
    "\n",
    "We'll create some randomly generated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = random.normal(key, shape=(200, 4))\n",
    "X[0:5], X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's some `y_true` values that I've snuck in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.dot(X, np.array([1, 2, 3, 4])) + 5\n",
    "y_true = y_true.reshape(-1, 1)\n",
    "y_true[0:5], y_true.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll pass data through the linear model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_fun??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import vmap\n",
    "from functools import partial\n",
    "\n",
    "y_pred = vmap(partial(apply_fun, params_initial))(X)\n",
    "y_pred[0:5], y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voil√†! We have a simple linear model implemented just like that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization\n",
    "\n",
    "Next question: how do we *optimize* the parameters using JAX?\n",
    "\n",
    "Instead of writing a training loop on our own, we can take advantage of JAX's optimizers, which are also written in a functional paradigm!\n",
    "\n",
    "JAX's optimizers are constructed as a \"triplet\" set of functions:\n",
    "\n",
    "- `init`: Takes `params` and initializes them in as a `state`, which is structured in a fashion that `update` can operate on.\n",
    "- `update`: Takes in `i`, `g`, and `state`, which respectively are:\n",
    "    - `i`: The current loop iteration\n",
    "    - `g`: Gradients calculated from `grad`!\n",
    "    - `state`: The current state of the parameters.\n",
    "- `get_params`: Takes in the `state` at a given point, and returns the parameters structured correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import jit, grad\n",
    "from jax.experimental.optimizers import adam\n",
    "\n",
    "init, update, get_params = adam(step_size=1e-1)\n",
    "update = jit(update)\n",
    "get_params = jit(get_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're still missing a piece here, that is the loss function.\n",
    "For illustration purposes, let's use the mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mseloss(params, model, x, y_true):\n",
    "    y_preds = vmap(partial(model, params))(x)\n",
    "    return np.mean(np.power(y_preds - y_true, 2))\n",
    "\n",
    "dmseloss = grad(mseloss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Step\" portion of update loop\n",
    "\n",
    "Now, we're going to define the \"step\" portion of the update loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(i, state, dlossfunc, get_params, update, model, x, y_true):\n",
    "    params = get_params(state)\n",
    "    g = dlossfunc(params, model, x, y_true)\n",
    "    state = update(i, g, state)\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JIT compilation\n",
    "\n",
    "Because it takes so many parameters (in order to remain pure, and not rely on notebook state),\n",
    "we're going to bind some of them using `functools.partial`.\n",
    "\n",
    "I'm also going to show you what happens when we JIT-compile vs. don't JIT-compile the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_partial = partial(step, get_params=get_params, dlossfunc=dmseloss, update=update, model=apply_fun, x=X, y_true=y_true)\n",
    "step_partial_jit = jit(step_partial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explicit loops\n",
    "\n",
    "Firstly, let's see what kind of code we'd write if we _did_ write the loop explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "start = time()\n",
    "state = init(params_initial)\n",
    "for i in range(1000):\n",
    "    params = get_params(state)\n",
    "    g = dmseloss(params, apply_fun, X, y_true)\n",
    "    state = update(i, g, state)\n",
    "end = time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partialled out loop step\n",
    "\n",
    "Now, let's run the loop with the partialled out function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "state = init(params_initial)\n",
    "for i in range(1000):\n",
    "    state = step_partial(i, state)\n",
    "end = time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JIT-compiled loop!\n",
    "\n",
    "This is much cleaner of a loop, but we did have to do some work up-front.\n",
    "\n",
    "What happens if we now use the JIT-ed function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "state = init(params_initial)\n",
    "for i in range(1000):\n",
    "    state = step_partial_jit(i, state)\n",
    "end = time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoa, holy smokes, that's fast! At least 10X faster using JIT-compilation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `lax.scan` loop\n",
    "\n",
    "Now we'll use some JAX trickery ot write a training loop without ever writing a for-loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import lax\n",
    "\n",
    "def make_scannable_step(stepfunc):\n",
    "    def scannable_step(previous_state, iteration):\n",
    "        new_state = stepfunc(iteration, previous_state)\n",
    "        return new_state, previous_state\n",
    "    return scannable_step\n",
    "\n",
    "scannable_step = make_scannable_step(step_partial_jit)\n",
    "\n",
    "start = time()\n",
    "initial_state = init(params_initial)\n",
    "final_state, states_history = lax.scan(scannable_step, initial_state, np.arange(1000))\n",
    "end = time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_params(final_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `vmap`-ed training loop over multiple starting points\n",
    "\n",
    "Now, we're going to do the ultimate: we'll create at least 100 different parameter initializations and run our training loop over each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_training_start(params_initializer, state_initializer, scanfunc, n_steps):\n",
    "    def train_one_start(key):\n",
    "        output_shape, params = params_initializer(key)\n",
    "        initial_state = state_initializer(params)\n",
    "        final_state, states_history = lax.scan(scanfunc, initial_state, np.arange(n_steps))\n",
    "        return final_state, states_history\n",
    "    return train_one_start\n",
    "\n",
    "train_linear = make_training_start(partial(init_fun, input_shape=(-1, 4)), init, scannable_step, 1000)\n",
    "\n",
    "start = time()\n",
    "N_INITIALIZATIONS = 100\n",
    "initialization_keys = random.split(key, N_INITIALIZATIONS)\n",
    "final_states, states_histories = vmap(train_linear)(initialization_keys)\n",
    "end = time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_final, b_final = vmap(get_params)(final_states)\n",
    "w_final.squeeze()[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_final.squeeze()[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we were also able to run the whole optimization pretty fast, _and_ recover the correct parameters over multiple training starts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JIT-compiled training loop\n",
    "\n",
    "What happens if we JIT-compile the vmapped initialization?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "N_INITIALIZATIONS = 100\n",
    "initialization_keys = random.split(key, N_INITIALIZATIONS)\n",
    "train_linear_jit = jit(train_linear)\n",
    "final_states, states_histories = vmap(train_linear_jit)(initialization_keys)\n",
    "vmap(get_params)(final_states)  # this line exists to just block the computation until it completes.\n",
    "end = time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HOOOOOLY SMOKES! Did you see that? With JIT-compilation, we essentially took the training time down to be identical to training on one starting point. Naturally, I don't expect this result to hold 100% of the time, but it's pretty darn rad to see that live. \n",
    "\n",
    "The craziest piece here is that we could `vmap` our training loop over multiple starting points and get massive speedups there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Model: Redux\n",
    "\n",
    "We're now going to try rewriting the neural network model that we had earlier on, now using `stax` syntax, and traing it using the syntax that we have learned above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reconstruct model using `stax.serial`\n",
    "\n",
    "Firstly, let's replicate the model using `stax.serial`. It's a serial composition of a Dense+Tanh layer, followed by a Dense+Sigmoid layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_init, nn_apply = stax.serial(\n",
    "    stax.Dense(20),\n",
    "    stax.Tanh,\n",
    "    stax.Dense(1),\n",
    "    stax.Sigmoid\n",
    ")\n",
    "\n",
    "\n",
    "def nn_init_wrapper(input_shape):\n",
    "    def inner(key):\n",
    "        return nn_init(key, input_shape)\n",
    "    return inner\n",
    "\n",
    "nn_initializer = nn_init_wrapper(input_shape=(-1, 41))\n",
    "nn_initializer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we initialize one instance of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_shape, params_init = nn_initializer(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need a loss funciton to optimize as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as np\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred, tol=1e-6):\n",
    "    return y_true * np.log(y_pred + tol) + (1 - y_true) * np.log(1 - y_pred + tol)\n",
    "\n",
    "def logistic_loss(params, model, x, y):\n",
    "    preds = vmap(partial(model, params))(x)\n",
    "    bces = vmap(binary_cross_entropy)(y, preds)\n",
    "    return -np.sum(bces)\n",
    "\n",
    "dlogistic_loss = grad(logistic_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in data\n",
    "\n",
    "Now, we load in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pyprojroot import here\n",
    "\n",
    "X = pd.read_csv(here() / 'data/biodeg_X.csv', index_col=0)\n",
    "y = pd.read_csv(here() / 'data/biodeg_y.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test-drive functions to make sure they work\n",
    "\n",
    "Always important. It'll reveal whether there's anything wrong with our code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_loss(params_init, nn_apply, X.values, y.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Progressively construct our training functions\n",
    "\n",
    "Firstly, we make sure the step function works with our logistic loss, model func, and actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental.optimizers import adam\n",
    "\n",
    "adam_init, update, get_params = adam(0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stepfunc_nn = partial(step, dlossfunc=dlogistic_loss, get_params=get_params, update=update, model=nn_apply, x=X.values, y_true=y.values)\n",
    "scannable_step = make_scannable_step(stepfunc_nn)\n",
    "train_nn = make_training_start(nn_initializer, adam_init, scannable_step, n_steps=3000)\n",
    "start = time()\n",
    "final_state, states_history = train_nn(key)\n",
    "end = time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Friends, if you remember where we started in the `tutorial.ipynb` notebook, the original neural network took approximately a minute to train on a GPU (and longer if on a CPU).\n",
    "\n",
    "Let's now start by ploting the loss over training iterations. We start first with a function that returns the loss from a given `state` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def calculate_loss(state, get_params, model, lossfunc, x, y):\n",
    "    params = get_params(state)\n",
    "    return lossfunc(params, model, x, y)\n",
    "\n",
    "calculate_loss(final_state, get_params, nn_apply, logistic_loss, X.values, y.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we need to `vmap` it over all states in the states history, to get back the loss score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_loss_vmap = partial(\n",
    "    calculate_loss,\n",
    "    get_params=get_params,\n",
    "    model=nn_apply,\n",
    "    lossfunc=logistic_loss,\n",
    "    x=X.values,\n",
    "    y=y.values\n",
    ")\n",
    "start = time()\n",
    "losses = vmap(calc_loss_vmap)(states_history)\n",
    "end = time()\n",
    "print(end - start)\n",
    "\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with multiple starting points\n",
    "\n",
    "Just as above, we can also train the neural network with multiple starting points, again by `vmap`-ing our training function across split PRNGKeys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = random.split(key, 5)\n",
    "\n",
    "start = time()\n",
    "final_states, state_histories = vmap(train_nn)(keys)\n",
    "end = time()\n",
    "print(end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_params(final_states)[0][0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the losses over each of the state histories. Our last function `calc_loss_vmap` calculates loss score for one time point, which we then `vmap` over a single `states_history`, so we need another function that encapsulates this behaviour and `vmap`s over all state histories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_history_loss(state_history):\n",
    "    losses = vmap(calc_loss_vmap)(state_history)\n",
    "    return losses\n",
    "\n",
    "losses = vmap(state_history_loss)(state_histories)\n",
    "losses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correctly-shaped! And now plotting it..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that's pretty cool! We were able to see the loss from three independent runs. \n",
    "\n",
    "With sufficient memory, one would be able to do more runs; when I was writing this notebook early on, I saw that it was getting difficult to do on the order of tens of runs due to memory allocation issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we saw a few things in action.\n",
    "\n",
    "Firstly, we saw how to use the `stax` module on a linear model. Anytime we have a new framework for doing differential programming, it's super important to be able to explore it in the context of a linear model, which is _basically_ the foundation of all deep learning.\n",
    "\n",
    "Secondly, we also explored how to leverage the JAX idioms to create fast parallelized training loops. We mixed-and-matched together `jit`, `vmap`, `lax.scan`, and `grad` into a performant training loop that was minimally nested.\n",
    "\n",
    "A corollary of this programming style is that _every piece of the code can, in principle, be properly tested_, because they are properly isolated. Have you written training loops where you modify a little piece here and a little piece there, until you lost what your original working one looked like? With training functions that are minimally nested, we can control the behaviour explicitly using closures/partials easily. Even when doing experimenation, our code can run reliably and fast.\n",
    "\n",
    "Thirdly, we saw how to apply the same lessons to training a neural network _really fast_ with multiple starting points. The essence of the solution was to properly structure our program in progressively higher level layers of abstraction. We carefully wrote the program to go from the inner most layer out until we hit our goal of allowing for a set of multiple starts. The key here is that each level of abstraction is very natural, and corresponds to a \"unit computation\" being applied consistently across an \"array\" of things. Once we identify that \"unit computation\", writing the `vmap`-able or `lax.scan`-able function becomes very easy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-workshop",
   "language": "python",
   "name": "dl-workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
